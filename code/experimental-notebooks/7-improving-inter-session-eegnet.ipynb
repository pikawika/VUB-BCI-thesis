{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Improving inter-session EEGNet\n",
    "\n",
    "In the previous notebooks `6-DL-based-classification.ipynb` it was shown that EEGNet has excellent performance for offline classification of MI EEG data of the same patient on the same session. However, once we considered inter-session data we saw that the non stationarity of the EEG data on different sessions caused issues. This is to be expected as it is a general issue in BCI applications that remains largely unsolved. Before experimenting with the even more challanging inter-subject setting, where a `general` BCI model is trained to work on an unseen patient, we see if improvements can be made to the inter-session performance of EEGNet through multiple strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "  - Correct anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct file access\n",
    "  - Checking TensorFlow GPU support\n",
    "- About the data\n",
    "- Tensorboard\n",
    "- Fine-tuning EEGNet on few samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct anaconda environment\n",
    "\n",
    "The `bci-master-thesis` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: bci-master-thesis\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following codeblock will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE version (1.0.2 recommended): 1.0.2\n",
      "PyRieMann version (0.2.7 recommended): 0.2.7\n",
      "Numpy version (1.21.5 recommended): 1.21.5\n",
      "Pandas version (1.4.1 recommended): 1.4.1\n",
      "Scikit-learn version (1.0.2 recommended): 1.0.2\n",
      "TensorFlow version (2.8.0 recommended): 2.8.0\n",
      "Keras version (2.8.0 recommended): 2.8.0\n",
      "Pickle version (4.0 recommended): 4.0\n",
      "Matplotlib version (3.5.1 recommended): 3.5.1\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Load utils with reloading allowed\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "import TF_tools\n",
    "importlib.reload(CLA_dataset)\n",
    "importlib.reload(TF_tools)\n",
    "\n",
    "# Load EEGModels\n",
    "import EEGModels\n",
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "\n",
    "import pyriemann as prm; print(f\"PyRieMann version (0.2.7 recommended): {prm.__version__}\")\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf;  print(f\"TensorFlow version (2.8.0 recommended): {tf.__version__}\")\n",
    "\n",
    "import keras; print(f\"Keras version (2.8.0 recommended): {keras.__version__}\")\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this experimental notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. FIF files from this same dataset are also made available in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). A check on the availability of these two datasets is performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matlab CLA file access: True\n",
      "Full MNE CLA file access: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a71dde",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Checking TensorFlow GPU support\n",
    "\n",
    "If you want to use TensorFlow with GPU acceleration, the below codeblock can help you gather insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8d50f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 CPUs available under the names:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "\n",
      "\n",
      "There are not GPUs available.\n"
     ]
    }
   ],
   "source": [
    "TF_tools.check_tf_cpu_gpu_presence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1629ba2",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## About the data\n",
    "\n",
    "Remember the meaning of the markers:\n",
    "- 0: “blank” or nothing is displayed in eGUI\n",
    "    - Can be seen as a break between stimuli, thus random EEG data that should probably be ignored\n",
    "- 1: Left hand action\n",
    "    - EEG data for MI of the left hand\n",
    "- 2: Right hand action\n",
    "    - EEG data for MI of the right hand\n",
    "- 3: Passive/neutral\n",
    "    - EEG data for MI of neither left nor right hand but 'focused'\n",
    "- 91: inter-session rest break period\n",
    "- 92: experiment end\n",
    "- 99: initial relaxation period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960330e",
   "metadata": {},
   "source": [
    "<hr> <hr>\n",
    "\n",
    "## Tensorboard\n",
    "\n",
    "To launch the tensorboard use the following command in the `experimental-notebooks` folder:\n",
    "- Windows: `tensorboard --logdir=./logs/`\n",
    "- MacOS: `tensorboard --logdir='./logs/'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b35576",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Fine-tuning EEGNet on few samples\n",
    "\n",
    "Since we want to improve the results on a new trial, it is not unreasonable to think that we want to recalibrate the new model.\n",
    "In it's simplest form, recalibration can be done by taking the previously trained model and training it further on new data from the new trial.\n",
    "This new data may consists of only a small amount of samples as to not become annoying for the user.\n",
    "Because we are fine-tuning the model for the new trial rather then completely retraining it, the learning rate is often lowered to combat overfitting to the few samples at hand.\n",
    "In our case, we consider that we have access to 30 samples of the new trial (10 for each label) for recalibration as this corresponds roughly to 5 minutes of user input.\n",
    "\n",
    "In this experiment we compare our approach with the previous obtained results to see if we have improved it.\n",
    "\n",
    "**Remember the results for subject `C`**\n",
    "\n",
    "| **Test index** | **Train index** | **Same-session EEGNet** | **Inter-session EEGNet** | **Fine-tuned EEGNet** |\n",
    "|----------------|-----------------|-------------------------|--------------------------|-----------------------|\n",
    "| 0              | 1 + 2           | 0.944 / 0.934           | 0.916 / 0.920            | 0.xxx / 0.xxx         |\n",
    "| 1              | 0 + 2           | 0.931 / 0.913           | 0.852 / 0.860            | 0.xxx / 0.xxx         |\n",
    "| 2              | 1 + 2           | 0.892 / 0.878           | 0.607 / 0.594            | 0.xxx / 0.xxx         |\n",
    "| 0              | 1               | 0.944 / 0.934           | 0.891 / 0.860            | 0.xxx / 0.xxx         |\n",
    "| 1              | 2               | 0.931 / 0.913           | 0.794 / 0.783            | 0.xxx / 0.xxx         |\n",
    "\n",
    "**For subject `B` these results are**\n",
    "\n",
    "| **Test index** | **Train index** | **Same-session EEGNet** | **Inter-session EEGNet** | **Fine-tuned EEGNet** |\n",
    "|----------------|-----------------|-------------------------|--------------------------|-----------------------|\n",
    "| 0              | 1 + 2           | 0.569 / 0.587           | 0.498 / 0.489            | 0.xxx / 0.xxx         |\n",
    "| 1              | 0 + 2           | 0.701 / 0.684           | 0.640 / 0.654            | 0.xxx / 0.xxx         |\n",
    "| 2              | 1 + 2           | 0.816 / 0.792           | 0.677 / 0.660            | 0.xxx / 0.xxx         |\n",
    "\n",
    "**For subject `E` the results are**\n",
    "\n",
    "| **Test index** | **Train index** | **Same-session EEGNet** | **Inter-session EEGNet** | **Fine-tuned EEGNet** |\n",
    "|----------------|-----------------|-------------------------|--------------------------|-----------------------|\n",
    "| 0              | 1 + 2           | 0.792 / 0.785           | 0.714 / 0.713            | 0.xxx / 0.xxx         |\n",
    "| 1              | 0 + 2           | 0.795 / 0.799           | 0.667 / 0.646            | 0.xxx / 0.xxx         |\n",
    "| 2              | 1 + 2           | 0.864 / 0.864           | 0.734 / 0.688            | 0.xxx / 0.xxx         |\n",
    "\n",
    "Again, we see that the DL approach using EEGNet far outperforms the CSP approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f70e7a",
   "metadata": {},
   "source": [
    "### Training the model on the train data\n",
    "\n",
    "We start by simply training the model on the available test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "757ebb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 666799  =      0.000 ...  3333.995 secs...\n",
      "Reading 0 ... 681199  =      0.000 ...  3405.995 secs...\n",
      "Reading 0 ... 669399  =      0.000 ...  3346.995 secs...\n",
      "\n",
      "Loaded train epochs and extracted labels\n",
      "Loaded train epochs and extracted labels\n",
      "Loaded test epochs and extracted labels\n",
      "\n",
      "\n",
      "Concatenated the train epochs\n",
      "Total amount of train labels: 1920\n",
      "\n",
      "\n",
      "Got the test epochs\n",
      "Total amount of train labels: 959\n",
      "\n",
      "Train labels OHE match regular labels: True\n",
      "Test labels OHE match regular labels: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# PREPPING THE DATA\n",
    "####################################################\n",
    "\n",
    "# Specify params\n",
    "test_subject = \"C\"\n",
    "test_idx = 2\n",
    "\n",
    "# Get data and choose test trial\n",
    "mne_raws = CLA_dataset.get_all_raw_mne_data_for_subject(subject_id= test_subject)\n",
    "test_trial = mne_raws[test_idx]\n",
    "\n",
    "# Init variables\n",
    "epochs = []\n",
    "labels_train = []\n",
    "\n",
    "print()\n",
    "\n",
    "for single_mne_raw in mne_raws:\n",
    "    # Load epochs from raw\n",
    "    mne_fixed_window_epochs = CLA_dataset.get_usefull_epochs_from_raw(single_mne_raw, start_offset=-1.5, end_offset=1.5)['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "    if (single_mne_raw == test_trial):\n",
    "        mne_fixed_window_epochs_test = mne_fixed_window_epochs\n",
    "        labels_test = mne_fixed_window_epochs.events[:, -1]\n",
    "        print(\"Loaded test epochs and extracted labels\")\n",
    "    else:  \n",
    "        epochs.append(mne_fixed_window_epochs)\n",
    "        labels_train.extend(mne_fixed_window_epochs.events[:, -1])\n",
    "        print(\"Loaded train epochs and extracted labels\")\n",
    "    \n",
    "# Make single epoch object for training\n",
    "with io.capture_output():\n",
    "    mne_fixed_window_epochs_train = mne.concatenate_epochs(epochs, verbose=False)\n",
    "print(\"\\n\\nConcatenated the train epochs\")\n",
    "print(f\"Total amount of train labels: {len(labels_train)}\")\n",
    "    \n",
    "# Show test epoch object\n",
    "with io.capture_output():\n",
    "    mne_fixed_window_epochs_train = mne.concatenate_epochs(epochs, verbose=False)\n",
    "print(\"\\n\\nGot the test epochs\")\n",
    "print(f\"Total amount of train labels: {len(labels_test)}\")\n",
    "\n",
    "# Go to 2D representation\n",
    "labels_train = np.array(labels_train).reshape(-1, 1)\n",
    "labels_test = np.array(labels_test).reshape(-1, 1)\n",
    "\n",
    "# One Hot Encode the labels\n",
    "ohe = OneHotEncoder()\n",
    "ohe_labels_train = ohe.fit_transform(labels_train).toarray()\n",
    "ohe_labels_test = ohe.transform(labels_test).toarray()\n",
    "\n",
    "# Validate OHE\n",
    "print(f\"\\nTrain labels OHE match regular labels: {np.array_equal(ohe.inverse_transform(ohe_labels_train), labels_train)}\")\n",
    "print(f\"Test labels OHE match regular labels: {np.array_equal(ohe.inverse_transform(ohe_labels_test), labels_test)}\")\n",
    "\n",
    "# Get training data\n",
    "with io.capture_output():\n",
    "    mne_fixed_window_epochs_train_data = mne_fixed_window_epochs_train.get_data(tmin=0.2, tmax=0.7)\n",
    "    mne_fixed_window_epochs_test_data = mne_fixed_window_epochs_test.get_data(tmin=0.2, tmax=0.7)\n",
    "\n",
    "# Fix scaling sensitivity as MNE stores as data * 10e-6\n",
    "mne_fixed_window_epochs_train_data = mne_fixed_window_epochs_train_data * 1000000\n",
    "mne_fixed_window_epochs_test_data = mne_fixed_window_epochs_test_data * 1000000\n",
    "\n",
    "# Remove unused variables\n",
    "del epochs\n",
    "del mne_fixed_window_epochs\n",
    "del test_trial\n",
    "del single_mne_raw\n",
    "del mne_raws\n",
    "del mne_fixed_window_epochs_train\n",
    "del mne_fixed_window_epochs_test\n",
    "del labels_train\n",
    "del labels_test\n",
    "del test_subject\n",
    "del test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4cef7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 21, 100, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 21, 100, 8)        400       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 21, 100, 8)       32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_3 (Depthwi  (None, 1, 100, 16)       336       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 1, 100, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1, 100, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_6 (Averag  (None, 1, 25, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " spatial_dropout2d_6 (Spatia  (None, 1, 25, 16)        0         \n",
      " lDropout2D)                                                     \n",
      "                                                                 \n",
      " separable_conv2d_3 (Separab  (None, 1, 25, 16)        512       \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 1, 25, 16)        64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1, 25, 16)         0         \n",
      "                                                                 \n",
      " average_pooling2d_7 (Averag  (None, 1, 3, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " spatial_dropout2d_7 (Spatia  (None, 1, 3, 16)         0         \n",
      " lDropout2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 147       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,555\n",
      "Trainable params: 1,475\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CREATE EEGNET MODEL\n",
    "####################################################\n",
    "\n",
    "# Create the TensorFlow Keras model\n",
    "keras_eegnet_model = EEGNet(\n",
    "    nb_classes = 3, # int, number of classes to classify. \n",
    "    Chans = 21, # number of channels in the EEG data. \n",
    "    Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "    dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "    kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "    F1 = 8, # number of temporal filters. (default: 8)\n",
    "    F2 = 16, # number of pointwise filters. (default: 16)\n",
    "    D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "    norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "    dropoutType = 'SpatialDropout2D' # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "    )\n",
    "\n",
    "# Compile the model so it can be fitted\n",
    "# Loss and optimizer from EEGNet paper\n",
    "keras_eegnet_model.compile(loss= 'categorical_crossentropy', \n",
    "                           optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001), \n",
    "                           metrics= [\"accuracy\"])\n",
    "\n",
    "# Show summary of the model\n",
    "keras_eegnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fecd45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 1.1339 - accuracy: 0.3148\n",
      "Epoch 1: val_loss improved from inf to 1.09554, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.39236, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 2s 92ms/step - loss: 1.1321 - accuracy: 0.3140 - val_loss: 1.0955 - val_accuracy: 0.3924\n",
      "Epoch 2/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 1.0800 - accuracy: 0.4281\n",
      "Epoch 2: val_loss improved from 1.09554 to 1.09140, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.39236 to 0.44965, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 1.0788 - accuracy: 0.4338 - val_loss: 1.0914 - val_accuracy: 0.4497\n",
      "Epoch 3/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 1.0451 - accuracy: 0.4977\n",
      "Epoch 3: val_loss improved from 1.09140 to 1.07404, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.44965 to 0.48958, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 1.0450 - accuracy: 0.4978 - val_loss: 1.0740 - val_accuracy: 0.4896\n",
      "Epoch 4/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 1.0012 - accuracy: 0.5430\n",
      "Epoch 4: val_loss improved from 1.07404 to 1.04617, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.48958 to 0.60243, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 1.0022 - accuracy: 0.5417 - val_loss: 1.0462 - val_accuracy: 0.6024\n",
      "Epoch 5/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.9508 - accuracy: 0.6008\n",
      "Epoch 5: val_loss improved from 1.04617 to 1.01029, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.60243 to 0.69965, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.9489 - accuracy: 0.6012 - val_loss: 1.0103 - val_accuracy: 0.6997\n",
      "Epoch 6/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.9011 - accuracy: 0.6359\n",
      "Epoch 6: val_loss improved from 1.01029 to 0.97934, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.69965\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.8992 - accuracy: 0.6391 - val_loss: 0.9793 - val_accuracy: 0.6979\n",
      "Epoch 7/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.8661 - accuracy: 0.6633\n",
      "Epoch 7: val_loss improved from 0.97934 to 0.95481, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.69965\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.8626 - accuracy: 0.6667 - val_loss: 0.9548 - val_accuracy: 0.6806\n",
      "Epoch 8/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.8289 - accuracy: 0.6945\n",
      "Epoch 8: val_loss improved from 0.95481 to 0.92576, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.69965 to 0.71875, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.8289 - accuracy: 0.6935 - val_loss: 0.9258 - val_accuracy: 0.7188\n",
      "Epoch 9/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.8063 - accuracy: 0.6820\n",
      "Epoch 9: val_loss improved from 0.92576 to 0.90576, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.71875 to 0.73264, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.8041 - accuracy: 0.6853 - val_loss: 0.9058 - val_accuracy: 0.7326\n",
      "Epoch 10/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7679 - accuracy: 0.7250\n",
      "Epoch 10: val_loss improved from 0.90576 to 0.88455, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.73264 to 0.75174, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.7649 - accuracy: 0.7247 - val_loss: 0.8846 - val_accuracy: 0.7517\n",
      "Epoch 11/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7457 - accuracy: 0.7289\n",
      "Epoch 11: val_loss improved from 0.88455 to 0.86780, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.75174\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.7432 - accuracy: 0.7307 - val_loss: 0.8678 - val_accuracy: 0.7448\n",
      "Epoch 12/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7224 - accuracy: 0.7555\n",
      "Epoch 12: val_loss improved from 0.86780 to 0.85034, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.75174\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.7209 - accuracy: 0.7560 - val_loss: 0.8503 - val_accuracy: 0.7517\n",
      "Epoch 13/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7056 - accuracy: 0.7594\n",
      "Epoch 13: val_loss improved from 0.85034 to 0.82999, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.75174 to 0.75347, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.7018 - accuracy: 0.7619 - val_loss: 0.8300 - val_accuracy: 0.7535\n",
      "Epoch 14/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6908 - accuracy: 0.7773\n",
      "Epoch 14: val_loss improved from 0.82999 to 0.82186, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.75347\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.6880 - accuracy: 0.7760 - val_loss: 0.8219 - val_accuracy: 0.7431\n",
      "Epoch 15/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6807 - accuracy: 0.7852\n",
      "Epoch 15: val_loss improved from 0.82186 to 0.80812, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.75347 to 0.76562, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.6802 - accuracy: 0.7857 - val_loss: 0.8081 - val_accuracy: 0.7656\n",
      "Epoch 16/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6606 - accuracy: 0.7867\n",
      "Epoch 16: val_loss improved from 0.80812 to 0.78584, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.76562\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.6607 - accuracy: 0.7850 - val_loss: 0.7858 - val_accuracy: 0.7656\n",
      "Epoch 17/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6506 - accuracy: 0.8023\n",
      "Epoch 17: val_loss improved from 0.78584 to 0.77622, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.76562\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.6526 - accuracy: 0.8021 - val_loss: 0.7762 - val_accuracy: 0.7639\n",
      "Epoch 18/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6302 - accuracy: 0.8109\n",
      "Epoch 18: val_loss improved from 0.77622 to 0.76894, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.76562 to 0.77257, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.6287 - accuracy: 0.8147 - val_loss: 0.7689 - val_accuracy: 0.7726\n",
      "Epoch 19/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6200 - accuracy: 0.8203\n",
      "Epoch 19: val_loss improved from 0.76894 to 0.75278, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.77257\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.6183 - accuracy: 0.8222 - val_loss: 0.7528 - val_accuracy: 0.7587\n",
      "Epoch 20/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.6064 - accuracy: 0.8133\n",
      "Epoch 20: val_loss improved from 0.75278 to 0.74360, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.77257\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.6069 - accuracy: 0.8132 - val_loss: 0.7436 - val_accuracy: 0.7656\n",
      "Epoch 21/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5966 - accuracy: 0.8305\n",
      "Epoch 21: val_loss improved from 0.74360 to 0.72991, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.77257 to 0.78819, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.6001 - accuracy: 0.8289 - val_loss: 0.7299 - val_accuracy: 0.7882\n",
      "Epoch 22/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5742 - accuracy: 0.8453\n",
      "Epoch 22: val_loss improved from 0.72991 to 0.69942, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.78819 to 0.81250, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.5757 - accuracy: 0.8460 - val_loss: 0.6994 - val_accuracy: 0.8125\n",
      "Epoch 23/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5762 - accuracy: 0.8367\n",
      "Epoch 23: val_loss did not improve from 0.69942\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.81250\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.5740 - accuracy: 0.8385 - val_loss: 0.7004 - val_accuracy: 0.8073\n",
      "Epoch 24/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5662 - accuracy: 0.8430\n",
      "Epoch 24: val_loss improved from 0.69942 to 0.69255, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.81250\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.5647 - accuracy: 0.8460 - val_loss: 0.6926 - val_accuracy: 0.8073\n",
      "Epoch 25/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5437 - accuracy: 0.8555\n",
      "Epoch 25: val_loss improved from 0.69255 to 0.68011, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.81250 to 0.81597, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 0.5446 - accuracy: 0.8557 - val_loss: 0.6801 - val_accuracy: 0.8160\n",
      "Epoch 26/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5616 - accuracy: 0.8430\n",
      "Epoch 26: val_loss improved from 0.68011 to 0.66202, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.81597 to 0.81944, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.5612 - accuracy: 0.8408 - val_loss: 0.6620 - val_accuracy: 0.8194\n",
      "Epoch 27/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5417 - accuracy: 0.8594\n",
      "Epoch 27: val_loss improved from 0.66202 to 0.65774, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.81944 to 0.82986, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.5436 - accuracy: 0.8571 - val_loss: 0.6577 - val_accuracy: 0.8299\n",
      "Epoch 28/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5456 - accuracy: 0.8562\n",
      "Epoch 28: val_loss improved from 0.65774 to 0.64525, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.82986\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.5449 - accuracy: 0.8571 - val_loss: 0.6453 - val_accuracy: 0.8212\n",
      "Epoch 29/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5399 - accuracy: 0.8477\n",
      "Epoch 29: val_loss improved from 0.64525 to 0.63743, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.82986\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.5401 - accuracy: 0.8475 - val_loss: 0.6374 - val_accuracy: 0.8247\n",
      "Epoch 30/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5178 - accuracy: 0.8516\n",
      "Epoch 30: val_loss improved from 0.63743 to 0.61777, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.82986 to 0.84375, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.5192 - accuracy: 0.8504 - val_loss: 0.6178 - val_accuracy: 0.8438\n",
      "Epoch 31/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5120 - accuracy: 0.8727\n",
      "Epoch 31: val_loss improved from 0.61777 to 0.61428, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.84375\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.5112 - accuracy: 0.8705 - val_loss: 0.6143 - val_accuracy: 0.8385\n",
      "Epoch 32/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5048 - accuracy: 0.8562\n",
      "Epoch 32: val_loss improved from 0.61428 to 0.61243, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.84375\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.5046 - accuracy: 0.8564 - val_loss: 0.6124 - val_accuracy: 0.8351\n",
      "Epoch 33/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4959 - accuracy: 0.8813\n",
      "Epoch 33: val_loss improved from 0.61243 to 0.60950, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.84375\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.4979 - accuracy: 0.8802 - val_loss: 0.6095 - val_accuracy: 0.8351\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5019 - accuracy: 0.8698\n",
      "Epoch 34: val_loss improved from 0.60950 to 0.60378, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.84375\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.5019 - accuracy: 0.8698 - val_loss: 0.6038 - val_accuracy: 0.8385\n",
      "Epoch 35/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4965 - accuracy: 0.8750\n",
      "Epoch 35: val_loss improved from 0.60378 to 0.60201, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.84375\n",
      "11/11 [==============================] - 1s 83ms/step - loss: 0.5003 - accuracy: 0.8720 - val_loss: 0.6020 - val_accuracy: 0.8351\n",
      "Epoch 36/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4883 - accuracy: 0.8766\n",
      "Epoch 36: val_loss improved from 0.60201 to 0.58890, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.84375 to 0.84549, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.4885 - accuracy: 0.8765 - val_loss: 0.5889 - val_accuracy: 0.8455\n",
      "Epoch 37/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4857 - accuracy: 0.8727\n",
      "Epoch 37: val_loss improved from 0.58890 to 0.58346, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.84549\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4863 - accuracy: 0.8735 - val_loss: 0.5835 - val_accuracy: 0.8455\n",
      "Epoch 38/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4940 - accuracy: 0.8680\n",
      "Epoch 38: val_loss improved from 0.58346 to 0.57719, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.84549 to 0.84722, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.4917 - accuracy: 0.8698 - val_loss: 0.5772 - val_accuracy: 0.8472\n",
      "Epoch 39/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4801 - accuracy: 0.8797\n",
      "Epoch 39: val_loss did not improve from 0.57719\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.4794 - accuracy: 0.8824 - val_loss: 0.5795 - val_accuracy: 0.8333\n",
      "Epoch 40/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4621 - accuracy: 0.8859\n",
      "Epoch 40: val_loss did not improve from 0.57719\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.4631 - accuracy: 0.8847 - val_loss: 0.5844 - val_accuracy: 0.8299\n",
      "Epoch 41/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4586 - accuracy: 0.8813\n",
      "Epoch 41: val_loss improved from 0.57719 to 0.56791, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.4615 - accuracy: 0.8795 - val_loss: 0.5679 - val_accuracy: 0.8472\n",
      "Epoch 42/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4728 - accuracy: 0.8781\n",
      "Epoch 42: val_loss improved from 0.56791 to 0.56032, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.4714 - accuracy: 0.8795 - val_loss: 0.5603 - val_accuracy: 0.8472\n",
      "Epoch 43/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4624 - accuracy: 0.8805\n",
      "Epoch 43: val_loss did not improve from 0.56032\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4620 - accuracy: 0.8810 - val_loss: 0.5629 - val_accuracy: 0.8403\n",
      "Epoch 44/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4605 - accuracy: 0.8773\n",
      "Epoch 44: val_loss did not improve from 0.56032\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4631 - accuracy: 0.8728 - val_loss: 0.5755 - val_accuracy: 0.8368\n",
      "Epoch 45/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4500 - accuracy: 0.8883\n",
      "Epoch 45: val_loss did not improve from 0.56032\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.4519 - accuracy: 0.8884 - val_loss: 0.5637 - val_accuracy: 0.8368\n",
      "Epoch 46/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4440 - accuracy: 0.8797\n",
      "Epoch 46: val_loss improved from 0.56032 to 0.55319, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.4440 - accuracy: 0.8802 - val_loss: 0.5532 - val_accuracy: 0.8403\n",
      "Epoch 47/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4586 - accuracy: 0.8852\n",
      "Epoch 47: val_loss did not improve from 0.55319\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.4557 - accuracy: 0.8876 - val_loss: 0.5543 - val_accuracy: 0.8472\n",
      "Epoch 48/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4485 - accuracy: 0.8828\n",
      "Epoch 48: val_loss improved from 0.55319 to 0.55197, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.84722\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.4519 - accuracy: 0.8824 - val_loss: 0.5520 - val_accuracy: 0.8438\n",
      "Epoch 49/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4642 - accuracy: 0.8789\n",
      "Epoch 49: val_loss improved from 0.55197 to 0.53392, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.84722 to 0.85069, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.4612 - accuracy: 0.8802 - val_loss: 0.5339 - val_accuracy: 0.8507\n",
      "Epoch 50/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4351 - accuracy: 0.8867\n",
      "Epoch 50: val_loss did not improve from 0.53392\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.85069\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4336 - accuracy: 0.8884 - val_loss: 0.5473 - val_accuracy: 0.8438\n",
      "Epoch 51/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4351 - accuracy: 0.8742\n",
      "Epoch 51: val_loss did not improve from 0.53392\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.85069\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4341 - accuracy: 0.8765 - val_loss: 0.5460 - val_accuracy: 0.8420\n",
      "Epoch 52/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4226 - accuracy: 0.8938\n",
      "Epoch 52: val_loss improved from 0.53392 to 0.53016, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.85069 to 0.85243, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.4246 - accuracy: 0.8929 - val_loss: 0.5302 - val_accuracy: 0.8524\n",
      "Epoch 53/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4503 - accuracy: 0.8727\n",
      "Epoch 53: val_loss did not improve from 0.53016\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4524 - accuracy: 0.8705 - val_loss: 0.5405 - val_accuracy: 0.8420\n",
      "Epoch 54/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4258 - accuracy: 0.8945\n",
      "Epoch 54: val_loss did not improve from 0.53016\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4229 - accuracy: 0.8951 - val_loss: 0.5400 - val_accuracy: 0.8403\n",
      "Epoch 55/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4347 - accuracy: 0.8859\n",
      "Epoch 55: val_loss improved from 0.53016 to 0.52489, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.4329 - accuracy: 0.8884 - val_loss: 0.5249 - val_accuracy: 0.8472\n",
      "Epoch 56/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4268 - accuracy: 0.8867\n",
      "Epoch 56: val_loss improved from 0.52489 to 0.52122, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4292 - accuracy: 0.8854 - val_loss: 0.5212 - val_accuracy: 0.8490\n",
      "Epoch 57/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4240 - accuracy: 0.8883\n",
      "Epoch 57: val_loss did not improve from 0.52122\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.4250 - accuracy: 0.8884 - val_loss: 0.5383 - val_accuracy: 0.8368\n",
      "Epoch 58/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4239 - accuracy: 0.8891\n",
      "Epoch 58: val_loss did not improve from 0.52122\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4216 - accuracy: 0.8891 - val_loss: 0.5284 - val_accuracy: 0.8507\n",
      "Epoch 59/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4135 - accuracy: 0.8914\n",
      "Epoch 59: val_loss did not improve from 0.52122\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.4119 - accuracy: 0.8943 - val_loss: 0.5286 - val_accuracy: 0.8490\n",
      "Epoch 60/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4106 - accuracy: 0.8914\n",
      "Epoch 60: val_loss did not improve from 0.52122\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4132 - accuracy: 0.8914 - val_loss: 0.5303 - val_accuracy: 0.8524\n",
      "Epoch 61/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4131 - accuracy: 0.8969\n",
      "Epoch 61: val_loss did not improve from 0.52122\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.85243\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4124 - accuracy: 0.8973 - val_loss: 0.5349 - val_accuracy: 0.8420\n",
      "Epoch 62/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4103 - accuracy: 0.8961\n",
      "Epoch 62: val_loss improved from 0.52122 to 0.51673, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.85243 to 0.85417, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.4131 - accuracy: 0.8958 - val_loss: 0.5167 - val_accuracy: 0.8542\n",
      "Epoch 63/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4125 - accuracy: 0.8914\n",
      "Epoch 63: val_loss improved from 0.51673 to 0.51598, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.85417 to 0.85764, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4127 - accuracy: 0.8899 - val_loss: 0.5160 - val_accuracy: 0.8576\n",
      "Epoch 64/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4052 - accuracy: 0.8961\n",
      "Epoch 64: val_loss did not improve from 0.51598\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.85764\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.4069 - accuracy: 0.8973 - val_loss: 0.5186 - val_accuracy: 0.8559\n",
      "Epoch 65/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4034 - accuracy: 0.8938\n",
      "Epoch 65: val_loss did not improve from 0.51598\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.85764\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4039 - accuracy: 0.8929 - val_loss: 0.5465 - val_accuracy: 0.8472\n",
      "Epoch 66/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4151 - accuracy: 0.8883\n",
      "Epoch 66: val_loss did not improve from 0.51598\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.85764\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.4123 - accuracy: 0.8906 - val_loss: 0.5506 - val_accuracy: 0.8351\n",
      "Epoch 67/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3981 - accuracy: 0.8961\n",
      "Epoch 67: val_loss did not improve from 0.51598\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.85764\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3974 - accuracy: 0.8958 - val_loss: 0.5233 - val_accuracy: 0.8438\n",
      "Epoch 68/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4035 - accuracy: 0.8961\n",
      "Epoch 68: val_loss improved from 0.51598 to 0.50679, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.85764 to 0.85938, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.4047 - accuracy: 0.8921 - val_loss: 0.5068 - val_accuracy: 0.8594\n",
      "Epoch 69/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3997 - accuracy: 0.8938\n",
      "Epoch 69: val_loss did not improve from 0.50679\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.85938\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.4034 - accuracy: 0.8914 - val_loss: 0.5115 - val_accuracy: 0.8576\n",
      "Epoch 70/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3957 - accuracy: 0.8945\n",
      "Epoch 70: val_loss did not improve from 0.50679\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.85938\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3951 - accuracy: 0.8943 - val_loss: 0.5328 - val_accuracy: 0.8507\n",
      "Epoch 71/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3826 - accuracy: 0.9055\n",
      "Epoch 71: val_loss did not improve from 0.50679\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.85938\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3813 - accuracy: 0.9055 - val_loss: 0.5126 - val_accuracy: 0.8576\n",
      "Epoch 72/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3821 - accuracy: 0.9055\n",
      "Epoch 72: val_loss did not improve from 0.50679\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.85938\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3836 - accuracy: 0.9033 - val_loss: 0.5129 - val_accuracy: 0.8559\n",
      "Epoch 73/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3975 - accuracy: 0.8930\n",
      "Epoch 73: val_loss did not improve from 0.50679\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.85938\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3970 - accuracy: 0.8929 - val_loss: 0.5151 - val_accuracy: 0.8542\n",
      "Epoch 74/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3858 - accuracy: 0.8969\n",
      "Epoch 74: val_loss improved from 0.50679 to 0.50480, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.85938 to 0.86111, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_highest_acc_model.hdf5\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.3860 - accuracy: 0.8966 - val_loss: 0.5048 - val_accuracy: 0.8611\n",
      "Epoch 75/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3891 - accuracy: 0.8945\n",
      "Epoch 75: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.3898 - accuracy: 0.8951 - val_loss: 0.5166 - val_accuracy: 0.8455\n",
      "Epoch 76/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3894 - accuracy: 0.8961\n",
      "Epoch 76: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3861 - accuracy: 0.8988 - val_loss: 0.5127 - val_accuracy: 0.8524\n",
      "Epoch 77/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3663 - accuracy: 0.9094\n",
      "Epoch 77: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3655 - accuracy: 0.9085 - val_loss: 0.5168 - val_accuracy: 0.8524\n",
      "Epoch 78/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3754 - accuracy: 0.9094\n",
      "Epoch 78: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3735 - accuracy: 0.9107 - val_loss: 0.5215 - val_accuracy: 0.8490\n",
      "Epoch 79/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3734 - accuracy: 0.9086\n",
      "Epoch 79: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3751 - accuracy: 0.9077 - val_loss: 0.5104 - val_accuracy: 0.8472\n",
      "Epoch 80/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3747 - accuracy: 0.9047\n",
      "Epoch 80: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3751 - accuracy: 0.9033 - val_loss: 0.5328 - val_accuracy: 0.8299\n",
      "Epoch 81/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3748 - accuracy: 0.9023\n",
      "Epoch 81: val_loss did not improve from 0.50480\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3751 - accuracy: 0.9018 - val_loss: 0.5149 - val_accuracy: 0.8490\n",
      "Epoch 82/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3755 - accuracy: 0.8945\n",
      "Epoch 82: val_loss improved from 0.50480 to 0.50157, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3725 - accuracy: 0.8981 - val_loss: 0.5016 - val_accuracy: 0.8559\n",
      "Epoch 83/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3681 - accuracy: 0.9078\n",
      "Epoch 83: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3653 - accuracy: 0.9085 - val_loss: 0.5121 - val_accuracy: 0.8542\n",
      "Epoch 84/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3738 - accuracy: 0.9133\n",
      "Epoch 84: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3699 - accuracy: 0.9167 - val_loss: 0.5105 - val_accuracy: 0.8559\n",
      "Epoch 85/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3661 - accuracy: 0.9070\n",
      "Epoch 85: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3640 - accuracy: 0.9085 - val_loss: 0.5178 - val_accuracy: 0.8524\n",
      "Epoch 86/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3478 - accuracy: 0.9062\n",
      "Epoch 86: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3474 - accuracy: 0.9070 - val_loss: 0.5181 - val_accuracy: 0.8524\n",
      "Epoch 87/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3693 - accuracy: 0.9031\n",
      "Epoch 87: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3653 - accuracy: 0.9040 - val_loss: 0.5232 - val_accuracy: 0.8490\n",
      "Epoch 88/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3666 - accuracy: 0.8875\n",
      "Epoch 88: val_loss did not improve from 0.50157\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3700 - accuracy: 0.8869 - val_loss: 0.5155 - val_accuracy: 0.8490\n",
      "Epoch 89/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3555 - accuracy: 0.9047\n",
      "Epoch 89: val_loss improved from 0.50157 to 0.49672, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.3546 - accuracy: 0.9055 - val_loss: 0.4967 - val_accuracy: 0.8594\n",
      "Epoch 90/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3546 - accuracy: 0.9031\n",
      "Epoch 90: val_loss did not improve from 0.49672\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3555 - accuracy: 0.9003 - val_loss: 0.5054 - val_accuracy: 0.8559\n",
      "Epoch 91/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3493 - accuracy: 0.9031\n",
      "Epoch 91: val_loss did not improve from 0.49672\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3518 - accuracy: 0.9025 - val_loss: 0.5101 - val_accuracy: 0.8472\n",
      "Epoch 92/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3558 - accuracy: 0.9133\n",
      "Epoch 92: val_loss did not improve from 0.49672\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3542 - accuracy: 0.9144 - val_loss: 0.5117 - val_accuracy: 0.8490\n",
      "Epoch 93/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3493 - accuracy: 0.9125\n",
      "Epoch 93: val_loss improved from 0.49672 to 0.49496, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.3462 - accuracy: 0.9137 - val_loss: 0.4950 - val_accuracy: 0.8594\n",
      "Epoch 94/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3527 - accuracy: 0.9086\n",
      "Epoch 94: val_loss improved from 0.49496 to 0.48929, saving model to ./saved_variables/7/EEGNet/EEGNet_multisession_C2_100hz_lowest_loss_model.hdf5\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3552 - accuracy: 0.9077 - val_loss: 0.4893 - val_accuracy: 0.8594\n",
      "Epoch 95/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3457 - accuracy: 0.9133\n",
      "Epoch 95: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3432 - accuracy: 0.9137 - val_loss: 0.5260 - val_accuracy: 0.8472\n",
      "Epoch 96/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3547 - accuracy: 0.9109\n",
      "Epoch 96: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3557 - accuracy: 0.9100 - val_loss: 0.5289 - val_accuracy: 0.8438\n",
      "Epoch 97/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3505 - accuracy: 0.9148\n",
      "Epoch 97: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3496 - accuracy: 0.9167 - val_loss: 0.5001 - val_accuracy: 0.8507\n",
      "Epoch 98/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3616 - accuracy: 0.9031\n",
      "Epoch 98: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3600 - accuracy: 0.9025 - val_loss: 0.5123 - val_accuracy: 0.8507\n",
      "Epoch 99/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3453 - accuracy: 0.8953\n",
      "Epoch 99: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3435 - accuracy: 0.8966 - val_loss: 0.5129 - val_accuracy: 0.8594\n",
      "Epoch 100/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3351 - accuracy: 0.9187\n",
      "Epoch 100: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3379 - accuracy: 0.9174 - val_loss: 0.4938 - val_accuracy: 0.8559\n",
      "Epoch 101/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3515 - accuracy: 0.9094\n",
      "Epoch 101: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 101: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3478 - accuracy: 0.9107 - val_loss: 0.5006 - val_accuracy: 0.8524\n",
      "Epoch 102/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3494 - accuracy: 0.8984\n",
      "Epoch 102: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 102: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3513 - accuracy: 0.8988 - val_loss: 0.5108 - val_accuracy: 0.8507\n",
      "Epoch 103/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3385 - accuracy: 0.9094\n",
      "Epoch 103: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 103: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3403 - accuracy: 0.9077 - val_loss: 0.5187 - val_accuracy: 0.8472\n",
      "Epoch 104/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3268 - accuracy: 0.9062\n",
      "Epoch 104: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 104: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3283 - accuracy: 0.9062 - val_loss: 0.5213 - val_accuracy: 0.8403\n",
      "Epoch 105/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3356 - accuracy: 0.9117\n",
      "Epoch 105: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 105: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3333 - accuracy: 0.9144 - val_loss: 0.5113 - val_accuracy: 0.8507\n",
      "Epoch 106/500\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3368 - accuracy: 0.9000\n",
      "Epoch 106: val_loss did not improve from 0.48929\n",
      "\n",
      "Epoch 106: val_accuracy did not improve from 0.86111\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.3324 - accuracy: 0.9025 - val_loss: 0.5165 - val_accuracy: 0.8472\n",
      "Epoch 107/500\n",
      " 5/11 [============>.................] - ETA: 0s - loss: 0.3390 - accuracy: 0.9016"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m (retrain_model): \u001b[39m# Retrain or not\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=14'>15</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/cpu:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=15'>16</a>\u001b[0m         keras_eegnet_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=16'>17</a>\u001b[0m             x\u001b[39m=\u001b[39;49m mne_fixed_window_epochs_train_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=17'>18</a>\u001b[0m             y\u001b[39m=\u001b[39;49m ohe_labels_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=18'>19</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m, \u001b[39m# Default: 32\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=19'>20</a>\u001b[0m             epochs\u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m, \u001b[39m# Default: 500 (EEGNet paper)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=20'>21</a>\u001b[0m             verbose\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m# 0 = silent, 1 = progress bar, 2 = one line per epoch\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=21'>22</a>\u001b[0m             callbacks\u001b[39m=\u001b[39;49m [TF_tools\u001b[39m.\u001b[39;49mtensorboard_callback(log_name\u001b[39m=\u001b[39;49m tensorboard_name),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=22'>23</a>\u001b[0m                         TF_tools\u001b[39m.\u001b[39;49mlowest_loss_model_save_callback(filepath\u001b[39m=\u001b[39;49m best_model_filename),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=23'>24</a>\u001b[0m                         TF_tools\u001b[39m.\u001b[39;49mhighest_accuracy_model_save_callback(filepath\u001b[39m=\u001b[39;49m best_model_filename)],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=24'>25</a>\u001b[0m             validation_split\u001b[39m=\u001b[39;49m \u001b[39m0.3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=25'>26</a>\u001b[0m             shuffle\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=26'>27</a>\u001b[0m             sample_weight\u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \u001b[39m# Can be interesting due to time series\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=27'>28</a>\u001b[0m             use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# Done for faster speed\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=28'>29</a>\u001b[0m             workers\u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m \u001b[39m# Done for faster speed\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=29'>30</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=31'>32</a>\u001b[0m \u001b[39m# Convert labels back to original\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lennertbontinck/Documents/GitHub/VUB-BCI-thesis/code/experimental-notebooks/7-improving-inter-session-eegnet.ipynb#ch0000024?line=32'>33</a>\u001b[0m y_test \u001b[39m=\u001b[39m ohe\u001b[39m.\u001b[39minverse_transform(ohe_labels_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///Users/lennertbontinck/opt/anaconda3/envs/bci-master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# TRAIN EEGNET MODEL\n",
    "####################################################\n",
    "\n",
    "# Configure\n",
    "retrain_model = True\n",
    "tensorboard_name = \"EEGNet_multisession_C2_100hz\"\n",
    "best_model_filename = f\"./saved_variables/7/EEGNet/{tensorboard_name}\"\n",
    "\n",
    "\n",
    "# Train on train/test split of data from one session\n",
    "## Note: the model is forced to use GPU, if GPU is not available replace with what is available e.g. /cpu:0\n",
    "## TODO: Change to CPU/GPU preference\n",
    "if (retrain_model): # Retrain or not\n",
    "    with tf.device('/cpu:0'):\n",
    "        keras_eegnet_model.fit(\n",
    "            x= mne_fixed_window_epochs_train_data,\n",
    "            y= ohe_labels_train,\n",
    "            batch_size= 128, # Default: 32\n",
    "            epochs= 500, # Default: 500 (EEGNet paper)\n",
    "            verbose= 1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "            callbacks= [TF_tools.tensorboard_callback(log_name= tensorboard_name),\n",
    "                        TF_tools.lowest_loss_model_save_callback(filepath= best_model_filename),\n",
    "                        TF_tools.highest_accuracy_model_save_callback(filepath= best_model_filename)],\n",
    "            validation_split= 0.3,\n",
    "            shuffle= True,\n",
    "            sample_weight= None, # Can be interesting due to time series\n",
    "            use_multiprocessing=True, # Done for faster speed\n",
    "            workers= 4 # Done for faster speed\n",
    "            )\n",
    "\n",
    "# Convert labels back to original\n",
    "y_test = ohe.inverse_transform(ohe_labels_test)\n",
    "\n",
    "# Get results for best validation loss model\n",
    "print(\"Results for lowest loss model without optimisations\")\n",
    "keras_eegnet_model = TF_tools.load_lowest_loss_model(filepath= best_model_filename)\n",
    "\n",
    "y_pred = keras_eegnet_model.predict(mne_fixed_window_epochs_test_data)\n",
    "y_pred = ohe.inverse_transform(y_pred)\n",
    "\n",
    "accuracy =  accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of: {accuracy}\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Get results for best validation loss model\n",
    "print(\"\\n\\nResults for highest accuracy model without optimisations\")\n",
    "kkeras_eegnet_model = TF_tools.load_highest_accuracy_model(filepath= best_model_filename)\n",
    "\n",
    "y_pred = keras_eegnet_model.predict(mne_fixed_window_epochs_test_data)\n",
    "y_pred = ohe.inverse_transform(y_pred)\n",
    "\n",
    "accuracy =  accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of: {accuracy}\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Remove unused variables\n",
    "del accuracy\n",
    "del best_model_filename\n",
    "del retrain_model\n",
    "del tensorboard_name\n",
    "del y_pred\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAUP\n",
    "####################################################\n",
    "\n",
    "# delete unused variables\n",
    "del keras_eegnet_model\n",
    "del mne_fixed_window_epochs_test_data\n",
    "del mne_fixed_window_epochs_train_data\n",
    "del ohe\n",
    "del ohe_labels_test\n",
    "del ohe_labels_train"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84064af20c740daff42ac3b3e9f22c9848b19c2b0c948f28e817991b26c67d86"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('bci-master-thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
