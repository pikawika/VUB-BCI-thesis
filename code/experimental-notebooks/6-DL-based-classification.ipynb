{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Deep Learning based classification\n",
    "\n",
    "In the previous notebooks `4-CSP-based-classification` and `5-CSP-params-new-subject`, we used CSP as a way to engineer feautures from raw EEG data and used common ML classifiers to use those features for classification.\n",
    "This had very varying results.\n",
    "As said by Kostas et al ([2021](https://doi.org/10.3389/fnhum.2021.653659)): \n",
    ">Deep Neural Network (DNN) models used for classifying EEG data thus need to both develop useful features from EEG signals and subsequently classify those features.\n",
    ">This frames both the promise and the challenge of using DNNs for supervised EEG classification. \n",
    ">On the one hand, it promises to almost entirely circumvent the need for feature engineering, but on the other hand, both feature discovery and classification need to be learned from a limited supply of (relevant) high-dimensional data.\n",
    "\n",
    "\n",
    "In this notebook we will explore 3 common Deep Learnging (DL) algorithms for EEG classifications: [EEGNet V2](http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta), [DeepConvNet](https://doi.org/10.1002/hbm.23730) and [ShallowConvNet](https://doi.org/10.1002/hbm.23730).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "  - Correct anaconda environment\n",
    "  - Correct module access\n",
    "  - Correct file access\n",
    "- Loading in data\n",
    "- EEGNet\n",
    "- ShallowConvNet (TODO)\n",
    "- DeepConvNet (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct anaconda environment\n",
    "\n",
    "The `bci-master-thesis` anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: bci-master-thesis\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following codeblock will load in all required modules and show if the versions match those that are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE version (1.0.2 recommended): 1.0.2\n",
      "PyRieMann version (0.2.7 recommended): 0.2.7\n",
      "Numpy version (1.21.5 recommended): 1.21.5\n",
      "Pandas version (1.4.1 recommended): 1.4.1\n",
      "Scikit-learn version (1.0.2 recommended): 1.0.2\n",
      "TensorFlow version (2.8.0 recommended): 2.8.0\n",
      "Pickle version (4.0 recommended): 4.0\n",
      "Matplotlib version (3.5.1 recommended): 3.5.1\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "\n",
    "# Load EEGModels\n",
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "\n",
    "import pyriemann as prm; print(f\"PyRieMann version (0.2.7 recommended): {prm.__version__}\")\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Deel Learning libraries\n",
    "import tensorflow as tf;  print(f\"TensorFlow version (2.8.0 recommended): {tf.__version__}\")\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this experimental notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. FIF files from this same dataset are also made available in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). A check on the availability of these two datasets is performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matlab CLA file access: True\n",
      "Full MNE CLA file access: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a71dde",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Loading in data\n",
    "\n",
    "In this step, we load the data.\n",
    "The loaded data is that of a specific subject and thus can exist of multiple MNE RAW objects.\n",
    "This list of objects is provided as well as a singular one, namely the first of the list.\n",
    "\n",
    "Remember the meaning of the markers:\n",
    "- 0: “blank” or nothing is displayed in eGUI\n",
    "    - Can be seen as a break between stimuli, thus random EEG data that should probably be ignored\n",
    "- 1: Left hand action\n",
    "    - EEG data for MI of the left hand\n",
    "- 2: Right hand action\n",
    "    - EEG data for MI of the right hand\n",
    "- 3: Passive/neutral\n",
    "    - EEG data for MI of neither left nor right hand but 'focused'\n",
    "- 91: inter-session rest break period\n",
    "- 92: experiment end\n",
    "- 99: initial relaxation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399f5c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNE raws for subject C: 3 files.\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING RAW MNE DATA AND EPOCS\n",
    "####################################################\n",
    "\n",
    "# The previous notebooks used subject C (best) as well as B and E (worse).\n",
    "subject = \"C\"\n",
    "\n",
    "# Load RAW MNE files and select first as singular MNE file\n",
    "mne_raws = CLA_dataset.get_raw_mne_data_for_subject(subject)\n",
    "mne_raw = mne_raws[0]\n",
    "print(f\"Loaded MNE raws for subject {subject}: {len(mne_raws)} files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26506383",
   "metadata": {},
   "source": [
    "<hr> <hr>\n",
    "\n",
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49eec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Global vars\n",
    "####################################################\n",
    "\n",
    "def tensorboard_callback(log_name: str):\n",
    "    return tf.keras.callbacks.TensorBoard('./logs/' + log_name, update_freq=\"batch\", profile_batch=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960330e",
   "metadata": {},
   "source": [
    "<hr> <hr>\n",
    "\n",
    "## Tensorboard\n",
    "\n",
    "To launch the tensorboard use `tensorboard --logdir='./logs'` in the experimental notebook folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a71dde",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## EEGNet\n",
    "\n",
    "EEGNet is a compact convolutional neural network for EEG-based brain–computer interfaces by Lawhern et al ([2018](https://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta)). There were two proposed version, the latter of which is the referenced published version. Since the latter performs fat better and is the most commonly known EEGNet, this V2 is used here. The EEGModels library provided by the Army Research Laboratory on [GitHub](https://github.com/vlawhern/arl-eegmodels) is used for easy use of this model.\n",
    "\n",
    "### Fixed window classification: Single trial | EEGNet | three class MI task | 100Hz input signal\n",
    "\n",
    "The model recommends using 128 samples.\n",
    "If you remember from the previous notebooks, the data was sampled at 200Hz, which means there are 200 samples per second.\n",
    "In the previous notebook we processed data of 0.5 seconds, thus 100 samples.\n",
    "To get initial results, we will use the model on this 100 samples data.\n",
    "It will be compared to a 128Hz variant later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14156d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 960 events and 801 original time points ...\n",
      "0 bad epochs dropped\n",
      "Loaded fixed window binary epochs:\n",
      "\n",
      "Extracted labels from epochs: [1 2 1 3 1 3 3 2 3 2]\n",
      "One Hot Encoded labels: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "Labels match before and after the One Hot Encoding: True\n",
      "Shape of data (epochs, channels, samples): (960, 21, 100)\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# PREPPING THE DATA\n",
    "####################################################\n",
    "\n",
    "# Get the epoch from the RAW limited to MI tasks\n",
    "# Include period before and after to enable filtering possibilities\n",
    "mne_fixed_window_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw, start_offset=-1.5, end_offset=1.5)['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "# Load the epochs, we don't need filtering since this is what we want to learn!\n",
    "mne_fixed_window_epochs.load_data()\n",
    "print(f\"Loaded fixed window binary epochs:\\n\")\n",
    "\n",
    "# Labels: should be one hot encoded!\n",
    "labels = mne_fixed_window_epochs.events[:, -1]\n",
    "print(f\"Extracted labels from epochs: {labels[:10]}\")\n",
    "\n",
    "# Go to 2D representation\n",
    "labels = labels.reshape(-1, 1)\n",
    "\n",
    "# One Hot Encode the labels\n",
    "ohe = OneHotEncoder()\n",
    "ohe_labels = ohe.fit_transform(labels).toarray()\n",
    "print(f\"One Hot Encoded labels: {ohe_labels[:10]}\")\n",
    "\n",
    "# Show ohe labels\n",
    "np.shape(ohe_labels)\n",
    "ohe_labels[:10]\n",
    "\n",
    "# Validate OHE\n",
    "print(f\"Labels match before and after the One Hot Encoding: {np.array_equal(ohe.inverse_transform(ohe_labels), labels)}\")\n",
    "\n",
    "# Get effective data (half a second)\n",
    "mne_fixed_window_epochs_data = mne_fixed_window_epochs.get_data(tmin=0.2, tmax=0.7)\n",
    "\n",
    "# Delete unused variables\n",
    "del mne_fixed_window_epochs\n",
    "print(f\"Shape of data (epochs, channels, samples): {np.shape(mne_fixed_window_epochs_data)}\")\n",
    "\n",
    "# Remove unused variables\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca901652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 21, 100, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 100, 8)        400       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 21, 100, 8)       32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_2 (Depthwi  (None, 1, 100, 16)       336       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 1, 100, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1, 100, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, 1, 25, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 25, 16)         0         \n",
      "                                                                 \n",
      " separable_conv2d_2 (Separab  (None, 1, 25, 16)        512       \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 1, 25, 16)        64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1, 25, 16)         0         \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, 1, 3, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 3, 16)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 147       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,555\n",
      "Trainable params: 1,475\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CREATE EEGNET MODEL\n",
    "####################################################\n",
    "\n",
    "# Create the TensorFlow Keras model\n",
    "keras_eegnet_model = EEGNet(\n",
    "    nb_classes = 3, # int, number of classes to classify. \n",
    "    Chans = 21, # number of channels in the EEG data. \n",
    "    Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "    dropoutRate = 0.2, # dropout fraction. (default: 0.5)\n",
    "    kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "    F1 = 8, # number of temporal filters. (default: 8)\n",
    "    F2 = 16, # number of pointwise filters. (default: 16)\n",
    "    D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "    norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "    dropoutType = 'Dropout' # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "    )\n",
    "\n",
    "# Compile the model so it can be fitted\n",
    "keras_eegnet_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Show summary of the model\n",
    "keras_eegnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca901652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 1.0988 - accuracy: 0.3836 - val_loss: 1.0993 - val_accuracy: 0.2741\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 1.0684 - accuracy: 0.5233 - val_loss: 1.0998 - val_accuracy: 0.2741\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.9980 - accuracy: 0.5866 - val_loss: 1.0995 - val_accuracy: 0.2741\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.9180 - accuracy: 0.6518 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.8170 - accuracy: 0.7579 - val_loss: 1.0976 - val_accuracy: 0.3333\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7595 - accuracy: 0.7821 - val_loss: 1.0978 - val_accuracy: 0.3333\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.7044 - accuracy: 0.8101 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6648 - accuracy: 0.8119 - val_loss: 1.1007 - val_accuracy: 0.3333\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6265 - accuracy: 0.8343 - val_loss: 1.1045 - val_accuracy: 0.2741\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.5973 - accuracy: 0.8622 - val_loss: 1.1072 - val_accuracy: 0.2741\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.5613 - accuracy: 0.8659 - val_loss: 1.1098 - val_accuracy: 0.2741\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.5455 - accuracy: 0.8622 - val_loss: 1.1152 - val_accuracy: 0.2741\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.5189 - accuracy: 0.8883 - val_loss: 1.1213 - val_accuracy: 0.2741\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.5031 - accuracy: 0.8864 - val_loss: 1.1233 - val_accuracy: 0.2741\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.4879 - accuracy: 0.8920 - val_loss: 1.1227 - val_accuracy: 0.2741\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.4686 - accuracy: 0.8957 - val_loss: 1.1235 - val_accuracy: 0.2741\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.4467 - accuracy: 0.9236 - val_loss: 1.1230 - val_accuracy: 0.2741\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.4399 - accuracy: 0.9032 - val_loss: 1.1431 - val_accuracy: 0.2741\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.4269 - accuracy: 0.9125 - val_loss: 1.1580 - val_accuracy: 0.2741\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.4176 - accuracy: 0.9218 - val_loss: 1.1562 - val_accuracy: 0.2741\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.4180 - accuracy: 0.9050 - val_loss: 1.1691 - val_accuracy: 0.2741\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.4071 - accuracy: 0.9143 - val_loss: 1.1715 - val_accuracy: 0.2741\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.3991 - accuracy: 0.9162 - val_loss: 1.1792 - val_accuracy: 0.2741\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.3841 - accuracy: 0.9311 - val_loss: 1.1609 - val_accuracy: 0.2741\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.4017 - accuracy: 0.8939 - val_loss: 1.1574 - val_accuracy: 0.2741\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.3855 - accuracy: 0.9143 - val_loss: 1.1521 - val_accuracy: 0.2741\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3758 - accuracy: 0.9274 - val_loss: 1.1194 - val_accuracy: 0.2741\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3674 - accuracy: 0.9199 - val_loss: 1.1031 - val_accuracy: 0.2741\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3619 - accuracy: 0.9274 - val_loss: 1.0478 - val_accuracy: 0.2741\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3608 - accuracy: 0.9292 - val_loss: 0.9673 - val_accuracy: 0.2889\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3517 - accuracy: 0.9274 - val_loss: 0.9460 - val_accuracy: 0.4519\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3450 - accuracy: 0.9311 - val_loss: 0.8816 - val_accuracy: 0.6741\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3396 - accuracy: 0.9348 - val_loss: 0.8858 - val_accuracy: 0.6148\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3354 - accuracy: 0.9311 - val_loss: 0.8826 - val_accuracy: 0.5926\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.3356 - accuracy: 0.9348 - val_loss: 0.8575 - val_accuracy: 0.6074\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3263 - accuracy: 0.9404 - val_loss: 0.7987 - val_accuracy: 0.6519\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.3290 - accuracy: 0.9255 - val_loss: 0.7306 - val_accuracy: 0.7407\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3186 - accuracy: 0.9367 - val_loss: 0.6311 - val_accuracy: 0.8370\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.3217 - accuracy: 0.9348 - val_loss: 0.5440 - val_accuracy: 0.9037\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.3154 - accuracy: 0.9330 - val_loss: 0.5747 - val_accuracy: 0.8519\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3208 - accuracy: 0.9274 - val_loss: 0.5691 - val_accuracy: 0.7926\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3104 - accuracy: 0.9385 - val_loss: 0.4874 - val_accuracy: 0.9111\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.3029 - accuracy: 0.9404 - val_loss: 0.5271 - val_accuracy: 0.8444\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.3161 - accuracy: 0.9292 - val_loss: 0.5913 - val_accuracy: 0.7259\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.3053 - accuracy: 0.9348 - val_loss: 0.3863 - val_accuracy: 0.9407\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.3135 - accuracy: 0.9274 - val_loss: 0.3631 - val_accuracy: 0.9333\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2944 - accuracy: 0.9441 - val_loss: 0.3492 - val_accuracy: 0.9259\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2943 - accuracy: 0.9385 - val_loss: 0.3400 - val_accuracy: 0.9111\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2861 - accuracy: 0.9460 - val_loss: 0.3301 - val_accuracy: 0.9037\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2810 - accuracy: 0.9553 - val_loss: 0.3924 - val_accuracy: 0.8370\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2837 - accuracy: 0.9348 - val_loss: 0.3911 - val_accuracy: 0.8222\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2843 - accuracy: 0.9423 - val_loss: 0.3960 - val_accuracy: 0.8296\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2814 - accuracy: 0.9311 - val_loss: 0.3700 - val_accuracy: 0.8593\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2811 - accuracy: 0.9348 - val_loss: 0.3156 - val_accuracy: 0.8963\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2698 - accuracy: 0.9385 - val_loss: 0.3134 - val_accuracy: 0.8963\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2714 - accuracy: 0.9423 - val_loss: 0.2944 - val_accuracy: 0.9185\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2669 - accuracy: 0.9423 - val_loss: 0.2938 - val_accuracy: 0.9037\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2558 - accuracy: 0.9609 - val_loss: 0.2642 - val_accuracy: 0.9259\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2774 - accuracy: 0.9423 - val_loss: 0.3078 - val_accuracy: 0.9111\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2684 - accuracy: 0.9423 - val_loss: 0.2903 - val_accuracy: 0.9037\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2600 - accuracy: 0.9441 - val_loss: 0.3159 - val_accuracy: 0.8815\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2545 - accuracy: 0.9423 - val_loss: 0.3446 - val_accuracy: 0.8815\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2597 - accuracy: 0.9441 - val_loss: 0.3938 - val_accuracy: 0.8222\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2515 - accuracy: 0.9404 - val_loss: 0.4097 - val_accuracy: 0.8296\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2585 - accuracy: 0.9423 - val_loss: 0.3745 - val_accuracy: 0.8519\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2570 - accuracy: 0.9330 - val_loss: 0.2897 - val_accuracy: 0.8963\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2434 - accuracy: 0.9497 - val_loss: 0.2514 - val_accuracy: 0.9407\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2542 - accuracy: 0.9330 - val_loss: 0.2583 - val_accuracy: 0.9185\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2481 - accuracy: 0.9385 - val_loss: 0.2527 - val_accuracy: 0.9259\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.2352 - accuracy: 0.9534 - val_loss: 0.2342 - val_accuracy: 0.9185\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.2477 - accuracy: 0.9497 - val_loss: 0.2369 - val_accuracy: 0.9556\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.2478 - accuracy: 0.9479 - val_loss: 0.2551 - val_accuracy: 0.9111\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.2399 - accuracy: 0.9404 - val_loss: 0.2648 - val_accuracy: 0.8889\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.2504 - accuracy: 0.9423 - val_loss: 0.4395 - val_accuracy: 0.7926\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2301 - accuracy: 0.9516 - val_loss: 0.3276 - val_accuracy: 0.8519\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2371 - accuracy: 0.9441 - val_loss: 0.3226 - val_accuracy: 0.8815\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2296 - accuracy: 0.9460 - val_loss: 0.2667 - val_accuracy: 0.8963\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2308 - accuracy: 0.9441 - val_loss: 0.2377 - val_accuracy: 0.9407\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2344 - accuracy: 0.9460 - val_loss: 0.2688 - val_accuracy: 0.9111\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2235 - accuracy: 0.9534 - val_loss: 0.2186 - val_accuracy: 0.9481\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2191 - accuracy: 0.9609 - val_loss: 0.2452 - val_accuracy: 0.9185\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.2296 - accuracy: 0.9441 - val_loss: 0.2672 - val_accuracy: 0.8667\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.2258 - accuracy: 0.9516 - val_loss: 0.2688 - val_accuracy: 0.9111\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.2216 - accuracy: 0.9497 - val_loss: 0.3646 - val_accuracy: 0.8296\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2097 - accuracy: 0.9590 - val_loss: 0.3862 - val_accuracy: 0.8148\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2159 - accuracy: 0.9479 - val_loss: 0.2366 - val_accuracy: 0.9259\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2171 - accuracy: 0.9479 - val_loss: 0.2969 - val_accuracy: 0.8593\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2241 - accuracy: 0.9385 - val_loss: 0.2235 - val_accuracy: 0.9333\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2212 - accuracy: 0.9479 - val_loss: 0.8797 - val_accuracy: 0.6074\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2257 - accuracy: 0.9479 - val_loss: 0.9703 - val_accuracy: 0.5481\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2134 - accuracy: 0.9534 - val_loss: 1.0978 - val_accuracy: 0.5037\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2157 - accuracy: 0.9497 - val_loss: 1.6706 - val_accuracy: 0.3630\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2175 - accuracy: 0.9441 - val_loss: 1.7478 - val_accuracy: 0.3704\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2169 - accuracy: 0.9572 - val_loss: 0.8757 - val_accuracy: 0.6074\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2060 - accuracy: 0.9553 - val_loss: 0.7363 - val_accuracy: 0.7037\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2033 - accuracy: 0.9553 - val_loss: 1.1123 - val_accuracy: 0.5111\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2060 - accuracy: 0.9497 - val_loss: 1.3028 - val_accuracy: 0.4519\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2114 - accuracy: 0.9572 - val_loss: 1.3199 - val_accuracy: 0.4444\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2073 - accuracy: 0.9572 - val_loss: 1.1257 - val_accuracy: 0.5185\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.1966 - accuracy: 0.9534 - val_loss: 1.2493 - val_accuracy: 0.4667\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.2026 - accuracy: 0.9441 - val_loss: 1.0659 - val_accuracy: 0.5111\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1947 - accuracy: 0.9628 - val_loss: 0.5514 - val_accuracy: 0.7481\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.1950 - accuracy: 0.9497 - val_loss: 0.3893 - val_accuracy: 0.8370\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2053 - accuracy: 0.9460 - val_loss: 0.2589 - val_accuracy: 0.8889\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1883 - accuracy: 0.9534 - val_loss: 0.2571 - val_accuracy: 0.8963\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1875 - accuracy: 0.9572 - val_loss: 0.5183 - val_accuracy: 0.7926\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1873 - accuracy: 0.9665 - val_loss: 0.2683 - val_accuracy: 0.8963\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1881 - accuracy: 0.9646 - val_loss: 0.2638 - val_accuracy: 0.8889\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1980 - accuracy: 0.9534 - val_loss: 0.2332 - val_accuracy: 0.9407\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1833 - accuracy: 0.9590 - val_loss: 0.2262 - val_accuracy: 0.9185\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1888 - accuracy: 0.9609 - val_loss: 0.2290 - val_accuracy: 0.9111\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1941 - accuracy: 0.9497 - val_loss: 0.2068 - val_accuracy: 0.9481\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1848 - accuracy: 0.9572 - val_loss: 0.2074 - val_accuracy: 0.9333\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1797 - accuracy: 0.9534 - val_loss: 0.3386 - val_accuracy: 0.8519\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1814 - accuracy: 0.9628 - val_loss: 0.3334 - val_accuracy: 0.8667\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1751 - accuracy: 0.9609 - val_loss: 0.2446 - val_accuracy: 0.9185\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1877 - accuracy: 0.9572 - val_loss: 0.2669 - val_accuracy: 0.9111\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1757 - accuracy: 0.9758 - val_loss: 0.2772 - val_accuracy: 0.9111\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1815 - accuracy: 0.9572 - val_loss: 0.3066 - val_accuracy: 0.8889\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.1883 - accuracy: 0.9497 - val_loss: 0.2790 - val_accuracy: 0.9037\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1744 - accuracy: 0.9646 - val_loss: 0.6325 - val_accuracy: 0.7111\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1733 - accuracy: 0.9590 - val_loss: 1.0375 - val_accuracy: 0.5556\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1730 - accuracy: 0.9646 - val_loss: 0.9933 - val_accuracy: 0.5852\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1743 - accuracy: 0.9534 - val_loss: 0.5571 - val_accuracy: 0.7481\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1867 - accuracy: 0.9553 - val_loss: 0.3446 - val_accuracy: 0.8593\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1722 - accuracy: 0.9665 - val_loss: 0.2409 - val_accuracy: 0.8963\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.1701 - accuracy: 0.9553 - val_loss: 0.1866 - val_accuracy: 0.9259\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1874 - accuracy: 0.9516 - val_loss: 0.1907 - val_accuracy: 0.9407\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1778 - accuracy: 0.9572 - val_loss: 0.2343 - val_accuracy: 0.9111\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1734 - accuracy: 0.9479 - val_loss: 0.3271 - val_accuracy: 0.8593\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1663 - accuracy: 0.9646 - val_loss: 0.4060 - val_accuracy: 0.8148\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1799 - accuracy: 0.9590 - val_loss: 0.3872 - val_accuracy: 0.8444\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.1697 - accuracy: 0.9572 - val_loss: 0.4228 - val_accuracy: 0.8148\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1656 - accuracy: 0.9665 - val_loss: 0.4724 - val_accuracy: 0.7852\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1598 - accuracy: 0.9572 - val_loss: 0.5696 - val_accuracy: 0.7259\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1645 - accuracy: 0.9572 - val_loss: 0.6371 - val_accuracy: 0.7185\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1690 - accuracy: 0.9516 - val_loss: 0.4982 - val_accuracy: 0.7778\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1703 - accuracy: 0.9572 - val_loss: 0.4246 - val_accuracy: 0.8296\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.1714 - accuracy: 0.9572 - val_loss: 0.3599 - val_accuracy: 0.8667\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1670 - accuracy: 0.9534 - val_loss: 0.2421 - val_accuracy: 0.9333\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1652 - accuracy: 0.9646 - val_loss: 0.2618 - val_accuracy: 0.9185\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1565 - accuracy: 0.9609 - val_loss: 0.3689 - val_accuracy: 0.8444\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1555 - accuracy: 0.9665 - val_loss: 0.5416 - val_accuracy: 0.7407\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1571 - accuracy: 0.9553 - val_loss: 0.2379 - val_accuracy: 0.9333\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1579 - accuracy: 0.9628 - val_loss: 0.2243 - val_accuracy: 0.9259\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1642 - accuracy: 0.9628 - val_loss: 0.3126 - val_accuracy: 0.8741\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1605 - accuracy: 0.9646 - val_loss: 0.5042 - val_accuracy: 0.7778\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1465 - accuracy: 0.9628 - val_loss: 0.5128 - val_accuracy: 0.7778\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1599 - accuracy: 0.9646 - val_loss: 0.4305 - val_accuracy: 0.8000\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1641 - accuracy: 0.9590 - val_loss: 0.4978 - val_accuracy: 0.7778\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.1567 - accuracy: 0.9609 - val_loss: 0.5015 - val_accuracy: 0.7852\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1566 - accuracy: 0.9553 - val_loss: 0.4760 - val_accuracy: 0.7926\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1621 - accuracy: 0.9590 - val_loss: 0.2337 - val_accuracy: 0.9185\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.1569 - accuracy: 0.9553 - val_loss: 0.2358 - val_accuracy: 0.9185\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1527 - accuracy: 0.9516 - val_loss: 0.2007 - val_accuracy: 0.9111\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1410 - accuracy: 0.9683 - val_loss: 0.2044 - val_accuracy: 0.9407\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1491 - accuracy: 0.9572 - val_loss: 0.2961 - val_accuracy: 0.9037\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1495 - accuracy: 0.9590 - val_loss: 0.2555 - val_accuracy: 0.9259\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1583 - accuracy: 0.9590 - val_loss: 0.2302 - val_accuracy: 0.9333\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1504 - accuracy: 0.9628 - val_loss: 0.2587 - val_accuracy: 0.8963\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1457 - accuracy: 0.9628 - val_loss: 0.2305 - val_accuracy: 0.9111\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1472 - accuracy: 0.9721 - val_loss: 0.2798 - val_accuracy: 0.8963\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1449 - accuracy: 0.9646 - val_loss: 0.3856 - val_accuracy: 0.8370\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1423 - accuracy: 0.9721 - val_loss: 0.3114 - val_accuracy: 0.8815\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1419 - accuracy: 0.9721 - val_loss: 0.2471 - val_accuracy: 0.8963\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1558 - accuracy: 0.9646 - val_loss: 0.1716 - val_accuracy: 0.9259\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.1457 - accuracy: 0.9646 - val_loss: 0.2135 - val_accuracy: 0.9111\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.1433 - accuracy: 0.9572 - val_loss: 0.1632 - val_accuracy: 0.9630\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1419 - accuracy: 0.9721 - val_loss: 0.1948 - val_accuracy: 0.9111\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1573 - accuracy: 0.9646 - val_loss: 0.3268 - val_accuracy: 0.8593\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1441 - accuracy: 0.9628 - val_loss: 0.2719 - val_accuracy: 0.9037\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1323 - accuracy: 0.9702 - val_loss: 0.2564 - val_accuracy: 0.9111\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1484 - accuracy: 0.9665 - val_loss: 0.2688 - val_accuracy: 0.8963\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.1394 - accuracy: 0.9683 - val_loss: 0.2599 - val_accuracy: 0.8963\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1326 - accuracy: 0.9721 - val_loss: 0.3940 - val_accuracy: 0.8296\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.1435 - accuracy: 0.9609 - val_loss: 0.2195 - val_accuracy: 0.9185\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1394 - accuracy: 0.9665 - val_loss: 0.2512 - val_accuracy: 0.8889\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1642 - accuracy: 0.9441 - val_loss: 0.3831 - val_accuracy: 0.8296\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1307 - accuracy: 0.9665 - val_loss: 0.4609 - val_accuracy: 0.8074\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1345 - accuracy: 0.9721 - val_loss: 0.4669 - val_accuracy: 0.7926\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1395 - accuracy: 0.9609 - val_loss: 0.2142 - val_accuracy: 0.9037\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.1318 - accuracy: 0.9702 - val_loss: 0.1851 - val_accuracy: 0.9556\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1312 - accuracy: 0.9683 - val_loss: 0.1759 - val_accuracy: 0.9630\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1353 - accuracy: 0.9702 - val_loss: 0.2014 - val_accuracy: 0.9259\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1303 - accuracy: 0.9739 - val_loss: 0.1719 - val_accuracy: 0.9407\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1368 - accuracy: 0.9628 - val_loss: 0.1611 - val_accuracy: 0.9407\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1281 - accuracy: 0.9683 - val_loss: 0.2201 - val_accuracy: 0.9185\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1445 - accuracy: 0.9590 - val_loss: 0.2957 - val_accuracy: 0.8444\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1352 - accuracy: 0.9739 - val_loss: 0.1864 - val_accuracy: 0.9333\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1361 - accuracy: 0.9683 - val_loss: 0.1962 - val_accuracy: 0.9185\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1335 - accuracy: 0.9665 - val_loss: 0.1783 - val_accuracy: 0.9333\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1336 - accuracy: 0.9683 - val_loss: 0.2175 - val_accuracy: 0.9111\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1261 - accuracy: 0.9758 - val_loss: 0.3819 - val_accuracy: 0.8296\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1394 - accuracy: 0.9609 - val_loss: 0.5343 - val_accuracy: 0.7778\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.1339 - accuracy: 0.9646 - val_loss: 0.3516 - val_accuracy: 0.8519\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.1254 - accuracy: 0.9646 - val_loss: 0.2585 - val_accuracy: 0.9037\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1310 - accuracy: 0.9683 - val_loss: 0.2784 - val_accuracy: 0.8519\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1294 - accuracy: 0.9739 - val_loss: 0.3290 - val_accuracy: 0.8444\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1147 - accuracy: 0.9814 - val_loss: 0.2690 - val_accuracy: 0.8593\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.1286 - accuracy: 0.9683 - val_loss: 0.1995 - val_accuracy: 0.9259\n",
      "Trained EEGNet on single session using train/test split and got accuracy of: 0.9201388888888888\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################\n",
    "# TRAIN EEGNET MODEL\n",
    "####################################################\n",
    "\n",
    "# Get train/test split of data from one session\n",
    "X_train, X_test, y_train, y_test = train_test_split(mne_fixed_window_epochs_data, ohe_labels, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    shuffle= True,\n",
    "                                                    stratify= ohe_labels,                                                    \n",
    "                                                    random_state=98)\n",
    "\n",
    "\n",
    "# Train on train/test split of data from one session\n",
    "## Train for 100 epochs\n",
    "keras_eegnet_model.fit(\n",
    "    x= X_train,\n",
    "    y= y_train,\n",
    "    batch_size= 32, # Default: 32\n",
    "    epochs= 200, # Default: 1, how many times to loop over data\n",
    "    verbose= 1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "    callbacks= [tensorboard_callback(\"EEGNet_raw_signal_0.5s_100samps_50kernlen_02drop\")], # To be used for TF Board\n",
    "    validation_split= 0.2,\n",
    "    shuffle= True,\n",
    "    sample_weight= None, # Can be interesting due to time series\n",
    "    )\n",
    "\n",
    "# Get predictions\n",
    "y_pred = keras_eegnet_model.predict(X_test)\n",
    "\n",
    "# Convert labels back to original\n",
    "y_test = ohe.inverse_transform(y_test)\n",
    "y_pred = ohe.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy =  accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Show results on rain/test split\n",
    "print(f\"Trained EEGNet on single session using train/test split and got accuracy of: {accuracy}\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Remove unused variables\n",
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del accuracy\n",
    "del y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18770e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAUP\n",
    "####################################################\n",
    "\n",
    "# delete unused variables\n",
    "del keras_eegnet_model\n",
    "del mne_fixed_window_epochs_data\n",
    "del ohe\n",
    "del ohe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0b75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
