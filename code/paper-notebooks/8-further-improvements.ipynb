{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Further improvements\n",
    "\n",
    "This notebook performs pilot studies into possible routes for further improving the classification results obtained from the offline experiments.\n",
    "This includes:\n",
    "   - Calibration on 5 minutes worth of data\n",
    "   - Subsampling electrodes\n",
    "\n",
    "Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. We will use the utility file `bci-master-thesis/code/utils/CLA_dataset.py` to work with this data. The data was stored as FIF files, which are included in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "   - Correct Anaconda environment\n",
    "   - Correct module access\n",
    "   - Correct file access\n",
    "   - Checking TensorFlow support\n",
    "- Calibrating on 5 minutes of training\n",
    "   - EEGNet: new session\n",
    "   - EEGNet: new subject\n",
    "   - LSTM EEGNet\n",
    "      - Results\n",
    "- Subsampling electrodes\n",
    "   - CSP + LDA\n",
    "      - Results\n",
    "   - EEGNet\n",
    "      - Results\n",
    "   - LSTM EEGNet\n",
    "      - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct Anaconda environment\n",
    "\n",
    "The `bci-master-thesis` Anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: bci-master-thesis\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE version (1.0.2 recommended): 1.0.2\n",
      "Numpy version (1.21.5 recommended): 1.21.5\n",
      "Pandas version (1.4.1 recommended): 1.4.1\n",
      "Scikit-learn version (1.0.2 recommended): 1.0.2\n",
      "TensorFlow version (2.8.0 recommended): 2.8.0\n",
      "Keras version (2.8.0 recommended): 2.8.0\n",
      "Pickle version (4.0 recommended): 4.0\n",
      "Matplotlib version (3.5.1 recommended): 3.5.1\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# allow reloading of libraries\n",
    "import importlib\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "import TF_tools\n",
    "importlib.reload(CLA_dataset)\n",
    "importlib.reload(TF_tools)\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "\n",
    "# EEGNet model\n",
    "import EEGModels\n",
    "from EEGModels import EEGNet\n",
    "\n",
    "# EEGNet model with LSTM\n",
    "import EEGNet_with_lstm\n",
    "from EEGNet_with_lstm import EEGNet_bidirectional_lstm, EEGNet_lstm_1Dconv\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "import copy\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf;  print(f\"TensorFlow version (2.8.0 recommended): {tf.__version__}\")\n",
    "\n",
    "import keras; print(f\"Keras version (2.8.0 recommended): {keras.__version__}\")\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. The following code block checks if all required files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matlab CLA file access: True\n",
      "Full MNE CLA file access: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a71dde",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Checking TensorFlow support\n",
    "\n",
    "If you want to use TensorFlow with GPU acceleration, the below codeblock can help you gather insight.\n",
    "\n",
    "To launch the tensorboard use the following command in the `paper-notebooks` folder, be sure to have the right environments active:\n",
    "- Windows: `tensorboard --logdir=./logs/`\n",
    "- MacOS: `tensorboard --logdir='./logs/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8d50f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 CPUs available under the names:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "\n",
      "\n",
      "There are 1 GPUs available under the names:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "TF_tools.check_tf_cpu_gpu_presence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Calibrating on 5 minutes of training\n",
    "\n",
    "In the data collection setup the marker stayed on the screen for one singular second and was followed by a 1.5 â€“ 2.5 second pause after which the next signal was shown.\n",
    "This totals 3.5 seconds per sample taken.\n",
    "As such, in 5 minutes, or 300 seconds, 85 samples should be obtainable.\n",
    "This is reduced to 75 samples in order to obtain 25 samples per class.\n",
    "These 25 samples per class are then used for calibration of both the EEGNet model and the EEGNet model extension with LSTM provided by us to compare both.\n",
    "Different layers are frozen to test different setups.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### EEGNet: new session\n",
    "\n",
    "Results for EEGNet base were:\n",
    "\n",
    "| **Subject** | **EEGNet: best validation accuracy** | **EEGNet: best validation loss** | **EEGNet: test split accuracy (best acc model)** | **EEGNet: test split accuracy (best loss model)** |\n",
    "|-------------|--------------------------------------|----------------------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| B           | 0.6927 @ epoch 1473                  | 0.761 @ epoch 1472               | 0.65                                             | 0.6573                                            |\n",
    "| C           | 0.8837 @ epoch 1159                  | 0.3827 @ epoch 584               | 0.7132                                           | 0.6986                                            |\n",
    "| E           | 0.7674 @ epoch 782                   | 0.5884 @ epoch 1028              | 0.7068                                           | 0.7246                                            |\n",
    "\n",
    "Result after calibration are:\n",
    "\n",
    "| **Subject** | **EEGNet: best validation accuracy** | **EEGNet: best validation loss** | **EEGNet: test split accuracy (best acc model)** | **EEGNet: test split accuracy (best loss model)** |\n",
    "|-------------|--------------------------------------|----------------------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| B           | 0.6667 @ epoch 459                   | 0.8669 @ epoch 1846              | 0.6429                                           | 0.6181                                            |\n",
    "| C           | 0.8 @ epoch 39                       | 0.798 @ epoch 2486               | 0.7104                                           | 0.7387                                            |\n",
    "| E           | 0.7667 @ epoch 56                    | 0.597 @ epoch 2500               | 0.7568                                           | 0.758                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf624836",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEGNet(\n",
    "    nb_classes = 3, # int, number of classes to classify. \n",
    "    Chans = 21, # number of channels in the EEG data. \n",
    "    Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "    dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "    kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "    F1 = 8, # number of temporal filters. (default: 8)\n",
    "    F2 = 16, # number of pointwise filters. (default: 16)\n",
    "    D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "    norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "    dropoutType = 'SpatialDropout2D' # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    ").layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CALIBRATE EEGNET ON NEW SESSION\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "do_experiment = False\n",
    "\n",
    "keras_eegnet_model = EEGNet(\n",
    "        nb_classes = 3, # int, number of classes to classify. \n",
    "        Chans = 21, # number of channels in the EEG data. \n",
    "        Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "        dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "        kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "        F1 = 8, # number of temporal filters. (default: 8)\n",
    "        F2 = 16, # number of pointwise filters. (default: 16)\n",
    "        D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "        norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "        dropoutType = 'SpatialDropout2D' # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "        )\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all found results\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                print()\n",
    "                print(\"####################################################\")\n",
    "                print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "                print(\"####################################################\")\n",
    "                print()\n",
    "                \n",
    "                ################### LOAD DATA ###################\n",
    "                # Names for model\n",
    "                pretrained_model_name = f\"saved_variables/4/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                tensorboard_name = f\"paper-notebook8_eegnet_calibration_newsession_subject{subject_id}\"\n",
    "                best_base_model_filename =  f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                # Open lowest loss model from file, lowest loss is chosen as it likely needs \"least calibration\"\n",
    "                pretrained_model = TF_tools.load_lowest_loss_model(filepath= pretrained_model_name)\n",
    "                \n",
    "                # Get train and test split\n",
    "                with io.capture_output():\n",
    "                        # Get new session data\n",
    "                        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "                        \n",
    "                        # Get epochs for new session\n",
    "                        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                             start_offset= start_offset,\n",
    "                                                                             end_offset= end_offset,\n",
    "                                                                             baseline= baseline)\n",
    "                        \n",
    "                        # Only keep epochs from the MI tasks\n",
    "                        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                        \n",
    "                        # Fix the indexing\n",
    "                        # NOTE: this is some weird MNE behaviour, the index retreived is not the actual index if not reset due to filtering done\n",
    "                        mne_epochs.reset_drop_log_selection()\n",
    "                        \n",
    "                        # Load epochs into memory\n",
    "                        mne_epochs.load_data()\n",
    "                        \n",
    "                        # Get calibration test split\n",
    "                        calibration_items, test_items = CLA_dataset.get_calibration_test_split_from_epochs(epochs= mne_epochs,\n",
    "                                                                                                           amount_of_samples_in_calibration_per_class = 25)\n",
    "                        \n",
    "                        \n",
    "                # Get OHE from file\n",
    "                with open(f\"saved_variables/4/samesubject_differentsession/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "                        ohe = pickle.load(f)\n",
    "                        \n",
    "                        \n",
    "                calibration_epochs = mne_epochs[calibration_items]\n",
    "                test_epochs = mne_epochs[test_items]\n",
    "\n",
    "                # Get labels\n",
    "                y_train = calibration_epochs.events[:, -1]\n",
    "                y_test = test_epochs.events[:, -1]\n",
    "\n",
    "                # Get train and test data\n",
    "                X_train = calibration_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                X_test = test_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "\n",
    "                # Fix scaling sensitivity as MNE stores as data * 10e-6\n",
    "                X_train = X_train * 1000000\n",
    "                X_test = X_test * 1000000\n",
    "                \n",
    "                # Further devide the calibration split in train and eval samples\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                                  y_train,\n",
    "                                                                  test_size = 0.4, # Use 5 samples per class for validation\n",
    "                                                                  shuffle= True,\n",
    "                                                                  stratify= y_train,                                                    \n",
    "                                                                  random_state= 1998)\n",
    "                \n",
    "                # Convert labels with OHE for Keras\n",
    "                y_train = ohe.transform(y_train.reshape(-1, 1)).toarray()\n",
    "                y_test = ohe.transform(y_test.reshape(-1, 1)).toarray()\n",
    "                y_val = ohe.transform(y_val.reshape(-1, 1)).toarray()\n",
    "                \n",
    "                # Print stats\n",
    "                print(f\"Calibrating with {np.shape(X_train)} windows\")\n",
    "                print(f\"Testing with {np.shape(X_test)} windows\")\n",
    "                \n",
    "                print(\"Calibrating epochs\")\n",
    "                display(calibration_epochs)\n",
    "                \n",
    "                print(\"test epochs\")\n",
    "                display(test_epochs)\n",
    "                        \n",
    "                ################### CALIBRATE MODEL ###################\n",
    "                # Make a calibration model\n",
    "                calibration_model = keras.models.clone_model(keras_eegnet_model)\n",
    "\n",
    "                \n",
    "                # Copy weights from pretrained model\n",
    "                calibration_model.set_weights(pretrained_model.get_weights())\n",
    "                \n",
    "                for layer in calibration_model.layers:\n",
    "                        layer.trainable = False\n",
    "                \n",
    "                # Allow last conv layer to learn\n",
    "                #calibration_model.layers[8].trainable = True # Overfits\n",
    "                \n",
    "                # Allow last layers to train (softmax)\n",
    "                calibration_model.layers[14].trainable = True\n",
    "                calibration_model.layers[15].trainable = True\n",
    "                \n",
    "                # Allow batch norm to train due to weird behaviour\n",
    "                calibration_model.layers[2].trainable = True\n",
    "                calibration_model.layers[4].trainable = True\n",
    "                calibration_model.layers[9].trainable = True\n",
    "                \n",
    "                \n",
    "                # Change dropout\n",
    "                calibration_model.layers[7].rate = 0\n",
    "                calibration_model.layers[12].rate = 0.2\n",
    "                \n",
    "                # Compile the model so it can be fitted, note a lower learning rate is set\n",
    "                calibration_model.compile(loss = 'categorical_crossentropy', optimizer = tf.optimizers.Adam(learning_rate= 0.0001), metrics=[\"accuracy\"])\n",
    "                \n",
    "                # Train model with GPU as means of recalibrating\n",
    "                # NOTE: change GPU to CPU if no GPU present\n",
    "                with tf.device('/gpu:0'):\n",
    "                        history = calibration_model.fit(\n",
    "                                x= X_train,\n",
    "                                y= y_train,\n",
    "                                batch_size= 128, # All calibration data at once\n",
    "                                epochs= 2500, # Very small due to direct overfit expected\n",
    "                                verbose= 1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                                callbacks= [TF_tools.tensorboard_callback(log_name= tensorboard_name),\n",
    "                                        TF_tools.lowest_loss_model_save_callback(filepath= best_base_model_filename),\n",
    "                                        TF_tools.highest_accuracy_model_save_callback(filepath= best_base_model_filename)],\n",
    "                                validation_data= (X_val, y_val),\n",
    "                                shuffle= True,\n",
    "                                use_multiprocessing= True, # Done for faster speed\n",
    "                                workers= 4 # Done for faster speed\n",
    "                                )\n",
    "                        \n",
    "                # Store the fitting history\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/fitting_history.pickle\", 'wb') as file:\n",
    "                        pickle.dump(history.history, file)\n",
    "                        \n",
    "                # Store the test data\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/X_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                        \n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/y_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "        \n",
    "                # remove unused vars\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del pretrained_model_name\n",
    "                del pretrained_model\n",
    "                del f\n",
    "                del X_test\n",
    "                del y_test\n",
    "                del ohe\n",
    "                del calibration_epochs\n",
    "                del calibration_items\n",
    "                del test_epochs\n",
    "                del test_items\n",
    "                del X_train\n",
    "                del y_train\n",
    "                del X_val\n",
    "                del y_val\n",
    "                del tensorboard_name\n",
    "                del history\n",
    "                del best_base_model_filename\n",
    "                del calibration_model\n",
    "                del file\n",
    "                del layer\n",
    "        \n",
    "        del subject_id\n",
    "        \n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset\n",
    "del keras_eegnet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb0936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print()\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print()\n",
    "    \n",
    "    ################### load data ###################\n",
    "    # Names for model\n",
    "    best_base_model_filename = f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "    \n",
    "    # Open models from file\n",
    "    lowest_loss_model = TF_tools.load_lowest_loss_model(filepath= best_base_model_filename)\n",
    "    highest_accuracy_model = TF_tools.load_highest_accuracy_model(filepath= best_base_model_filename)\n",
    "    \n",
    "    # Get test data session\n",
    "    with io.capture_output():\n",
    "        # Get test data\n",
    "        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "        \n",
    "        # Get epochs for test MNE raw\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the test labels\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/y_test.pickle\", 'rb') as f:\n",
    "            y_test = pickle.load(f)\n",
    "            \n",
    "        # Get the test data\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/X_test.pickle\", 'rb') as f:\n",
    "            X_test = pickle.load(f)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "        \n",
    "    # Get OHE from file\n",
    "    with open(f\"saved_variables/4/samesubject_differentsession/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "        ohe = pickle.load(f)\n",
    "        \n",
    "    # Get history from file\n",
    "    with open(f\"saved_variables/8/calibration/EEGNet/samesubject_differentsession/subject{subject_id}/fitting_history.pickle\", 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Inverse y_test to label\n",
    "    y_test = ohe.inverse_transform(y_test)\n",
    "    \n",
    "    ################### history stats ###################\n",
    "    print(\"#### results of training ####\")\n",
    "    print(f\"Best training accuracy (max) {np.round(np.max(history['accuracy']), 4)} @ epoch {np.argmax(history['accuracy']) + 1}\")\n",
    "    print(f\"Best training loss (min) {np.round(np.min(history['loss']), 4)} @ epoch {np.argmin(history['loss']) + 1}\")\n",
    "    print()\n",
    "    print(f\"Best validation accuracy (max) {np.round(np.max(history['val_accuracy']), 4)} @ epoch {np.argmax(history['val_accuracy']) + 1}\")\n",
    "    print(f\"Best validation loss (min) {np.round(np.min(history['val_loss']), 4)} @ epoch {np.argmin(history['val_loss']) + 1}\")\n",
    "    \n",
    "    ################### highest accuracy model ###################\n",
    "    print(\"\\n#### results for highest accuracy model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = highest_accuracy_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### lowest loss model ###################\n",
    "    print(\"\\n#### results for lowest loss model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = lowest_loss_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### cleanup ###################\n",
    "    # remove unused vars\n",
    "    del best_base_model_filename\n",
    "    del lowest_loss_model\n",
    "    del highest_accuracy_model\n",
    "    del f\n",
    "    del X_test\n",
    "    del y_test\n",
    "    del history\n",
    "    del ohe\n",
    "    del y_pred\n",
    "    del accuracy\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del subject_id\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6102d0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### EEGNet: new subject\n",
    "\n",
    "Results for EEGNet base were:\n",
    "\n",
    "| **Subject**   | **EEGNet: best validation accuracy** | **EEGNet: best validation loss** | **EEGNet: test split accuracy (best acc model)** | **EEGNet: test split accuracy (best loss model)** |\n",
    "|---------------|--------------------------------------|----------------------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| B (Train C&E) | 0.7654 @ epoch 92                    | 0.5768 @ epoch 135               | 0.6469                                           | 0.6479                                            |\n",
    "| C (Train B&E) | 0.6993 @ epoch 458                   | 0.6921 @ epoch 344               | 0.5892                                           | 0.5996                                            |\n",
    "| E (Train B&C) | 0.7454 @ epoch 889                   | 0.6425 @ epoch 1196              | 0.6419                                           | 0.6262                                            |\n",
    "\n",
    "Result after calibration are:\n",
    "\n",
    "| **Subject**   | **EEGNet: best validation accuracy** | **EEGNet: best validation loss** | **EEGNet: test split accuracy (best acc model)** | **EEGNet: test split accuracy (best loss model)** |\n",
    "|---------------|--------------------------------------|----------------------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| B (Train C&E) | 0.6 @ epoch 193                      | 0.8591 @ epoch 194               | 0.6373                                           | 0.6373                                            |\n",
    "| C (Train B&E) | 0.7667 @ epoch 304                   | 0.6773 @ epoch 2486              | 0.6233                                           | 0.6188                                            |\n",
    "| E (Train B&C) | 0.7333 @ epoch 85                    | 0.6853 @ epoch 2374              | 0.6432                                           | 0.6602                                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffa456",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CALIBRATE EEGNET ON NEW SESSION\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "do_experiment = False\n",
    "\n",
    "keras_eegnet_model = EEGNet(\n",
    "        nb_classes = 3, # int, number of classes to classify. \n",
    "        Chans = 21, # number of channels in the EEG data. \n",
    "        Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "        dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "        kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "        F1 = 8, # number of temporal filters. (default: 8)\n",
    "        F2 = 16, # number of pointwise filters. (default: 16)\n",
    "        D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "        norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "        dropoutType = 'SpatialDropout2D' # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "        )\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all found results\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                print()\n",
    "                print(\"####################################################\")\n",
    "                print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "                print(\"####################################################\")\n",
    "                print()\n",
    "                \n",
    "                ################### LOAD DATA ###################\n",
    "                # Names for model\n",
    "                pretrained_model_name = f\"saved_variables/4/newsubject/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                tensorboard_name = f\"paper-notebook8_eegnet_calibration_newsubject_subject{subject_id}\"\n",
    "                best_base_model_filename =  f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                # Open lowest loss model from file, lowest loss is chosen as it likely needs \"least calibration\"\n",
    "                pretrained_model = TF_tools.load_lowest_loss_model(filepath= pretrained_model_name)\n",
    "                \n",
    "                # Get train and test split\n",
    "                with io.capture_output():\n",
    "                        # Get new session data\n",
    "                        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "                        \n",
    "                        # Get epochs for new session\n",
    "                        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                             start_offset= start_offset,\n",
    "                                                                             end_offset= end_offset,\n",
    "                                                                             baseline= baseline)\n",
    "                        \n",
    "                        # Only keep epochs from the MI tasks\n",
    "                        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                        \n",
    "                        # Fix the indexing\n",
    "                        # NOTE: this is some weird MNE behaviour, the index retreived is not the actual index if not reset due to filtering done\n",
    "                        mne_epochs.reset_drop_log_selection()\n",
    "                        \n",
    "                        # Load epochs into memory\n",
    "                        mne_epochs.load_data()\n",
    "                        \n",
    "                        # Get calibration test split\n",
    "                        calibration_items, test_items = CLA_dataset.get_calibration_test_split_from_epochs(epochs= mne_epochs,\n",
    "                                                                                                           amount_of_samples_in_calibration_per_class = 25)\n",
    "                        \n",
    "                        \n",
    "                # Get OHE from file\n",
    "                with open(f\"saved_variables/4/newsubject/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "                        ohe = pickle.load(f)\n",
    "                        \n",
    "                        \n",
    "                calibration_epochs = mne_epochs[calibration_items]\n",
    "                test_epochs = mne_epochs[test_items]\n",
    "\n",
    "                # Get labels\n",
    "                y_train = calibration_epochs.events[:, -1]\n",
    "                y_test = test_epochs.events[:, -1]\n",
    "\n",
    "                # Get train and test data\n",
    "                X_train = calibration_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                X_test = test_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "\n",
    "                # Fix scaling sensitivity as MNE stores as data * 10e-6\n",
    "                X_train = X_train * 1000000\n",
    "                X_test = X_test * 1000000\n",
    "                \n",
    "                # Further devide the calibration split in train and eval samples\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                                  y_train,\n",
    "                                                                  test_size = 0.4, # Use 5 samples per class for validation\n",
    "                                                                  shuffle= True,\n",
    "                                                                  stratify= y_train,                                                    \n",
    "                                                                  random_state= 1998)\n",
    "                \n",
    "                # Convert labels with OHE for Keras\n",
    "                y_train = ohe.transform(y_train.reshape(-1, 1)).toarray()\n",
    "                y_test = ohe.transform(y_test.reshape(-1, 1)).toarray()\n",
    "                y_val = ohe.transform(y_val.reshape(-1, 1)).toarray()\n",
    "                \n",
    "                # Print stats\n",
    "                print(f\"Calibrating with {np.shape(X_train)} windows\")\n",
    "                print(f\"Testing with {np.shape(X_test)} windows\")\n",
    "                \n",
    "                print(\"Calibrating epochs\")\n",
    "                display(calibration_epochs)\n",
    "                \n",
    "                print(\"test epochs\")\n",
    "                display(test_epochs)\n",
    "                        \n",
    "                ################### CALIBRATE MODEL ###################\n",
    "                # Make a calibration model\n",
    "                calibration_model = keras.models.clone_model(keras_eegnet_model)\n",
    "\n",
    "                \n",
    "                # Copy weights from pretrained model\n",
    "                calibration_model.set_weights(pretrained_model.get_weights())\n",
    "                \n",
    "                for layer in calibration_model.layers:\n",
    "                        layer.trainable = False\n",
    "                \n",
    "                # Allow last conv layer to learn\n",
    "                #calibration_model.layers[8].trainable = True # Overfits\n",
    "                \n",
    "                # Allow last layers to train (softmax)\n",
    "                calibration_model.layers[14].trainable = True\n",
    "                calibration_model.layers[15].trainable = True\n",
    "                \n",
    "                # Allow batch norm to train due to weird behaviour\n",
    "                calibration_model.layers[2].trainable = True\n",
    "                calibration_model.layers[4].trainable = True\n",
    "                calibration_model.layers[9].trainable = True\n",
    "                \n",
    "                \n",
    "                # Change dropout\n",
    "                calibration_model.layers[7].rate = 0\n",
    "                calibration_model.layers[12].rate = 0.2\n",
    "                \n",
    "                # Compile the model so it can be fitted, note a lower learning rate is set\n",
    "                calibration_model.compile(loss = 'categorical_crossentropy', optimizer = tf.optimizers.Adam(learning_rate= 0.0001), metrics=[\"accuracy\"])\n",
    "                \n",
    "                # Train model with GPU as means of recalibrating\n",
    "                # NOTE: change GPU to CPU if no GPU present\n",
    "                with tf.device('/gpu:0'):\n",
    "                        history = calibration_model.fit(\n",
    "                                x= X_train,\n",
    "                                y= y_train,\n",
    "                                batch_size= 128, # All calibration data at once\n",
    "                                epochs= 2500, # Very small due to direct overfit expected\n",
    "                                verbose= 1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                                callbacks= [TF_tools.tensorboard_callback(log_name= tensorboard_name),\n",
    "                                        TF_tools.lowest_loss_model_save_callback(filepath= best_base_model_filename),\n",
    "                                        TF_tools.highest_accuracy_model_save_callback(filepath= best_base_model_filename)],\n",
    "                                validation_data= (X_val, y_val),\n",
    "                                shuffle= True,\n",
    "                                use_multiprocessing= True, # Done for faster speed\n",
    "                                workers= 4 # Done for faster speed\n",
    "                                )\n",
    "                        \n",
    "                # Store the fitting history\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/fitting_history.pickle\", 'wb') as file:\n",
    "                        pickle.dump(history.history, file)\n",
    "                        \n",
    "                # Store the test data\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/X_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                        \n",
    "                with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/y_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "        \n",
    "                # remove unused vars\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del pretrained_model_name\n",
    "                del pretrained_model\n",
    "                del f\n",
    "                del X_test\n",
    "                del y_test\n",
    "                del ohe\n",
    "                del calibration_epochs\n",
    "                del calibration_items\n",
    "                del test_epochs\n",
    "                del test_items\n",
    "                del X_train\n",
    "                del y_train\n",
    "                del X_val\n",
    "                del y_val\n",
    "                del tensorboard_name\n",
    "                del history\n",
    "                del best_base_model_filename\n",
    "                del calibration_model\n",
    "                del file\n",
    "                del layer\n",
    "        \n",
    "        del subject_id\n",
    "        \n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset\n",
    "del keras_eegnet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1f8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print()\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print()\n",
    "    \n",
    "    ################### load data ###################\n",
    "    # Names for model\n",
    "    best_base_model_filename = f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/trained_model\"\n",
    "    \n",
    "    # Open models from file\n",
    "    lowest_loss_model = TF_tools.load_lowest_loss_model(filepath= best_base_model_filename)\n",
    "    highest_accuracy_model = TF_tools.load_highest_accuracy_model(filepath= best_base_model_filename)\n",
    "    \n",
    "    # Get test data session\n",
    "    with io.capture_output():\n",
    "        # Get test data\n",
    "        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "        \n",
    "        # Get epochs for test MNE raw\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the test labels\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/y_test.pickle\", 'rb') as f:\n",
    "            y_test = pickle.load(f)\n",
    "            \n",
    "        # Get the test data\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/X_test.pickle\", 'rb') as f:\n",
    "            X_test = pickle.load(f)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "        \n",
    "    # Get OHE from file\n",
    "    with open(f\"saved_variables/4/newsubject/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "        ohe = pickle.load(f)\n",
    "        \n",
    "    # Get history from file\n",
    "    with open(f\"saved_variables/8/calibration/EEGNet/newsubject/subject{subject_id}/fitting_history.pickle\", 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Inverse y_test to label\n",
    "    y_test = ohe.inverse_transform(y_test)\n",
    "    \n",
    "    ################### history stats ###################\n",
    "    print(\"#### results of training ####\")\n",
    "    print(f\"Best training accuracy (max) {np.round(np.max(history['accuracy']), 4)} @ epoch {np.argmax(history['accuracy']) + 1}\")\n",
    "    print(f\"Best training loss (min) {np.round(np.min(history['loss']), 4)} @ epoch {np.argmin(history['loss']) + 1}\")\n",
    "    print()\n",
    "    print(f\"Best validation accuracy (max) {np.round(np.max(history['val_accuracy']), 4)} @ epoch {np.argmax(history['val_accuracy']) + 1}\")\n",
    "    print(f\"Best validation loss (min) {np.round(np.min(history['val_loss']), 4)} @ epoch {np.argmin(history['val_loss']) + 1}\")\n",
    "    \n",
    "    ################### highest accuracy model ###################\n",
    "    print(\"\\n#### results for highest accuracy model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = highest_accuracy_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### lowest loss model ###################\n",
    "    print(\"\\n#### results for lowest loss model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = lowest_loss_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### cleanup ###################\n",
    "    # remove unused vars\n",
    "    del best_base_model_filename\n",
    "    del lowest_loss_model\n",
    "    del highest_accuracy_model\n",
    "    del f\n",
    "    del X_test\n",
    "    del y_test\n",
    "    del history\n",
    "    del ohe\n",
    "    del y_pred\n",
    "    del accuracy\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del subject_id\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79afa6c3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### EEGNet with LSTM conv: new session\n",
    "\n",
    "Results before calibration were:\n",
    "\n",
    "| **Subject** | **EEGNet: best validation accuracy** | **EEGNet: best validation loss** | **EEGNet: test split accuracy (best acc model)** | **EEGNet: test split accuracy (best loss model)** |\n",
    "|-------------|--------------------------------------|----------------------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| B           | 0.6927 @ epoch 1473                  | 0.761 @ epoch 1472               | 0.65                                             | 0.6573                                            |\n",
    "| C           | 0.8837 @ epoch 1159                  | 0.3827 @ epoch 584               | 0.7132                                           | 0.6986                                            |\n",
    "| E           | 0.7674 @ epoch 782                   | 0.5884 @ epoch 1028              | 0.7068                                           | 0.7246                                            |\n",
    "\n",
    "Result after calibration are:\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# MODEL OVERVIEW\n",
    "####################################################\n",
    "\n",
    "EEGNet_lstm_1Dconv(\n",
    "    nb_classes = 3, # int, number of classes to classify. \n",
    "    Chans = 21, # number of channels in the EEG data. \n",
    "    Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "    dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "    kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "    F1 = 8, # number of temporal filters. (default: 8)\n",
    "    D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "    norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "    dropoutType = 'SpatialDropout2D', # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "    lstm_filters = 40, # Amount of filters for LSTM layer Conv1D. Default: 32\n",
    "    lstm_kernel_size = 9, # Kernels size for LSTM layer Conv1D.\n",
    "    ltsm_dropout= 0.7,\n",
    "    ltsm_l1 = 0.00005, \n",
    "    ltsm_l2 = 0.00005\n",
    ").layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1efe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CALIBRATE EEGNET ON NEW SESSION\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "do_experiment = False\n",
    "\n",
    "keras_eegnet_lstm_1Dconv_model = EEGNet_lstm_1Dconv(\n",
    "    nb_classes = 3, # int, number of classes to classify. \n",
    "    Chans = 21, # number of channels in the EEG data. \n",
    "    Samples = 100, # number of time points in the EEG data. (default: 128)\n",
    "    dropoutRate = 0.5, # dropout fraction. (default: 0.5)\n",
    "    kernLength = 50, # length of temporal convolution in first layer. Suggested: half the sampling rate. (default: 64)\n",
    "    F1 = 8, # number of temporal filters. (default: 8)\n",
    "    D = 2, # number of spatial filters to learn within each temporal convolution. (default: 2)\n",
    "    norm_rate = 0.25, # Normalisation rate. (default: 0.25)\n",
    "    dropoutType = 'SpatialDropout2D', # Either SpatialDropout2D or Dropout, passed as a string. (default: Dropout)\n",
    "    lstm_filters = 40, # Amount of filters for LSTM layer Conv1D. Default: 32\n",
    "    lstm_kernel_size = 9, # Kernels size for LSTM layer Conv1D.\n",
    "    ltsm_dropout= 0.7,\n",
    "    ltsm_l1 = 0.00005, \n",
    "    ltsm_l2 = 0.00005\n",
    ")\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all found results\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                print()\n",
    "                print(\"####################################################\")\n",
    "                print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "                print(\"####################################################\")\n",
    "                print()\n",
    "                \n",
    "                ################### LOAD DATA ###################\n",
    "                # Names for model\n",
    "                pretrained_model_name = f\"saved_variables/7/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                tensorboard_name = f\"paper-notebook8_eegnet_convlstm_calibration_newsession_subject{subject_id}\"\n",
    "                best_base_model_filename =  f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "                \n",
    "                # Open lowest loss model from file, lowest loss is chosen as it likely needs \"least calibration\"\n",
    "                pretrained_model = TF_tools.load_lowest_loss_model(filepath= pretrained_model_name)\n",
    "                \n",
    "                # Get train and test split\n",
    "                with io.capture_output():\n",
    "                        # Get new session data\n",
    "                        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "                        \n",
    "                        # Get epochs for new session\n",
    "                        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                             start_offset= start_offset,\n",
    "                                                                             end_offset= end_offset,\n",
    "                                                                             baseline= baseline)\n",
    "                        \n",
    "                        # Only keep epochs from the MI tasks\n",
    "                        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                        \n",
    "                        # Fix the indexing\n",
    "                        # NOTE: this is some weird MNE behaviour, the index retreived is not the actual index if not reset due to filtering done\n",
    "                        mne_epochs.reset_drop_log_selection()\n",
    "                        \n",
    "                        # Load epochs into memory\n",
    "                        mne_epochs.load_data()\n",
    "                        \n",
    "                        # Get calibration test split\n",
    "                        calibration_items, test_items = CLA_dataset.get_calibration_test_split_from_epochs(epochs= mne_epochs,\n",
    "                                                                                                           amount_of_samples_in_calibration_per_class = 25)\n",
    "                        \n",
    "                        \n",
    "                # Get OHE from file\n",
    "                with open(f\"saved_variables/7/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "                        ohe = pickle.load(f)\n",
    "                        \n",
    "                        \n",
    "                calibration_epochs = mne_epochs[calibration_items]\n",
    "                test_epochs = mne_epochs[test_items]\n",
    "\n",
    "                # Get labels\n",
    "                y_train = calibration_epochs.events[:, -1]\n",
    "                y_test = test_epochs.events[:, -1]\n",
    "\n",
    "                # Get train and test data\n",
    "                X_train = calibration_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                X_test = test_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "\n",
    "                # Fix scaling sensitivity as MNE stores as data * 10e-6\n",
    "                X_train = X_train * 1000000\n",
    "                X_test = X_test * 1000000\n",
    "                \n",
    "                # Further devide the calibration split in train and eval samples\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                                  y_train,\n",
    "                                                                  test_size = 0.4, # Use 5 samples per class for validation\n",
    "                                                                  shuffle= True,\n",
    "                                                                  stratify= y_train,                                                    \n",
    "                                                                  random_state= 1998)\n",
    "                \n",
    "                # Convert labels with OHE for Keras\n",
    "                y_train = ohe.transform(y_train.reshape(-1, 1)).toarray()\n",
    "                y_test = ohe.transform(y_test.reshape(-1, 1)).toarray()\n",
    "                y_val = ohe.transform(y_val.reshape(-1, 1)).toarray()\n",
    "                \n",
    "                # Print stats\n",
    "                print(f\"Calibrating with {np.shape(X_train)} windows\")\n",
    "                print(f\"Testing with {np.shape(X_test)} windows\")\n",
    "                \n",
    "                print(\"Calibrating epochs\")\n",
    "                display(calibration_epochs)\n",
    "                \n",
    "                print(\"test epochs\")\n",
    "                display(test_epochs)\n",
    "                        \n",
    "                ################### CALIBRATE MODEL ###################\n",
    "                # Make a calibration model\n",
    "                calibration_model = keras.models.clone_model(keras_eegnet_lstm_1Dconv_model)\n",
    "\n",
    "                \n",
    "                # Copy weights from pretrained model\n",
    "                calibration_model.set_weights(pretrained_model.get_weights())\n",
    "                \n",
    "                for layer in calibration_model.layers:\n",
    "                        layer.trainable = False\n",
    "                \n",
    "                \n",
    "                # Allow last layers to train (softmax)\n",
    "                calibration_model.layers[11].trainable = True\n",
    "                calibration_model.layers[12].trainable = True\n",
    "                \n",
    "                # Allow batch norm to train due to weird behaviour\n",
    "                calibration_model.layers[2].trainable = True\n",
    "                calibration_model.layers[4].trainable = True\n",
    "                \n",
    "                \n",
    "                # Change dropout\n",
    "                calibration_model.layers[6].rate = 0\n",
    "                calibration_model.layers[9].rate = 0.2\n",
    "                \n",
    "                # Compile the model so it can be fitted, note a lower learning rate is set\n",
    "                calibration_model.compile(loss = 'categorical_crossentropy', optimizer = tf.optimizers.Adam(learning_rate= 0.0001), metrics=[\"accuracy\"])\n",
    "                \n",
    "                # Train model with GPU as means of recalibrating\n",
    "                # NOTE: change GPU to CPU if no GPU present\n",
    "                with tf.device('/gpu:0'):\n",
    "                        history = calibration_model.fit(\n",
    "                                x= X_train,\n",
    "                                y= y_train,\n",
    "                                batch_size= 128, # All calibration data at once\n",
    "                                epochs= 500, # Very small due to direct overfit expected\n",
    "                                verbose= 1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                                callbacks= [TF_tools.tensorboard_callback(log_name= tensorboard_name),\n",
    "                                        TF_tools.lowest_loss_model_save_callback(filepath= best_base_model_filename),\n",
    "                                        TF_tools.highest_accuracy_model_save_callback(filepath= best_base_model_filename)],\n",
    "                                validation_data= (X_val, y_val),\n",
    "                                shuffle= True,\n",
    "                                use_multiprocessing= True, # Done for faster speed\n",
    "                                workers= 4 # Done for faster speed\n",
    "                                )\n",
    "                        \n",
    "                # Store the fitting history\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/fitting_history.pickle\", 'wb') as file:\n",
    "                        pickle.dump(history.history, file)\n",
    "                        \n",
    "                # Store the test data\n",
    "                with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/X_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                        \n",
    "                with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/y_test.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "        \n",
    "                # remove unused vars\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del pretrained_model_name\n",
    "                del pretrained_model\n",
    "                del f\n",
    "                del X_test\n",
    "                del y_test\n",
    "                del ohe\n",
    "                del calibration_epochs\n",
    "                del calibration_items\n",
    "                del test_epochs\n",
    "                del test_items\n",
    "                del X_train\n",
    "                del y_train\n",
    "                del X_val\n",
    "                del y_val\n",
    "                del tensorboard_name\n",
    "                del history\n",
    "                del best_base_model_filename\n",
    "                del calibration_model\n",
    "                del file\n",
    "                del layer\n",
    "        \n",
    "        del subject_id\n",
    "        \n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset\n",
    "del keras_eegnet_lstm_1Dconv_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba7c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################\n",
      "# RESULTS FOR SUBJECT B\n",
      "####################################################\n",
      "\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "#### results of training ####\n",
      "Best training accuracy (max) 0.9333 @ epoch 303\n",
      "Best training loss (min) 0.5448 @ epoch 297\n",
      "\n",
      "Best validation accuracy (max) 0.7333 @ epoch 213\n",
      "Best validation loss (min) 0.8291 @ epoch 82\n",
      "\n",
      "#### results for highest accuracy model ####\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = None # Baseline correction using data before the visual queue\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print()\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print()\n",
    "    \n",
    "    ################### load data ###################\n",
    "    # Names for model\n",
    "    best_base_model_filename = f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/trained_model\"\n",
    "    \n",
    "    # Open models from file\n",
    "    lowest_loss_model = TF_tools.load_lowest_loss_model(filepath= best_base_model_filename)\n",
    "    highest_accuracy_model = TF_tools.load_highest_accuracy_model(filepath= best_base_model_filename)\n",
    "    \n",
    "    # Get test data session\n",
    "    with io.capture_output():\n",
    "        # Get test data\n",
    "        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id)\n",
    "        \n",
    "        # Get epochs for test MNE raw\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the test labels\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/y_test.pickle\", 'rb') as f:\n",
    "            y_test = pickle.load(f)\n",
    "            \n",
    "        # Get the test data\n",
    "        with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/X_test.pickle\", 'rb') as f:\n",
    "            X_test = pickle.load(f)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "        \n",
    "    # Get OHE from file\n",
    "    with open(f\"saved_variables/7/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/ohe-encoder.pickle\", 'rb') as f:\n",
    "        ohe = pickle.load(f)\n",
    "        \n",
    "    # Get history from file\n",
    "    with open(f\"saved_variables/8/calibration/EEGNet_lstmconv1D/samesubject_differentsession/subject{subject_id}/fitting_history.pickle\", 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Inverse y_test to label\n",
    "    y_test = ohe.inverse_transform(y_test)\n",
    "    \n",
    "    ################### history stats ###################\n",
    "    print(\"#### results of training ####\")\n",
    "    print(f\"Best training accuracy (max) {np.round(np.max(history['accuracy']), 4)} @ epoch {np.argmax(history['accuracy']) + 1}\")\n",
    "    print(f\"Best training loss (min) {np.round(np.min(history['loss']), 4)} @ epoch {np.argmin(history['loss']) + 1}\")\n",
    "    print()\n",
    "    print(f\"Best validation accuracy (max) {np.round(np.max(history['val_accuracy']), 4)} @ epoch {np.argmax(history['val_accuracy']) + 1}\")\n",
    "    print(f\"Best validation loss (min) {np.round(np.min(history['val_loss']), 4)} @ epoch {np.argmin(history['val_loss']) + 1}\")\n",
    "    \n",
    "    ################### highest accuracy model ###################\n",
    "    print(\"\\n#### results for highest accuracy model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = highest_accuracy_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### lowest loss model ###################\n",
    "    print(\"\\n#### results for lowest loss model ####\")\n",
    "    # Get predictions from lowest loss model and convert back to labels\n",
    "    y_pred = lowest_loss_model.predict(X_test)\n",
    "    y_pred = ohe.inverse_transform(y_pred)\n",
    "    \n",
    "    # Get accuracy score and print it\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of: {np.round(accuracy, 4)}\")\n",
    "    \n",
    "    # Show CM\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    ################### cleanup ###################\n",
    "    # remove unused vars\n",
    "    del best_base_model_filename\n",
    "    del lowest_loss_model\n",
    "    del highest_accuracy_model\n",
    "    del f\n",
    "    del X_test\n",
    "    del y_test\n",
    "    del history\n",
    "    del ohe\n",
    "    del y_pred\n",
    "    del accuracy\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del subject_id\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000c54",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Subsampling electrodes\n",
    "\n",
    "According to Kaya et al., the authors of the used dataset, the channels of most interest are C3, C4, T3, T4 and Cz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82add91f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92ed28e6a5fe026f22555c18fed88052bb861e5576fb72d2ac78e2247fef331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
