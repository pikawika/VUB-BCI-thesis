{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Standard CSP pipelines\n",
    "\n",
    "This notebook implements multiple standard CSP pipelines and tests their performance on the data from the database provided by [Kaya et al.](https://doi.org/10.1038/sdata.2018.211).\n",
    "The knowledge and utilities obtained from the experimental notebooks four to five are used throughout this notebook.\n",
    "\n",
    "This notebook works in an offline fashion and uses epochs with a length of 3 seconds.\n",
    "This epoch starts 1 second before the visual queue was given, includes the 1 second the visual queue was shown and ends 1 second after the visual queue was hidden, totalling 3 seconds.\n",
    "Baseline correction was done on the first second of the epoch, meaning the second before the visual queue was shown.\n",
    "The effective training and testing are done on a 2-second window, starting 0.5 seconds before the 1-second visual queue and ending 0.5 seconds after this visual queue.\n",
    "A window of 2 seconds was chosen as it is a common size for sliding window approaches in online systems.\n",
    "\n",
    "\n",
    "Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. We will use the utility file `bci-master-thesis/code/utils/CLA_dataset.py` to work with this data. The data was stored as FIF files, which are included in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "   - Correct Anaconda environment\n",
    "   - Correct module access\n",
    "   - Correct file access\n",
    "- Same subject, same session\n",
    "   - Same subject, same session: LDA classifier \n",
    "- Same subject, new session\n",
    "- New subject\n",
    "- New subject with calibration\n",
    "- Cleaning resedual notebook variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct Anaconda environment\n",
    "\n",
    "The `bci-master-thesis` Anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: bci-master-thesis\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE version (1.0.2 recommended): 1.0.2\n",
      "Scikit-learn version (1.0.2 recommended): 1.0.2\n",
      "Numpy version (1.21.5 recommended): 1.21.5\n",
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pickle version (4.0 recommended): 4.0\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. The following code block checks if all required files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matlab CLA file access: True\n",
      "Full MNE CLA file access: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Same subject, same session\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, using a singular session and testing on that same session.\n",
    "This is an over-optimistic testing scenario and has a great risk of overfitting with poor generalisation to new sessions or new subjects but can be an okay baseline test to see if *at least something* can be learned.\n",
    "We do this for three different traditional machine learning classifiers: linear discriminant analysis (LDA), support vector machines (SVM) and random forest (RF).\n",
    "K-nearest neighbour (KNN) is not considered as it is too time-consuming in predictions and complex models such as a multilayer perceptron (MLP) are not considered either as they are an integral part of the deep learning models considered in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14c042",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: LDA classifier\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the last recorded session of each of these participants, thus the one where the participant has the most experience\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use a two-second window including 0.5 seconds before and after the visual queue for training\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the 1-second windows of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 35Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> LDA\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 2 | 3 | 4 | 6 | 10\n",
    "         - For LDA:\n",
    "            - The optimizer: svd | lsqr | eigen\n",
    "            - When using SVD optimizer, the tol: 0.0001 | 0.00001 | 0.001 | 0.0004 | 0.00007 \n",
    "   - We use the test split for final validation on the best-found parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd2eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below 2Hz \n",
    "filter_upper_bound = 35 # Filter out any frequency above 35Hz\n",
    "\n",
    "# Loop over all subjects and perform the grid search for finding the best parameters\n",
    "for subject_id in subject_ids_to_test[:1]:\n",
    "    # Get MNE raw object for latest recording of that subject\n",
    "    mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "    # Get epochs for that MNE raw\n",
    "    mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                         start_offset= start_offset,\n",
    "                                                         end_offset= end_offset,\n",
    "                                                         baseline= baseline)\n",
    "    \n",
    "    # Only keep epochs from the MI tasks\n",
    "    mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "    # Load epochs into memory\n",
    "    mne_epochs.load_data()\n",
    "    \n",
    "    # Get the labels\n",
    "    labels = mne_epochs.events[:, -1]\n",
    "    \n",
    "    # Use a fixed filter\n",
    "    mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                      h_freq= filter_upper_bound,\n",
    "                      picks= \"all\",\n",
    "                      phase= \"minimum\",\n",
    "                      fir_window= \"blackman\",\n",
    "                      fir_design= \"firwin\",\n",
    "                      pad= 'median', \n",
    "                      n_jobs= -1,\n",
    "                      verbose= False)\n",
    "    \n",
    "    # Create a test and train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mne_epochs,\n",
    "                                                        labels,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        shuffle= True,\n",
    "                                                        stratify= labels,                                                    \n",
    "                                                        random_state= 1998)\n",
    "    \n",
    "    # Configure the pipeline components by specifying the default parameters\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\")\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                     priors=[1/3, 1/3, 1/3])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "    \n",
    "    # Configure cross validation to use\n",
    "    cv = StratifiedKFold(n_splits=4,\n",
    "                         shuffle= True,\n",
    "                         random_state= 2022)\n",
    "    \n",
    "    # Configure the hyperparameters to test\n",
    "    # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "    param_grid = [{\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                   \"LDA__solver\": [\"svd\"],\n",
    "                   \"LDA__tol\": [0.0001, 0.00001, 0.001, 0.0004, 0.00007]\n",
    "                   },\n",
    "                  {\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                   \"LDA__solver\": [\"lsqr\" , \"eigen\"]\n",
    "                   }]\n",
    "    \n",
    "    # Configure the grid search\n",
    "    grid_search = GridSearchCV(estimator= pipeline,\n",
    "                               param_grid= param_grid,\n",
    "                               scoring= \"balanced_accuracy\",\n",
    "                               n_jobs= -1,\n",
    "                               refit= True,\n",
    "                               cv= cv,\n",
    "                               verbose= 10,\n",
    "                               return_train_score= True)\n",
    "\n",
    "    # Do the grid search on the training data\n",
    "    grid_search.fit(X= X_train, \n",
    "                    y= y_train)\n",
    "    \n",
    "    # Store the results of the grid search\n",
    "    with open(f\"saved_variables/4/gridsearch_samesubject_samesession_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "            pickle.dump(grid_search, file)\n",
    "    \n",
    "    # Store the best model and the test data\n",
    "    with open(f\"saved_variables/4/bestmodel_samesubject_samesession_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "            pickle.dump(grid_search.best_estimator_, file)\n",
    "    with open(f\"saved_variables/4/testdata-x_samesubject_samesession_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "            pickle.dump(X_test, file)\n",
    "    with open(f\"saved_variables/4/testdata-y_samesubject_samesession_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "            pickle.dump(y_test, file)\n",
    "    \n",
    "    # Delete vars after singular experiment\n",
    "    del mne_raw\n",
    "    del mne_epochs\n",
    "    del csp\n",
    "    del lda\n",
    "    del pipeline\n",
    "    del labels\n",
    "    del cv\n",
    "    del file\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del y_train\n",
    "    del y_test \n",
    "    del grid_search\n",
    "    del param_grid\n",
    "    \n",
    "# Delete vars after all experiments\n",
    "del baseline\n",
    "del end_offset\n",
    "del start_offset\n",
    "del subject_id\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Same subject, new session\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: foresee as needed in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## New subject\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: foresee as needed in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## New subject with calibration\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: foresee as needed in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Cleaning resedual notebook variables\n",
    "\n",
    "This last codeblock cleans any resedual notebook variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN NOTEBOOK VARIABLES\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b02c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('bci-master-thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92ed28e6a5fe026f22555c18fed88052bb861e5576fb72d2ac78e2247fef331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
