{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Standard CSP pipelines\n",
    "\n",
    "This notebook implements multiple standard CSP pipelines and tests their performance on the data from the database provided by [Kaya et al.](https://doi.org/10.1038/sdata.2018.211).\n",
    "The knowledge and utilities obtained from the experimental notebooks four to five are used throughout this notebook.\n",
    "\n",
    "This notebook works in an offline fashion and uses epochs with a length of 3 seconds.\n",
    "This epoch starts 1 second before the visual queue was given, includes the 1 second the visual queue was shown and ends 1 second after the visual queue was hidden, totalling 3 seconds.\n",
    "Baseline correction was done on the first second of the epoch, meaning the second before the visual queue was shown.\n",
    "The effective training and testing are done on a 2-second window, starting 0.5 seconds before the 1-second visual queue and ending 0.5 seconds after this visual queue.\n",
    "A window of 2 seconds was chosen as it is a common size for sliding window approaches in online systems.\n",
    "\n",
    "\n",
    "Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. We will use the utility file `bci-master-thesis/code/utils/CLA_dataset.py` to work with this data. The data was stored as FIF files, which are included in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "   - Correct Anaconda environment\n",
    "   - Correct module access\n",
    "   - Correct file access\n",
    "- Same subject, same session\n",
    "   - Same subject, same session: LDA classifier \n",
    "   - Same subject, same session: SVM classifier \n",
    "   - Same subject, same session: RF classifier \n",
    "- Same subject, new session\n",
    "   - Same subject, new session: LDA classifier\n",
    "- New subject\n",
    "- Cleaning resedual notebook variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct Anaconda environment\n",
    "\n",
    "The `bci-master-thesis` Anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: bci-master-thesis\n",
      "Correct environment: True\n",
      "\n",
      "Python version: 3.8.10\n",
      "Correct Python version: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab632204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE version (1.0.2 recommended): 1.0.2\n",
      "Scikit-learn version (1.0.2 recommended): 1.0.2\n",
      "Numpy version (1.21.5 recommended): 1.21.5\n",
      "Pandas version (1.4.1 recommended): 1.4.1\n",
      "Matplotlib version (3.5.1 recommended): 3.5.1\n",
      "Pickle version (4.0 recommended): 4.0\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. The following code block checks if all required files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matlab CLA file access: True\n",
      "Full MNE CLA file access: True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Same subject, same session\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, using a singular session and testing on that same session.\n",
    "This is an over-optimistic testing scenario and has a great risk of overfitting with poor generalisation to new sessions or new subjects but can be an okay baseline test to see if *at least something* can be learned.\n",
    "We do this for three different traditional machine learning classifiers: linear discriminant analysis (LDA), support vector machines (SVM) and random forest (RF).\n",
    "K-nearest neighbour (KNN) is not considered as it is too time-consuming in predictions and complex models such as a multilayer perceptron (MLP) are not considered either as they are an integral part of the deep learning models considered in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14c042",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: LDA classifier\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the last recorded session of each of these participants, thus the one where the participant has the most experience\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> LDA\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 2 | 3 | 4 | 6 | 10\n",
    "         - For LDA:\n",
    "            - The optimizer: svd | lsqr | eigen\n",
    "            - When using SVD optimizer, the tol: 0.0001 | 0.00001 | 0.001 | 0.0004 | 0.00007 \n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = False # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                # Get MNE raw object for latest recording of that subject\n",
    "                mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                # Get epochs for that MNE raw\n",
    "                mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                     start_offset= start_offset,\n",
    "                                                                     end_offset= end_offset,\n",
    "                                                                     baseline= baseline)\n",
    "                \n",
    "                # Only keep epochs from the MI tasks\n",
    "                mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "                # Load epochs into memory\n",
    "                mne_epochs.load_data()\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                h_freq= filter_upper_bound,\n",
    "                                picks= \"all\",\n",
    "                                phase= \"minimum\",\n",
    "                                fir_window= \"blackman\",\n",
    "                                fir_design= \"firwin\",\n",
    "                                pad= 'median', \n",
    "                                n_jobs= -1,\n",
    "                                verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Create a test and train split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(mne_epochs_data,\n",
    "                                                                    labels,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    shuffle= True,\n",
    "                                                                    stratify= labels,                                                    \n",
    "                                                                    random_state= 1998)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                                 priors=[1/3, 1/3, 1/3])\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "                \n",
    "                # Configure cross validation to use\n",
    "                cv = StratifiedKFold(n_splits=4,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                               \"LDA__solver\": [\"svd\"],\n",
    "                               \"LDA__tol\": [0.0001, 0.00001, 0.001, 0.0004, 0.00007]\n",
    "                               },\n",
    "                              {\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                               \"LDA__solver\": [\"lsqr\" , \"eigen\"]\n",
    "                               }]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= X_train, \n",
    "                                y= y_train)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Store the train and test data so the best model can be retrained later\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-x_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-y_svm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-x_svm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_train, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-y_csplda_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_train, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del lda\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test \n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2d360",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| **Subject** | **CSP + LDA: cross validation accuracy** | **CSP + LDA: test split accuracy** | **Config**                                          |\n",
    "|-------------|------------------------------------------|------------------------------------|-----------------------------------------------------|\n",
    "| B           | 0.6615 +- 0.0504                         | 0.6094                             | 6 CSP components \\| LDA SVD solver with 0.0001 tol  |\n",
    "| C           | 0.7144 +- 0.0341                         | 0.7240                             | 10 CSP components \\| LDA SVD solver with 0.0001 tol |\n",
    "| E           | 0.7342 +- 0.0171                         | 0.7277                             | 10 CSP components \\| LDA SVD solver with 0.0001 tol |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13b5b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_csplda_subject{subject_id}.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {grid_search.best_score_} with parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2667921",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "best_found_csp_components = [4, 10 , 10]\n",
    "best_found_solver = [\"svd\", \"svd\", \"svd\"]\n",
    "best_found_tol = [0.0001, 0.0001, 0.0001]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open train and test data from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-x_csplda_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-y_csplda_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-x_csplda_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-y_csplda_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                     priors=[1/3, 1/3, 1/3],\n",
    "                                     solver= best_found_solver[i],\n",
    "                                     tol= best_found_tol[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {accuracy}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_solver\n",
    "del best_found_tol\n",
    "del i\n",
    "del f\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del lda\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e9a16",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: SVM classifier\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the last recorded session of each of these participants, thus the one where the participant has the most experience\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> SVM\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 4 | 6 | 10\n",
    "         - For SVM:\n",
    "            - The C: 0.01 | 0.1 | 1 | 10 | 100\n",
    "            - The kernel: rbf | sigmoid | linear\n",
    "            - When using the rbf of sigmoid kernel, the gamma: scale | auto | 10 | 1 | 0.1 | 0.01 | 0.001\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b557997",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = False # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                # Get MNE raw object for latest recording of that subject\n",
    "                mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                # Get epochs for that MNE raw\n",
    "                mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                     start_offset= start_offset,\n",
    "                                                                     end_offset= end_offset,\n",
    "                                                                     baseline= baseline)\n",
    "                \n",
    "                # Only keep epochs from the MI tasks\n",
    "                mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "                # Load epochs into memory\n",
    "                mne_epochs.load_data()\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                h_freq= filter_upper_bound,\n",
    "                                picks= \"all\",\n",
    "                                phase= \"minimum\",\n",
    "                                fir_window= \"blackman\",\n",
    "                                fir_design= \"firwin\",\n",
    "                                pad= 'median', \n",
    "                                n_jobs= -1,\n",
    "                                verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Create a test and train split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(mne_epochs_data,\n",
    "                                                                    labels,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    shuffle= True,\n",
    "                                                                    stratify= labels,                                                    \n",
    "                                                                    random_state= 1998)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                svm = SVC()\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('SVM', svm)])\n",
    "                \n",
    "                # Configure cross validation to use\n",
    "                cv = StratifiedKFold(n_splits=4,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\n",
    "                        \"CSP__n_components\": [4, 6, 10],\n",
    "                        \"SVM__C\": [0.01, 0.1, 1, 10, 100],\n",
    "                        \"SVM__kernel\": ['rbf', 'sigmoid'],\n",
    "                        \"SVM__gamma\":['scale', 'auto', 10, 1, 0.1, 0.01, 0.001]}\n",
    "                              ,{\n",
    "                        \"CSP__n_components\": [4, 6, 10],\n",
    "                        \"SVM__C\": [0.01, 0.1, 1, 10, 100],\n",
    "                        \"SVM__kernel\": ['linear']}]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= X_train, y= y_train)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_cspsvm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Store the train and test data so the best model can be retrained later\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-x_cspsvm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-y_cspsvm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-x_cspsvm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_train, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-y_cspsvm_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_train, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del svm\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test \n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cece0d",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| **Subject** | **CSP + LDA: cross validation accuracy** | **CSP + LDA: test split accuracy** | **Config**                                                |\n",
    "|-------------|------------------------------------------|------------------------------------|-----------------------------------------------------------|\n",
    "| B           | 0.6693 +- 0.02981                        | 0.6146                             | 4 CSP components \\| SVM RBF with C 0.1 and Gamma auto     |\n",
    "| C           | 0.7262 +- 0.0298                         | 0.7448                             | 6 CSP components \\| SVM RBF with C 100 and Gamma 0.001    |\n",
    "| E           | 0.7356 +- 0.0159                         | 0.7016                             | 6 CSP components \\| SVM sigmoid with C 100 and Gamma 0.01 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8fb2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_cspsvm_subject{subject_id}.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {grid_search.best_score_} with parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "best_found_csp_components = [4, 6 , 6]\n",
    "best_found_svm_kernel = [\"rbf\", \"rbf\", \"sigmoid\"]\n",
    "best_found_svm_c = [0.1, 100, 100]\n",
    "best_found_svm_gamma = [\"auto\", 0.001, 0.01]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open train and test data from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-x_cspsvm_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-y_cspsvm_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-x_cspsvm_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-y_cspsvm_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    svm = SVC(kernel= best_found_svm_kernel[i],\n",
    "              C= best_found_svm_c[i],\n",
    "              gamma= best_found_svm_gamma[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('SVM', svm)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {accuracy}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_svm_kernel\n",
    "del best_found_svm_c\n",
    "del best_found_svm_gamma\n",
    "del i\n",
    "del f\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del svm\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f198a04",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: RF classifier\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the last recorded session of each of these participants, thus the one where the participant has the most experience\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only half a second window taking into account that the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after the visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies of 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> RF\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 4 | 6 | 10\n",
    "         - For RF:\n",
    "            - Bootstrap is always set to True as it is, besides random feature subsets, one of the ways to reduce data biases during training\n",
    "            - The metric used for splitting criterion is gini as it is faster than entropy and the performance difference is negligible when looking at the experimental notebooks\n",
    "            - Number of estimators: 10 | 50 | 100 | 250 | 500\n",
    "            - Max depth of a tree: None | 3 | 10 | 20 | 35 | 50 | 75 | 100\n",
    "            - Minimum number of samples to do a split: 2 | 5 | 10\n",
    "            - Minimum samples to have a leaf is not set as it is related to the number of samples per split and max depth of the tree\n",
    "            - Maximum features per tree: sqrt | log2 | None\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = False # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                # Get MNE raw object for latest recording of that subject\n",
    "                mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                # Get epochs for that MNE raw\n",
    "                mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                     start_offset= start_offset,\n",
    "                                                                     end_offset= end_offset,\n",
    "                                                                     baseline= baseline)\n",
    "                \n",
    "                # Only keep epochs from the MI tasks\n",
    "                mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "                # Load epochs into memory\n",
    "                mne_epochs.load_data()\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                  h_freq= filter_upper_bound,\n",
    "                                  picks= \"all\",\n",
    "                                  phase= \"minimum\",\n",
    "                                  fir_window= \"blackman\",\n",
    "                                  fir_design= \"firwin\",\n",
    "                                  pad= 'median', \n",
    "                                  n_jobs= -1,\n",
    "                                  verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Create a test and train split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(mne_epochs_data,\n",
    "                                                                    labels,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    shuffle= True,\n",
    "                                                                    stratify= labels,                                                    \n",
    "                                                                    random_state= 1998)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                rf = RandomForestClassifier(bootstrap= True,\n",
    "                                            criterion= \"gini\")\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "                \n",
    "                # Configure cross validation to use\n",
    "                cv = StratifiedKFold(n_splits=4,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [4, 6, 10],\n",
    "                               \"RF__n_estimators\": [10, 50, 100, 250, 500],\n",
    "                               \"RF__max_depth\": [None, 3, 10],\n",
    "                               \"RF__min_samples_split\": [2, 5, 10],\n",
    "                               \"RF__max_features\": [\"sqrt\", \"log2\", \"None\", 0.2, 0.4, 0.6]}]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= X_train, y= y_train)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_csprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Store the train and test data so the best model can be retrained later\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-x_csprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/testdata-y_csprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-x_csprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_train, file)\n",
    "                with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/traindata-y_csprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_train, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del rf\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test \n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb465ba",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| **Subject** | **CSP + LDA: cross validation accuracy** | **CSP + LDA: test split accuracy** | **Config**                                                                                   |\n",
    "|-------------|------------------------------------------|------------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| B           | 0.6588 +- 0.0316                         | 0.6042                             | 4 CSP components \\| RF max depth 10, max features 0.4, min sample split 10, 50 estimators    |\n",
    "| C           | 0.7119 +- 0.0316                         | 0.7031                             | 6 CSP components \\| RF max depth 3, max features 0.4, min sample split 5, 250 estimators     |\n",
    "| E           | 0.7251 +- 0.0176                         | 0.7539                             | 10 CSP components \\| RF max depth None, max features 0.2, min sample split 2, 250 estimators |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ca6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_csprf_subject{subject_id}.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {grid_search.best_score_} with parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "best_found_csp_components = [4, 6 , 10]\n",
    "best_found_rf_depth = [10, 3, None]\n",
    "best_found_rf_max_featues = [0.4, 0.4, 0.2]\n",
    "best_found_rf_min_sample = [10, 5, 2]\n",
    "best_found_rf_n_estimators = [50, 250, 250]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open train and test data from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-x_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-y_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-x_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-y_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    rf = RandomForestClassifier(bootstrap= True,\n",
    "                                criterion= \"gini\",\n",
    "                                max_depth= best_found_rf_depth[i],\n",
    "                                max_features= best_found_rf_max_featues[i],\n",
    "                                min_samples_split= best_found_rf_min_sample[i],\n",
    "                                n_estimators= best_found_rf_n_estimators[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {accuracy}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_rf_depth\n",
    "del best_found_rf_max_featues\n",
    "del best_found_rf_min_sample\n",
    "del best_found_rf_n_estimators\n",
    "del i\n",
    "del f\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del rf\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b08c0",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Same subject, new session\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, but by using one or more sessions for training and testing on a new, unseen session.\n",
    "This is a harder task than the previous one, where training and testing were done for the same session.\n",
    "This section will train the same classifiers for the same participants as before but by using the first two datasets as training data and the third and final session of each participant as a standalone test set which is not used in training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff511998",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, new session: LDA classifier\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the first two recorded session of each of these participants for training and the last for testing.\n",
    "      - Thus, the CV scores are on the test split for the training data whilst the independent test set is from the unseen session not used during training. This avoids data leakage.\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> LDA\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 2 | 3 | 4 | 6 | 10\n",
    "         - For LDA:\n",
    "            - The optimizer: svd | lsqr | eigen\n",
    "            - When using SVD optimizer, the tol: 0.0001 | 0.00001 | 0.001 | 0.0004 | 0.00007 \n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0690c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 729399  =      0.000 ...  3646.995 secs...\n",
      "Reading 0 ... 667399  =      0.000 ...  3336.995 secs...\n",
      "Reading 0 ... 667799  =      0.000 ...  3338.995 secs...\n",
      "Using data from preloaded Raw for 1918 events and 601 original time points ...\n",
      "0 bad epochs dropped\n",
      "Fitting 6 folds for each of 35 candidates, totalling 210 fits\n",
      "Reading 0 ... 729399  =      0.000 ...  3646.995 secs...\n",
      "Reading 0 ... 667399  =      0.000 ...  3336.995 secs...\n",
      "Reading 0 ... 667799  =      0.000 ...  3338.995 secs...\n",
      "Using data from preloaded Raw for 1918 events and 601 original time points ...\n",
      "0 bad epochs dropped\n",
      "Fitting 6 folds for each of 35 candidates, totalling 210 fits\n",
      "Reading 0 ... 729399  =      0.000 ...  3646.995 secs...\n",
      "Reading 0 ... 667399  =      0.000 ...  3336.995 secs...\n",
      "Reading 0 ... 667799  =      0.000 ...  3338.995 secs...\n",
      "Using data from preloaded Raw for 1918 events and 601 original time points ...\n",
      "0 bad epochs dropped\n",
      "Fitting 6 folds for each of 35 candidates, totalling 210 fits\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = True # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                # Get all training data (all but last session of participant)\n",
    "                mne_raws= CLA_dataset.get_all_but_last_raw_mne_data_for_subject(subject_id= \"B\")\n",
    "                \n",
    "                # Combine training data into singular mne raw\n",
    "                mne_raw = mne.concatenate_raws(mne_raws)\n",
    "                \n",
    "                # Delete all raws since concat changes them\n",
    "                del mne_raws\n",
    "                \n",
    "                # Get epochs for all those MNE raws (all training sessions)\n",
    "                mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                     start_offset= start_offset,\n",
    "                                                                     end_offset= end_offset,\n",
    "                                                                     baseline= baseline)\n",
    "                \n",
    "                # Only keep epochs from the MI tasks\n",
    "                mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "                # Load epochs into memory\n",
    "                mne_epochs.load_data()\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                  h_freq= filter_upper_bound,\n",
    "                                  picks= \"all\",\n",
    "                                  phase= \"minimum\",\n",
    "                                  fir_window= \"blackman\",\n",
    "                                  fir_design= \"firwin\",\n",
    "                                  pad= 'median', \n",
    "                                  n_jobs= -1,\n",
    "                                  verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                                 priors=[1/3, 1/3, 1/3])\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "                \n",
    "                # Configure cross validation to use, more splits then before since we have more data\n",
    "                cv = StratifiedKFold(n_splits= 6,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                               \"LDA__solver\": [\"svd\"],\n",
    "                               \"LDA__tol\": [0.0001, 0.00001, 0.001, 0.0004, 0.00007]\n",
    "                               },\n",
    "                              {\"CSP__n_components\": [2, 3, 4, 6, 10],\n",
    "                               \"LDA__solver\": [\"lsqr\" , \"eigen\"]\n",
    "                               }]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= mne_epochs_data, y= labels)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/samesubject_differentsession/subject{subject_id}/gridsearch_csplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del lda\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e68a5e",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## New subject\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: foresee as needed in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df8105",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Cleaning resedual notebook variables\n",
    "\n",
    "This last codeblock cleans any resedual notebook variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CLEAN NOTEBOOK VARIABLES\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b02c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92ed28e6a5fe026f22555c18fed88052bb861e5576fb72d2ac78e2247fef331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
