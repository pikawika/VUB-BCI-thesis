{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Advanced CSP pipelines\n",
    "\n",
    "This notebook implements more advanced CSP pipelines and tests their performance on the data from the database provided by [Kaya et al.](https://doi.org/10.1038/sdata.2018.211).\n",
    "The knowledge and utilities obtained from the previous paper notebook 2 and the experimental notebooks four to five are used throughout this notebook.\n",
    "Due to the small variations in accuracy found between the tested classifiers in the paper notebook 2, we opt to use an LDA approach in this noteboook. \n",
    "This has the pro that LDA doesn't actually require hyperparameter tuning, saving a lot of computational time.\n",
    "\n",
    "This notebook works in an offline fashion and uses epochs with a length of 3 seconds.\n",
    "This epoch starts 1 second before the visual queue was given, includes the 1 second the visual queue was shown and ends 1 second after the visual queue was hidden, totalling 3 seconds.\n",
    "Baseline correction was done on the first second of the epoch, meaning the second before the visual queue was shown.\n",
    "The effective training and testing are done in a half-second window, starting 0.1 seconds after the start of the visual queue.\n",
    "A window of 0.5 seconds was chosen as it is a common size for sliding window approaches in online systems.\n",
    "Alternative experiments include longer windows, such as 1.5 seconds windows.\n",
    "\n",
    "Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. We will use the utility file `bci-master-thesis/code/utils/CLA_dataset.py` to work with this data. The data was stored as FIF files, which are included in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "   - Correct Anaconda environment\n",
    "   - Correct module access\n",
    "   - Correct file access\n",
    "- Learned filter thresholds\n",
    "   - Creation of custom SKLearn pipeline component for filter\n",
    "   - Same subject, same session: LDA classifier\n",
    "   - Same subject, new session: LDA classifier\n",
    "   - New subject: LDA classifier\n",
    "- FBCSP approaches\n",
    "   - Same subject, same session: LDA classifier\n",
    "   - Same subject, new session: LDA classifier\n",
    "   - New subject: LDA classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct Anaconda environment\n",
    "\n",
    "The `bci-master-thesis` Anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab632204",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "import copy\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "# Custom SKlearn components\n",
    "import custom_sklearn_components\n",
    "from custom_sklearn_components import EpochsToFilteredData\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. The following code block checks if all required files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Learned filter thresholds\n",
    "\n",
    "The previous paper notebook, notebook `2-standard-csp-pipelines`, made use of a fixed filter aproach, namely an overlap-add FIR filter to obtain the frequencies 2Hz to 32Hz.\n",
    "These thresholds will be learned in this experiment by using grid search for them, this should fit the frequencies to the subject(s) in question.\n",
    "To do this, a custom sklearn pipeline component is made from the previously used filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b846ee1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Creation of custom SKLearn pipeline component for filter\n",
    "\n",
    "Since we want to tune the lower and upper bound of the filter using gridsearch, we need to make the filter available as a function that is understood by SKLearn.\n",
    "This creation is done in the provided util file `custom_sklearn_components`.\n",
    "The below code block tests its implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST FILTER\n",
    "####################################################\n",
    "\n",
    "with io.capture_output():\n",
    "    # Get MNE raw object for latest recording of subject B\n",
    "    mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= \"B\")\n",
    "    # Get epochs for that MNE raw\n",
    "    mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                         start_offset= -1,\n",
    "                                                         end_offset= 1,\n",
    "                                                         baseline= (None, 0))\n",
    "    # mne raw not needed anymore\n",
    "    del mne_raw\n",
    "    \n",
    "    # Only keep epochs from the MI tasks\n",
    "    mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "    # Load epochs into memory\n",
    "    mne_epochs.load_data()\n",
    "\n",
    "# Create copies for testing both approaches\n",
    "manual_conversion = copy.deepcopy(mne_epochs)\n",
    "automatic_conversion = copy.deepcopy(mne_epochs)\n",
    "\n",
    "# Do the manuel conversion\n",
    "with io.capture_output():\n",
    "    manual_conversion.filter(l_freq= 8,\n",
    "                      h_freq= 30,\n",
    "                      picks= \"all\",\n",
    "                      phase= \"minimum\",\n",
    "                      fir_window= \"blackman\",\n",
    "                      fir_design= \"firwin\",\n",
    "                      pad= 'median', \n",
    "                      n_jobs= -1,\n",
    "                      verbose= False)\n",
    "    # Get a half second window\n",
    "    manual_result = manual_conversion.get_data(tmin=0.1, tmax=0.6)\n",
    "print(f\"Manual conversion gave the following output shape: {np.shape(manual_result)}\")\n",
    "\n",
    "# Do the automatic conversion\n",
    "automatic_conversion = automatic_conversion.get_data() # Works on data rather then epoch object\n",
    "custom_filter = EpochsToFilteredData(start_offset = -1,\n",
    "                                     data_tmin = 0.1,\n",
    "                                     data_tmax = 0.6,\n",
    "                                     sfreq= 200,\n",
    "                                     filter_lower_bound= 8,\n",
    "                                     filter_upper_bound= 30)\n",
    "custom_filter.fit(automatic_conversion)\n",
    "automatic_result = custom_filter.transform(automatic_conversion)\n",
    "print(f\"Automatic conversion gave the following output shape: {np.shape(automatic_result)}\")\n",
    "\n",
    "# Compare results\n",
    "manual_result = np.array(manual_result)\n",
    "automatic_result = np.array(automatic_result)\n",
    "if ((automatic_result.shape == manual_result.shape) and (automatic_result == manual_result).all() ):\n",
    "    match = True\n",
    "else:\n",
    "    match = False\n",
    "print(f\"results are equal: {match}\")\n",
    "\n",
    "# Remove unused variables\n",
    "del automatic_conversion\n",
    "del automatic_result\n",
    "del manual_conversion\n",
    "del manual_result\n",
    "del custom_filter\n",
    "del match\n",
    "del mne_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f198a04",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: LDA classifier\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, using a singular session and testing on that same session.\n",
    "This is an over-optimistic testing scenario and has a great risk of overfitting with poor generalisation to new sessions or new subjects but can be an okay baseline test to see if *at least something* can be learned.\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the first two recorded session of each of these participants for training and the last for testing.\n",
    "      - Thus, the CV scores are on the test split for the training data whilst the independent test set is from the unseen part of the session not used during training. This avoids data leakage.\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - Filter -> CSP -> LDA\n",
    "      - The following hyperparameters are tested\n",
    "         - For the filter:\n",
    "            - Lower threshold: 1 | 2 | 4 | 6 | 8 | 10 \n",
    "            - Upper threshold: 26 | 28 | 30 | 32 | 34\n",
    "         - For CSP:\n",
    "            - Number of components: 4 | 6 | 10\n",
    "         - For LDA:\n",
    "            - The optimizer: svd | lsqr\n",
    "            - When using SVD optimizer, the tol is set to 0.0001, which is the default and was the best for all previous experiments\n",
    "            - Shrinkage is fixed to None\n",
    "            - the priors are initialized to a balanced 1/3\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "do_experiment = True # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                with io.capture_output():\n",
    "                        # Get MNE raw object for latest recording of that subject\n",
    "                        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                        # Get epochs for that MNE raw\n",
    "                        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                             start_offset= start_offset,\n",
    "                                                                             end_offset= end_offset,\n",
    "                                                                             baseline= baseline)\n",
    "                        \n",
    "                        # Only keep epochs from the MI tasks\n",
    "                        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                        \n",
    "                        # Load epochs into memory\n",
    "                        mne_epochs.load_data()\n",
    "                        \n",
    "                        # Get the labels\n",
    "                        labels = mne_epochs.events[:, -1]\n",
    "                        \n",
    "                        # Create a test and train split\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(mne_epochs,\n",
    "                                                                            labels,\n",
    "                                                                            test_size = 0.2,\n",
    "                                                                            shuffle= True,\n",
    "                                                                            stratify= labels,                                                    \n",
    "                                                                            random_state= 1998)\n",
    "                        \n",
    "                        # make split back to MNE epoch object and then retreive only the data\n",
    "                        X_train = mne.concatenate_epochs(X_train).get_data()\n",
    "                        X_test = mne.concatenate_epochs(X_test).get_data()\n",
    "                        \n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                custom_filter = EpochsToFilteredData(start_offset = -1,\n",
    "                                                     data_tmin = 0.1,\n",
    "                                                     data_tmax = 0.6,\n",
    "                                                     sfreq= 200)\n",
    "                \n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                                 priors=[1/3, 1/3, 1/3])\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('filter', custom_filter), ('CSP', csp), ('LDA', lda)])\n",
    "                \n",
    "                # Configure cross validation to use\n",
    "                cv = StratifiedKFold(n_splits=4,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [4, 6, 10],\n",
    "                               \"LDA__solver\": [\"svd\"],\n",
    "                               \"LDA__tol\": [0.0001],\n",
    "                               \"filter__filter_lower_bound\": [1, 2, 4, 6, 8, 10],\n",
    "                               \"filter__filter_upper_bound\": [26, 28, 30, 32, 34],\n",
    "                               },\n",
    "                              {\"CSP__n_components\": [4, 6, 10],\n",
    "                               \"LDA__solver\": [\"lsqr\"],\n",
    "                               \"filter__filter_lower_bound\": [1, 2, 4, 6, 8, 10],\n",
    "                               \"filter__filter_upper_bound\": [26, 28, 30, 32, 34],\n",
    "                               }]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           n_jobs= -1,\n",
    "                                           return_train_score= True,\n",
    "                                           error_score=\"raise\")\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= X_train, y= y_train)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/gridsearch_autofreqcsplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Store the train and test data so the best model can be retrained later\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/testdata-x_autofreqcsplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/testdata-y_autofreqcsplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/traindata-x_autofreqcsplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_train, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/traindata-y_autofreqcsplda.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_train, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del csp\n",
    "                del lda\n",
    "                del custom_filter\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test \n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2d360",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| **Subject** | **CSP + LDA: cross validation accuracy** | **CSP + LDA: test split accuracy** | **Config**                                          |\n",
    "|-------------|------------------------------------------|------------------------------------|-----------------------------------------------------|\n",
    "| B           | 0.6615 +- 0.0504                         | 0.6094                             | 6 CSP components \\| LDA SVD solver with 0.0001 tol  |\n",
    "| C           | 0.7144 +- 0.0341                         | 0.7240                             | 10 CSP components \\| LDA SVD solver with 0.0001 tol |\n",
    "| E           | 0.7342 +- 0.0171                         | 0.7277                             | 10 CSP components \\| LDA SVD solver with 0.0001 tol |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ca6d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/gridsearch_autofreqcsplda.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {np.round(grid_search.best_score_, 4)} +- {np.round(grid_search.cv_results_['std_test_score'][grid_search.best_index_], 4)} with the following parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acec67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "best_found_csp_components = [4, 10 , 10]\n",
    "best_found_solver = [\"svd\", \"svd\", \"svd\"]\n",
    "best_found_tol = [0.0001, 0.0001, 0.0001]\n",
    "best_found_filter_lower_bound = [2, 2, 2]\n",
    "best_found_filter_upper_bound = [32, 32, 32]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open train and test data from file\n",
    "    with open(f\"saved_variables/3/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-x_autofreqcsplda.pickle\", 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/3/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-y_autofreqcsplda.pickle\", 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/3/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-x_autofreqcsplda.pickle\", 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(f\"saved_variables/3/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-y_autofreqcsplda.pickle\", 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        \n",
    "    # Make the classifier\n",
    "    custom_filter = EpochsToFilteredData(start_offset = -1,\n",
    "                                         data_tmin = 0.1,\n",
    "                                         data_tmax = 0.6,\n",
    "                                         sfreq= 200,\n",
    "                                         filter_lower_bound= best_found_filter_lower_bound[i],\n",
    "                                         filter_upper_bound= best_found_filter_upper_bound[i])\n",
    "    \n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(shrinkage= None,\n",
    "                                     priors=[1/3, 1/3, 1/3],\n",
    "                                     solver= best_found_solver[i],\n",
    "                                     tol= best_found_tol[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('filter', custom_filter), ('CSP', csp), ('LDA', lda)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {np.round(accuracy, 4)}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_solver\n",
    "del best_found_tol\n",
    "del best_found_filter_lower_bound\n",
    "del best_found_filter_upper_bound\n",
    "del i\n",
    "del f\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del lda\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('bci-master-thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "84064af20c740daff42ac3b3e9f22c9848b19c2b0c948f28e817991b26c67d86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
