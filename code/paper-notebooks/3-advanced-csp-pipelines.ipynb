{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337221df",
   "metadata": {},
   "source": [
    "# Advanced CSP pipelines\n",
    "\n",
    "This notebook implements more advanced CSP pipelines and tests their performance on the data from the database provided by [Kaya et al.](https://doi.org/10.1038/sdata.2018.211).\n",
    "The knowledge and utilities obtained from the previous paper notebook 2 and the experimental notebooks four to five are used throughout this notebook.\n",
    "Due to the, albeit limited, explainability RF offers and the minimal difference between classifiers in CSP approaches, only the RF classifier is considered here.\n",
    "\n",
    "This notebook works in an offline fashion and uses epochs with a length of 3 seconds.\n",
    "This epoch starts 1 second before the visual queue was given, includes the 1 second the visual queue was shown and ends 1 second after the visual queue was hidden, totalling 3 seconds.\n",
    "Baseline correction was done on the first second of the epoch, meaning the second before the visual queue was shown.\n",
    "The effective training and testing are done in a half-second window, starting 0.1 seconds after the start of the visual queue.\n",
    "A window of 0.5 seconds was chosen as it is a common size for sliding window approaches in online systems.\n",
    "Alternative experiments include longer windows, such as 1.5 seconds windows.\n",
    "\n",
    "Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. We will use the utility file `bci-master-thesis/code/utils/CLA_dataset.py` to work with this data. The data was stored as FIF files, which are included in [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341c6d",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Checking requirements\n",
    "   - Correct Anaconda environment\n",
    "   - Correct module access\n",
    "   - Correct file access\n",
    "- Learned filter thresholds\n",
    "   - Creation of custom SKLearn pipeline component for filter\n",
    "   - Same subject, same session: RF classifier\n",
    "   - Same subject, new session: RF classifier\n",
    "   - New subject: RF classifier\n",
    "- FBCSP approaches\n",
    "   - Same subject, same session: RF classifier\n",
    "   - Same subject, new session: RF classifier\n",
    "   - New subject: RF classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292165d3",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Checking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55ad17",
   "metadata": {},
   "source": [
    "### Correct Anaconda environment\n",
    "\n",
    "The `bci-master-thesis` Anaconda environment should be active to ensure proper support. Installation instructions are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CHECKING FOR RIGHT ANACONDA ENVIRONMENT\n",
    "####################################################\n",
    "\n",
    "import os\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Correct environment: {os.environ['CONDA_DEFAULT_ENV'] == 'bci-master-thesis'}\")\n",
    "print(f\"\\nPython version: {python_version()}\")\n",
    "print(f\"Correct Python version: {python_version() == '3.8.10'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166668",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct module access\n",
    "\n",
    "The following code block will load in all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab632204",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# LOADING MODULES\n",
    "####################################################\n",
    "\n",
    "# Load util function file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import CLA_dataset\n",
    "import custom_sklearn_components\n",
    "from custom_sklearn_components import EpochsToFilteredData\n",
    "\n",
    "# IO functions\n",
    "from IPython.utils import io\n",
    "import copy\n",
    "\n",
    "# Set logging level for MNE before loading MNE\n",
    "os.environ['MNE_LOGGING_LEVEL'] = 'WARNING'\n",
    "\n",
    "# Modules tailored for EEG data\n",
    "import mne; print(f\"MNE version (1.0.2 recommended): {mne.__version__}\")\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ML libraries\n",
    "import sklearn;  print(f\"Scikit-learn version (1.0.2 recommended): {sklearn.__version__}\")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "# Data manipulation modules\n",
    "import numpy as np; print(f\"Numpy version (1.21.5 recommended): {np.__version__}\")\n",
    "import pandas as pd; print(f\"Pandas version (1.4.1 recommended): {pd.__version__}\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib; print(f\"Matplotlib version (3.5.1 recommended): {matplotlib.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Storing files\n",
    "import pickle;  print(f\"Pickle version (4.0 recommended): {pickle.format_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb5de",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Correct file access\n",
    "\n",
    "As mentioned, this notebook uses a database provided by [Kaya et al](https://doi.org/10.1038/sdata.2018.211). The CLA dataset in particular. Instructions on where to get the data are available on [the GitHub repository of the BCI master thesis project](https://www.github.com/pikawika/bci-master-thesis). These instructions are under `bci-master-thesis/code/data/CLA/README.md`. The following code block checks if all required files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# CHECKING FILE ACCESS\n",
    "####################################################\n",
    "\n",
    "# Use util to determine if we have access\n",
    "print(\"Full Matlab CLA file access: \" + str(CLA_dataset.check_matlab_files_availability()))\n",
    "print(\"Full MNE CLA file access: \" + str(CLA_dataset.check_mne_files_availability()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdad109",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "## Learned filter thresholds\n",
    "\n",
    "The previous paper notebook, notebook `2-standard-csp-pipelines`, made use of a fixed filter aproach, namely an overlap-add FIR filter to obtain the frequencies 2Hz to 32Hz.\n",
    "These thresholds will be learned in this experiment by using grid search for them, this should fit the frequencies to the subject(s) in question.\n",
    "To do this, a custom sklearn pipeline component is made from the previously used filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b846ee1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Creation of custom SKLearn pipeline component for filter\n",
    "\n",
    "Since we want to tune the lower and upper bound of the filter using gridsearch, we need to make the filter available as a function that is understood by SKLearn.\n",
    "This creation is done in the provided util file `custom_sklearn_components`.\n",
    "The below code block tests its implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST FILTER\n",
    "####################################################\n",
    "\n",
    "with io.capture_output():\n",
    "    # Get MNE raw object for latest recording of subject B\n",
    "    mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= \"B\")\n",
    "    # Get epochs for that MNE raw\n",
    "    mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                         start_offset= -1,\n",
    "                                                         end_offset= 1,\n",
    "                                                         baseline= (None, 0))\n",
    "    # mne raw not needed anymore\n",
    "    del mne_raw\n",
    "    \n",
    "    # Only keep epochs from the MI tasks\n",
    "    mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "    # Load epochs into memory\n",
    "    mne_epochs.load_data()\n",
    "\n",
    "# Create copies for testing both approaches\n",
    "manual_conversion = copy.deepcopy(mne_epochs)\n",
    "automatic_conversion = copy.deepcopy(mne_epochs)\n",
    "\n",
    "# Do the manuel conversion\n",
    "with io.capture_output():\n",
    "    manual_conversion.filter(l_freq= 8,\n",
    "                      h_freq= 30,\n",
    "                      picks= \"all\",\n",
    "                      phase= \"minimum\",\n",
    "                      fir_window= \"blackman\",\n",
    "                      fir_design= \"firwin\",\n",
    "                      pad= 'median', \n",
    "                      n_jobs= -1,\n",
    "                      verbose= False)\n",
    "    # Get a half second window\n",
    "    manual_result = manual_conversion.get_data(tmin= 0, tmax= 0.5)\n",
    "print(f\"Manual conversion gave the following output shape: {np.shape(manual_result)}\")\n",
    "\n",
    "# Do the automatic conversion\n",
    "filter = EpochsToFilteredData(filter_lower_bound= 8, filter_upper_bound= 30, data_tmin=0, data_tmax=0.5)\n",
    "filter.fit(automatic_conversion)\n",
    "automatic_result = filter.transform(automatic_conversion)\n",
    "print(f\"Automatic conversion gave the following output shape: {np.shape(automatic_result)}\")\n",
    "\n",
    "# Compare results\n",
    "if ((len(manual_result) == len(automatic_result)) and (all(i in automatic_result for i in manual_result))):\n",
    "    match = True\n",
    "else:\n",
    "    match = False\n",
    "print(f\"results are equal: {match}\")\n",
    "\n",
    "# Remove unused variables\n",
    "del automatic_conversion\n",
    "del automatic_result\n",
    "del manual_conversion\n",
    "del manual_result\n",
    "del filter\n",
    "del match\n",
    "del mne_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f198a04",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, same session: RF classifier\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, using a singular session and testing on that same session.\n",
    "This is an over-optimistic testing scenario and has a great risk of overfitting with poor generalisation to new sessions or new subjects but can be an okay baseline test to see if *at least something* can be learned.\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the first two recorded session of each of these participants for training and the last for testing.\n",
    "      - Thus, the CV scores are on the test split for the training data whilst the independent test set is from the unseen part of the session not used during training. This avoids data leakage.\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - Filter -> CSP -> RF\n",
    "      - The following hyperparameters are tested\n",
    "         - For the filter:\n",
    "            - Lower threshold: 1 | 2 | 4 | 6 | 8 | 10 \n",
    "            - Upper threshold: 26 | 28 | 30 | 32 | 34\n",
    "         - For CSP:\n",
    "            - Number of components: 4 | 6 | 10\n",
    "         - For RF:\n",
    "            - Bootstrap is always set to True as it is, besides random feature subsets, one of the ways to reduce data biases during training\n",
    "            - The metric used for splitting criterion is gini as it is faster than entropy and the performance difference is negligible when looking at the experimental notebooks\n",
    "            - Number of estimators: 50 | 100 | 250 | 500\n",
    "            - Max depth of a tree: None | 3 | 5 | 10\n",
    "            - Minimum number of samples to do a split: 2 | 5 | 10\n",
    "            - Minimum samples to have a leaf is not set as it is related to the number of samples per split and max depth of the tree\n",
    "            - Maximum features per tree: sqrt | log2 | None | 0.2 | 0.4 | 0.6\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = True # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                with io.capture_output():\n",
    "                        # Get MNE raw object for latest recording of that subject\n",
    "                        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                        # Get epochs for that MNE raw\n",
    "                        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                             start_offset= start_offset,\n",
    "                                                                             end_offset= end_offset,\n",
    "                                                                             baseline= baseline)\n",
    "                        \n",
    "                        # Only keep epochs from the MI tasks\n",
    "                        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                        \n",
    "                        # Load epochs into memory\n",
    "                        mne_epochs.load_data()\n",
    "                        \n",
    "                        # Get the labels\n",
    "                        labels = mne_epochs.events[:, -1]\n",
    "                        \n",
    "                        # Create a test and train split\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(mne_epochs,\n",
    "                                                                            labels,\n",
    "                                                                            test_size = 0.2,\n",
    "                                                                            shuffle= True,\n",
    "                                                                            stratify= labels,                                                    \n",
    "                                                                            random_state= 1998)\n",
    "                        \n",
    "                        # make split back to MNE epoch object\n",
    "                        X_train = mne.concatenate_epochs(X_train)\n",
    "                        X_test = mne.concatenate_epochs(X_test)\n",
    "                        \n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                filter = EpochsToFilteredData(data_tmin=0, data_tmax=0.5)\n",
    "                \n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                rf = RandomForestClassifier(bootstrap= True,\n",
    "                                            criterion= \"gini\")\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('filter', filter), ('CSP', csp), ('RF', rf)])\n",
    "                \n",
    "                # Configure cross validation to use\n",
    "                cv = StratifiedKFold(n_splits=4,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                #param_grid = [{\"CSP__n_components\": [4, 6, 10],\n",
    "                #               \"RF__n_estimators\": [10, 50, 100, 250, 500],\n",
    "                #               \"RF__max_depth\": [None, 3, 10],\n",
    "                #               \"RF__min_samples_split\": [2, 5, 10],\n",
    "                #               \"RF__max_features\": [\"sqrt\", \"log2\", \"None\", 0.2, 0.4, 0.6]}]\n",
    "                param_grid = [{\"CSP__n_components\": [4, 6],\n",
    "                               \"RF__n_estimators\": [10, 50],\n",
    "                               \"filter__filter_lower_bound\": [2, 4],\n",
    "                               \"filter__filter_upper_bound\": [30, 32],\n",
    "                               }]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True,\n",
    "                                           error_score=\"raise\")\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= X_train, y= y_train)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/gridsearch_autofreqcsprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Store the train and test data so the best model can be retrained later\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/testdata-x_autofreqcsprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_test, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/testdata-y_autofreqcsprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_test, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/traindata-x_autofreqcsprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(X_train, file)\n",
    "                with open(f\"saved_variables/3/samesubject_samesession/subject{subject_id}/traindata-y_autofreqcsprf_subject{subject_id}.pickle\", 'wb') as file:\n",
    "                        pickle.dump(y_train, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del csp\n",
    "                del rf\n",
    "                del filter\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del X_train\n",
    "                del X_test\n",
    "                del y_train\n",
    "                del y_test \n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb465ba",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| **Subject** | **CSP + RF: cross validation accuracy** | **CSP + RF: test split accuracy** | **Config**                                                                                   |\n",
    "|-------------|------------------------------------------|------------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| B           | 0.6588 +- 0.0316                         | 0.6042                             | 4 CSP components \\| RF max depth 10, max features 0.4, min sample split 10, 50 estimators    |\n",
    "| C           | 0.7119 +- 0.0316                         | 0.7031                             | 6 CSP components \\| RF max depth 3, max features 0.4, min sample split 5, 250 estimators     |\n",
    "| E           | 0.7251 +- 0.0176                         | 0.7539                             | 10 CSP components \\| RF max depth None, max features 0.2, min sample split 2, 250 estimators |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ca6d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_id}/gridsearch_csprf_subject{subject_id}.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {np.round(grid_search.best_score_, 4)} +- {np.round(grid_search.cv_results_['std_test_score'][grid_search.best_index_], 4)} with the following parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acec67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "best_found_csp_components = [4, 6 , 10]\n",
    "best_found_rf_depth = [10, 3, None]\n",
    "best_found_rf_max_featues = [0.4, 0.4, 0.2]\n",
    "best_found_rf_min_sample = [10, 5, 2]\n",
    "best_found_rf_n_estimators = [50, 250, 250]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open train and test data from file\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-x_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/testdata-y_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-x_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(f\"saved_variables/2/samesubject_samesession/subject{subject_ids_to_test[i]}/traindata-y_csprf_subject{subject_ids_to_test[i]}.pickle\", 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    rf = RandomForestClassifier(bootstrap= True,\n",
    "                                criterion= \"gini\",\n",
    "                                max_depth= best_found_rf_depth[i],\n",
    "                                max_features= best_found_rf_max_featues[i],\n",
    "                                min_samples_split= best_found_rf_min_sample[i],\n",
    "                                n_estimators= best_found_rf_n_estimators[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {np.round(accuracy, 4)}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_rf_depth\n",
    "del best_found_rf_max_featues\n",
    "del best_found_rf_min_sample\n",
    "del best_found_rf_n_estimators\n",
    "del i\n",
    "del f\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del rf\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9f1dd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Same subject, new session: RF classifier\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "A classifier may be trained on a singular subject, but by using one or more sessions for training and testing on a new, unseen session.\n",
    "This is a harder task than the previous one, where training and testing were done for the same session.\n",
    "This section will train the same classifiers for the same participants as before but by using the first two datasets as training data and the third and final session of each participant as a standalone test set which is not used in training.\n",
    "\n",
    "# TODO\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use the first two recorded session of each of these participants for training and the last for testing.\n",
    "      - Thus, the CV scores are on the test split for the training data whilst the independent test set is from the unseen session not used during training. This avoids data leakage.\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We split the data in a train/test dataset with 20% test data balanced over all MI classes\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> SVM\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 2 | 3 | 4 | 6 | 10\n",
    "         - For RF:\n",
    "            - Bootstrap is always set to True as it is, besides random feature subsets, one of the ways to reduce data biases during training\n",
    "            - The metric used for splitting criterion is gini as it is faster than entropy and the performance difference is negligible when looking at the experimental notebooks\n",
    "            - Number of estimators: 10 | 50 | 100 | 250 | 500\n",
    "            - Max depth of a tree: None | 3 | 10 | 20 | 35 | 50 | 75 | 100\n",
    "            - Minimum number of samples to do a split: 2 | 5 | 10\n",
    "            - Minimum samples to have a leaf is not set as it is related to the number of samples per split and max depth of the tree\n",
    "            - Maximum features per tree: sqrt | log2 | None\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8572b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = False # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                # Get all training data (all but last session of participant)\n",
    "                mne_raws= CLA_dataset.get_all_but_last_raw_mne_data_for_subject(subject_id= subject_id)\n",
    "                \n",
    "                # Combine training data into singular mne raw\n",
    "                mne_raw = mne.concatenate_raws(mne_raws)\n",
    "                \n",
    "                # Delete all raws since concat changes them\n",
    "                del mne_raws\n",
    "                \n",
    "                # Get epochs for all those MNE raws (all training sessions)\n",
    "                mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                     start_offset= start_offset,\n",
    "                                                                     end_offset= end_offset,\n",
    "                                                                     baseline= baseline)\n",
    "                \n",
    "                # Only keep epochs from the MI tasks\n",
    "                mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "    \n",
    "                # Load epochs into memory\n",
    "                mne_epochs.load_data()\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                  h_freq= filter_upper_bound,\n",
    "                                  picks= \"all\",\n",
    "                                  phase= \"minimum\",\n",
    "                                  fir_window= \"blackman\",\n",
    "                                  fir_design= \"firwin\",\n",
    "                                  pad= 'median', \n",
    "                                  n_jobs= -1,\n",
    "                                  verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                rf = RandomForestClassifier(bootstrap= True,\n",
    "                                            criterion= \"gini\")\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "                \n",
    "                # Configure cross validation to use, more splits then before since we have more data\n",
    "                cv = StratifiedKFold(n_splits= 6,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [4, 6, 10],\n",
    "                               \"RF__n_estimators\": [10, 50, 100, 250, 500],\n",
    "                               \"RF__max_depth\": [None, 3, 10],\n",
    "                               \"RF__min_samples_split\": [2, 5, 10],\n",
    "                               \"RF__max_features\": [\"sqrt\", \"log2\", \"None\", 0.2, 0.4, 0.6]}]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= mne_epochs_data, y= labels)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/samesubject_differentsession/subject{subject_id}/gridsearch_csprf.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del rf\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del grid_search\n",
    "                del param_grid\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a8c33",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The CV results are based on the training set alone and thus only look at the first two sessions.\n",
    "The test result is for a new, unseen session and thus scores are expected to differ.\n",
    "\n",
    "| **Subject** | **CSP + RF: cross validation accuracy** | **CSP + RF: test split accuracy** | **Config**                                                                                   |\n",
    "|-------------|-----------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| B           | 0.4489 +- 0.0351                        | 0.4406                            | 10 CSP components \\| RF with max depth 3, 0.4 features, 10 min sample split, 500 estimators  |\n",
    "| C           | 0.8198 +- 0.0198                        | 0.3462                            | 10 CSP components \\| RF with max depth None, 0.2 features, 2 min sample split, 50 estimators |\n",
    "| E           | 0.5770 +- 0.0290                        | 0.4911                            | 10 CSP components \\| RF with max depth 10, 0.4 features, 10 min sample split, 250 estimators |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ecf9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/samesubject_differentsession/subject{subject_id}/gridsearch_csprf.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {np.round(grid_search.best_score_, 4)} +- {np.round(grid_search.cv_results_['std_test_score'][grid_search.best_index_], 4)} with the following parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d060e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "best_found_csp_components = [10, 10 , 10]\n",
    "best_found_rf_depth = [3, None, 10]\n",
    "best_found_rf_max_featues = [0.4, 0.2, 0.4]\n",
    "best_found_rf_min_sample = [10, 2, 10]\n",
    "best_found_rf_n_estimators = [500, 50, 250]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ################# TRAINING DATA #################\n",
    "    with io.capture_output():\n",
    "        # Get all training data (all but last session of participant)\n",
    "        mne_raws = CLA_dataset.get_all_but_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i])\n",
    "        \n",
    "        # Combine training data into singular mne raw\n",
    "        mne_raw = mne.concatenate_raws(mne_raws)\n",
    "        \n",
    "        # Get epochs for all those MNE raws (all training sessions)\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the labels\n",
    "        y_train = mne_epochs.events[:, -1]\n",
    "        \n",
    "        # Use a fixed filter\n",
    "        mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                          h_freq= filter_upper_bound,\n",
    "                          picks= \"all\",\n",
    "                          phase= \"minimum\",\n",
    "                          fir_window= \"blackman\",\n",
    "                          fir_design= \"firwin\",\n",
    "                          pad= 'median', \n",
    "                          n_jobs= -1,\n",
    "                          verbose= False)\n",
    "\n",
    "        # Get a half second window\n",
    "        X_train = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raws\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "    \n",
    "    ################# TESTING DATA #################\n",
    "    with io.capture_output():\n",
    "        # Get test data\n",
    "        mne_raw = CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i])\n",
    "        \n",
    "        # Get epochs for test MNE raw\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the labels\n",
    "        y_test = mne_epochs.events[:, -1]\n",
    "        \n",
    "        # Use a fixed filter\n",
    "        mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                          h_freq= filter_upper_bound,\n",
    "                          picks= \"all\",\n",
    "                          phase= \"minimum\",\n",
    "                          fir_window= \"blackman\",\n",
    "                          fir_design= \"firwin\",\n",
    "                          pad= 'median', \n",
    "                          n_jobs= -1,\n",
    "                          verbose= False)\n",
    "\n",
    "        # Get a half second window\n",
    "        X_test = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "    \n",
    "    ################# FIT AND PREDICT #################\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    rf = RandomForestClassifier(bootstrap= True,\n",
    "                                criterion= \"gini\",\n",
    "                                max_depth= best_found_rf_depth[i],\n",
    "                                max_features= best_found_rf_max_featues[i],\n",
    "                                min_samples_split= best_found_rf_min_sample[i],\n",
    "                                n_estimators= best_found_rf_n_estimators[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {np.round(accuracy, 4)}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_rf_depth\n",
    "del best_found_rf_max_featues\n",
    "del best_found_rf_min_sample\n",
    "del best_found_rf_n_estimators\n",
    "del i\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del rf\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n",
    "del start_offset\n",
    "del end_offset\n",
    "del baseline\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea76ac",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### New subject: RF classifier\n",
    "\n",
    "As discussed in the master's thesis, training and testing a classification system can happen using multiple strategies.\n",
    "Perhaps the hardest task is training a classifier on data from one or more subjects, but using it to classify data from a completely new user.\n",
    "This is the hardest task we'll discuss.\n",
    "This section will train the same classifiers for the same participants as before but by using one participant for testing and the other two for training.\n",
    "\n",
    "# TODO\n",
    "\n",
    "This experiment works as follows:\n",
    "   - We use participants with at least three recordings\n",
    "      - Participants: B, C, E\n",
    "      - NOTE: participant F has three files provided but one of those files has only three MI classes rather than three, hence it is not considered here\n",
    "   - We use one participant's data for testing and the other two participant's data for training\n",
    "      - Thus, the CV scores are on the test split for the training data whilst the independent test set is from the unseen subject not used during training. This avoids data leakage.\n",
    "   - We get epochs of 3 seconds, which includes one second before and after the visual queue\n",
    "      - We use only a half a second window taking into account the online system will use sliding windows.\n",
    "      - This window starts at 0.1 seconds after then visual queue and ends at 0.6 seconds after the visual queue\n",
    "   - We use grid search on the created window of each baseline corrected epoch from the train split to find the best parameters for the pipeline\n",
    "      - The frequency filtering uses fixed parameters to limit the training process and since CSP alternatives which perform automatic filtering exist and are recommended over manually finding the best frequencies through grid search\n",
    "         - According to [Afrakhteh and RezaMosavi](https://doi.org/10.1016/B978-0-12-819045-6.00002-9), the desired frequency band for MI classification is 8-30 Hz. \n",
    "         - However, the neutral task isn't a specific MI task and is more likely to correspond with a relaxed state, having a low frequency.\n",
    "         - To accommodate for the neutral task and a general configuration that suits all participants, the overlap-add FIR filter uses frequencies 2 to 32Hz \n",
    "      - The pipeline that is hyperparameter tuned is as follows\n",
    "         - CSP -> SVM\n",
    "      - The following hyperparameters are tested\n",
    "         - For CSP:\n",
    "            - Number of components: 2 | 3 | 4 | 6 | 10\n",
    "         - For RF:\n",
    "            - Bootstrap is always set to True as it is, besides random feature subsets, one of the ways to reduce data biases during training\n",
    "            - The metric used for splitting criterion is gini as it is faster than entropy and the performance difference is negligible when looking at the experimental notebooks\n",
    "            - Number of estimators: 10 | 50 | 100 | 250 | 500\n",
    "            - Max depth of a tree: None | 3 | 10 | 20 | 35 | 50 | 75 | 100\n",
    "            - Minimum number of samples to do a split: 2 | 5 | 10\n",
    "            - Minimum samples to have a leaf is not set as it is related to the number of samples per split and max depth of the tree\n",
    "            - Maximum features per tree: sqrt | log2 | None\n",
    "   - We use the test split for final validation on the best-found parameters\n",
    "\n",
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCHING BEST PIPELINE FOR EACH SUBJECT\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "do_experiment = False # Long experiment disabled per default\n",
    "\n",
    "if do_experiment:\n",
    "        # Loop over all subjects and perform the grid search for finding the best parameters\n",
    "        for subject_id in subject_ids_to_test:\n",
    "                ###################### PREPARE DATA ######################\n",
    "                with io.capture_output():\n",
    "                    # Determine the train subjects\n",
    "                    train_subjects = copy.deepcopy(subject_ids_to_test)\n",
    "                    train_subjects.remove(subject_id)\n",
    "                    \n",
    "                    mne_raws = []\n",
    "                    \n",
    "                    # Get all training data\n",
    "                    for train_subject in train_subjects:\n",
    "                        mne_raws.extend(CLA_dataset.get_all_raw_mne_data_for_subject(subject_id= train_subject))\n",
    "                    \n",
    "                    # Combine training data into singular mne raw\n",
    "                    mne_raw = mne.concatenate_raws(mne_raws)\n",
    "                    \n",
    "                    # Delete all raws since concat changes them\n",
    "                    del mne_raws\n",
    "                    \n",
    "                    # Get epochs for that MNE raw\n",
    "                    mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                         start_offset= start_offset,\n",
    "                                                                         end_offset= end_offset,\n",
    "                                                                         baseline= baseline)\n",
    "                    \n",
    "                    # Only keep epochs from the MI tasks\n",
    "                    mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                    \n",
    "                    # Load epochs into memory\n",
    "                    mne_epochs.load_data()\n",
    "                \n",
    "                # Show training data\n",
    "                print(f\"Using data from participants {train_subjects} to train for testing on participant {subject_id}\")\n",
    "                \n",
    "                # Get the labels\n",
    "                labels = mne_epochs.events[:, -1]\n",
    "                \n",
    "                # Use a fixed filter\n",
    "                mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                                  h_freq= filter_upper_bound,\n",
    "                                  picks= \"all\",\n",
    "                                  phase= \"minimum\",\n",
    "                                  fir_window= \"blackman\",\n",
    "                                  fir_design= \"firwin\",\n",
    "                                  pad= 'median', \n",
    "                                  n_jobs= -1,\n",
    "                                  verbose= False)\n",
    "    \n",
    "                # Get a half second window\n",
    "                mne_epochs_data = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "                \n",
    "                # Configure the pipeline components by specifying the default parameters\n",
    "                csp = CSP(norm_trace=False,\n",
    "                          component_order=\"mutual_info\",\n",
    "                          cov_est= \"epoch\")\n",
    "                \n",
    "                rf = RandomForestClassifier(bootstrap= True,\n",
    "                                            criterion= \"gini\")\n",
    "                \n",
    "                # Configure the pipeline\n",
    "                pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "                \n",
    "                # Configure cross validation to use, more splits then before since we have more data\n",
    "                cv = StratifiedKFold(n_splits= 6,\n",
    "                                     shuffle= True,\n",
    "                                     random_state= 2022)\n",
    "                \n",
    "                # Configure the hyperparameters to test\n",
    "                # NOTE: these are somewhat limited due to limitedd computational resources\n",
    "                param_grid = [{\"CSP__n_components\": [4, 6, 10],\n",
    "                               \"RF__n_estimators\": [10, 50, 100, 250, 500],\n",
    "                               \"RF__max_depth\": [None, 3, 10],\n",
    "                               \"RF__min_samples_split\": [2, 5, 10],\n",
    "                               \"RF__max_features\": [\"sqrt\", \"log2\", \"None\", 0.2, 0.4, 0.6]}]\n",
    "                               \n",
    "                # Configure the grid search\n",
    "                grid_search = GridSearchCV(estimator= pipeline,\n",
    "                                           param_grid= param_grid,\n",
    "                                           scoring= \"accuracy\",\n",
    "                                           n_jobs= -1,\n",
    "                                           refit= False, # We will do this manually\n",
    "                                           cv= cv,\n",
    "                                           verbose= 10,\n",
    "                                           return_train_score= True)\n",
    "\n",
    "                # Do the grid search on the training data\n",
    "                grid_search.fit(X= mne_epochs_data, y= labels)\n",
    "    \n",
    "                # Store the results of the grid search\n",
    "                with open(f\"saved_variables/2/newsubject/subject{subject_id}/gridsearch_csprf.pickle\", 'wb') as file:\n",
    "                        pickle.dump(grid_search, file)\n",
    "                \n",
    "                # Delete vars after singular experiment\n",
    "                del mne_raw\n",
    "                del mne_epochs\n",
    "                del mne_epochs_data\n",
    "                del csp\n",
    "                del rf\n",
    "                del pipeline\n",
    "                del labels\n",
    "                del cv\n",
    "                del file\n",
    "                del grid_search\n",
    "                del param_grid\n",
    "                del train_subject\n",
    "                del train_subjects\n",
    "    \n",
    "        # Delete vars after all experiments\n",
    "        del subject_id\n",
    "        \n",
    "# Del global vars\n",
    "del subject_ids_to_test\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound\n",
    "del baseline\n",
    "del do_experiment\n",
    "del end_offset\n",
    "del start_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049ad53",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The CV results are based on the training set alone and thus only look at the first two sessions.\n",
    "The test result is for a new, unseen session and thus scores are expected to differ.\n",
    "\n",
    "| **Subject**      | **CSP + RF: cross validation accuracy** | **CSP + RF: test split accuracy** | **Config**                                                                                               |\n",
    "|------------------|-----------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| B (Train on C&E) | 0.5983 +- 0.0068                        | 0.3923                            | 10 CSP components \\| RF with None max depth, 0.2 max features, 2 min samples split and 500 estimators    |\n",
    "| C (Train on B&E) | 0.504 +- 0.0197                         | 0.4571                            | 10 CSP components \\| RF with None max depth, sqrt max features, 10 min samples split and 250 estimators  |\n",
    "| E (Train on B&C) | 0.572 +- 0.0274                         | 0.3715                            | 10 CSP components \\| RF with 10 max depth, log2 max features, 10 min samples split and 250 estimators    |\n",
    "\n",
    "Again, performance is poor and LDA, SVM and RF perform very equal.\n",
    "It is clear the limiting factor here is the CSP feature extraction rather then the ML classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ddb3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# GRID SEARCH RESULTS\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "\n",
    "# Loop over all found results\n",
    "for subject_id in subject_ids_to_test:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# GRID SEARCH RESULTS FOR SUBJECT {subject_id}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Open from file\n",
    "    with open(f\"saved_variables/2/newsubject/subject{subject_id}/gridsearch_csprf.pickle\", 'rb') as f:\n",
    "        grid_search = pickle.load(f)\n",
    "        \n",
    "    # Print the results\n",
    "    print(f\"Best estimator has accuracy of {np.round(grid_search.best_score_, 4)} +- {np.round(grid_search.cv_results_['std_test_score'][grid_search.best_index_], 4)} with the following parameters\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # Get grid search results\n",
    "    grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Keep relevant columns and sort on rank\n",
    "    grid_search_results.drop(labels='params', axis=1, inplace= True)\n",
    "    grid_search_results.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "    # Display grid search resulst\n",
    "    print(\"\\n\\n Top 10 grid search results: \")\n",
    "    display(grid_search_results.head(10))\n",
    "    print(\"\\n\\n Worst 10 grid search results: \")\n",
    "    display(grid_search_results.tail(10))\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"\\n\\nIn total there are {len(grid_search_results)} different configurations tested.\")\n",
    "    max_score = grid_search_results['mean_test_score'].max()\n",
    "    print(f\"The best mean test score is {round(max_score, 4)}\")\n",
    "    shared_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score, max_score)])\n",
    "    print(f\"There are {shared_first_place_count} configurations with this maximum score\")\n",
    "    close_first_place_count = len(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)])\n",
    "    print(f\"There are {close_first_place_count} configurations within 0.02 of this maximum score\")\n",
    "\n",
    "    # Display statistics for best classifiers\n",
    "    print(\"\\n\\nThe describe of the configurations within 0.02 of this maximum score is as follows:\")\n",
    "    display(grid_search_results[grid_search_results['mean_test_score'].between(max_score-0.02, max_score)].describe(include=\"all\"))\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del f\n",
    "del grid_search\n",
    "del max_score\n",
    "del shared_first_place_count\n",
    "del close_first_place_count\n",
    "del grid_search_results\n",
    "del subject_ids_to_test\n",
    "del subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb13c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# TEST RESULTS FOR BEST FOUND GRID SEARCH\n",
    "####################################################\n",
    "\n",
    "# Configure global parameters for all experiments\n",
    "subject_ids_to_test = [\"B\", \"C\", \"E\"] # Subjects with three recordings\n",
    "start_offset = -1 # One second before visual queue\n",
    "end_offset = 1 # One second after visual queue\n",
    "baseline = (None, 0) # Baseline correction using data before the visual queue\n",
    "filter_lower_bound = 2 # Filter out any frequency below this \n",
    "filter_upper_bound = 32 # Filter out any frequency above this\n",
    "best_found_csp_components = [10, 10 , 10]\n",
    "best_found_rf_depth = [None, None, 10]\n",
    "best_found_rf_max_featues = [0.2, \"sqrt\", \"log2\"]\n",
    "best_found_rf_min_sample = [2, 10, 10]\n",
    "best_found_rf_n_estimators = [500, 250, 250]\n",
    "\n",
    "# Loop over all found results\n",
    "for i in range(len(subject_ids_to_test)):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"####################################################\")\n",
    "    print(f\"# TEST RESULTS FOR SUBJECT {subject_ids_to_test[i]}\")\n",
    "    print(\"####################################################\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ################# TRAINING DATA #################\n",
    "    with io.capture_output():\n",
    "        with io.capture_output():\n",
    "                    # Determine the train subjects\n",
    "                    train_subjects = copy.deepcopy(subject_ids_to_test)\n",
    "                    train_subjects.remove(subject_ids_to_test[i])\n",
    "                    \n",
    "                    mne_raws = []\n",
    "                    \n",
    "                    # Get all training data\n",
    "                    for train_subject in train_subjects:\n",
    "                        mne_raws.extend(CLA_dataset.get_all_raw_mne_data_for_subject(subject_id= train_subject))\n",
    "                    \n",
    "                    # Combine training data into singular mne raw\n",
    "                    mne_raw = mne.concatenate_raws(mne_raws)\n",
    "                    \n",
    "                    # Get epochs for that MNE raw\n",
    "                    mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                                         start_offset= start_offset,\n",
    "                                                                         end_offset= end_offset,\n",
    "                                                                         baseline= baseline)\n",
    "                    \n",
    "                    # Only keep epochs from the MI tasks\n",
    "                    mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "                    \n",
    "                    # Load epochs into memory\n",
    "                    mne_epochs.load_data()\n",
    "        \n",
    "        # Get the labels\n",
    "        y_train = mne_epochs.events[:, -1]\n",
    "        \n",
    "        # Use a fixed filter\n",
    "        mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                          h_freq= filter_upper_bound,\n",
    "                          picks= \"all\",\n",
    "                          phase= \"minimum\",\n",
    "                          fir_window= \"blackman\",\n",
    "                          fir_design= \"firwin\",\n",
    "                          pad= 'median', \n",
    "                          n_jobs= -1,\n",
    "                          verbose= False)\n",
    "\n",
    "        # Get a half second window\n",
    "        X_train = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raws\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "    \n",
    "    ################# TESTING DATA #################\n",
    "    with io.capture_output():\n",
    "        # Get test data\n",
    "        mne_raws = CLA_dataset.get_all_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i])\n",
    "        \n",
    "        # Combine test data into singular mne raw\n",
    "        mne_raw = mne.concatenate_raws(mne_raws)\n",
    "        \n",
    "        # Get epochs for test MNE raw\n",
    "        mne_epochs = CLA_dataset.get_usefull_epochs_from_raw(mne_raw,\n",
    "                                                             start_offset= start_offset,\n",
    "                                                             end_offset= end_offset,\n",
    "                                                             baseline= baseline)\n",
    "        \n",
    "        # Only keep epochs from the MI tasks\n",
    "        mne_epochs = mne_epochs['task/neutral', 'task/left', 'task/right']\n",
    "\n",
    "        # Load epochs into memory\n",
    "        mne_epochs.load_data()\n",
    "        \n",
    "        # Get the labels\n",
    "        y_test = mne_epochs.events[:, -1]\n",
    "        \n",
    "        # Use a fixed filter\n",
    "        mne_epochs.filter(l_freq= filter_lower_bound,\n",
    "                          h_freq= filter_upper_bound,\n",
    "                          picks= \"all\",\n",
    "                          phase= \"minimum\",\n",
    "                          fir_window= \"blackman\",\n",
    "                          fir_design= \"firwin\",\n",
    "                          pad= 'median', \n",
    "                          n_jobs= -1,\n",
    "                          verbose= False)\n",
    "\n",
    "        # Get a half second window\n",
    "        X_test = mne_epochs.get_data(tmin= 0.1, tmax= 0.6)\n",
    "        \n",
    "        # Delete resedual vars for training data\n",
    "        del mne_raw\n",
    "        del mne_epochs\n",
    "        del mne_raws\n",
    "    \n",
    "    ################# FIT AND PREDICT #################\n",
    "        \n",
    "    # Make the classifier\n",
    "    csp = CSP(norm_trace=False,\n",
    "              component_order=\"mutual_info\",\n",
    "              cov_est= \"epoch\",\n",
    "              n_components= best_found_csp_components[i])\n",
    "    \n",
    "    rf = RandomForestClassifier(bootstrap= True,\n",
    "                                criterion= \"gini\",\n",
    "                                max_depth= best_found_rf_depth[i],\n",
    "                                max_features= best_found_rf_max_featues[i],\n",
    "                                min_samples_split= best_found_rf_min_sample[i],\n",
    "                                n_estimators= best_found_rf_n_estimators[i])\n",
    "    \n",
    "    # Configure the pipeline\n",
    "    pipeline = Pipeline([('CSP', csp), ('RF', rf)])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    with io.capture_output():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get accuracy for single fit\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print accuracy results and CM\n",
    "    print(f\"Test accuracy for subject {subject_ids_to_test[i]}: {np.round(accuracy, 4)}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred= y_pred)\n",
    "    plt.show()\n",
    "        \n",
    "    # plot CSP patterns estimated on train data for visualization\n",
    "    pipeline['CSP'].plot_patterns(CLA_dataset.get_last_raw_mne_data_for_subject(subject_id= subject_ids_to_test[i]).info, ch_type='eeg', units='Patterns (AU)', size=1.5)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove unsused variables\n",
    "del subject_ids_to_test\n",
    "del best_found_csp_components\n",
    "del best_found_rf_depth\n",
    "del best_found_rf_max_featues\n",
    "del best_found_rf_min_sample\n",
    "del best_found_rf_n_estimators\n",
    "del i\n",
    "del X_test\n",
    "del y_test\n",
    "del X_train\n",
    "del y_train\n",
    "del csp\n",
    "del rf\n",
    "del train_subjects\n",
    "del train_subject\n",
    "del pipeline\n",
    "del y_pred\n",
    "del accuracy\n",
    "del start_offset\n",
    "del end_offset\n",
    "del baseline\n",
    "del filter_lower_bound\n",
    "del filter_upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92ed28e6a5fe026f22555c18fed88052bb861e5576fb72d2ac78e2247fef331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
