% TODO:
%   - Kijk na of titels in header overflowen
% ----------  
% Questions:
%   - XXX

% appendix paper arnaa

% beter onderscheid maken tussen wat dit is en volgend chapter, note wel de refs naar chapters/sections nog kloppen
% mss Herstructureren naar pipeline based op figure uit bci_review_arnau en dan per blokje bespreken en dan ander chapter echt een implementatie?

% preprocessing: After acquiring the signal, it typically needs to be cleaned because biosignals have a low signal-tonoise ratio (SNR). This means that there will be more noise present in the acquired data, making it harder to extract useful information. Therefore, a preprocessing step is necessary to improve SNR and yield a ‘clean’ signal that eliminates artifacts, such as those resulting from power supplies or unrelated muscle

% classification vs regression: This can either be classification into a discrete set of classes or regression into a value that is relevant to the control application. regression vooral in exoskeleton en prosthesis by data from bci_review_arnau

% The most common form of ML is supervised learning, in which we assume that the data is presented as a set of input-output pairs, a dataset, which we call labeled data, as each input is labeled with its corresponding output.


 
% Methods and algorithms that allow to concretely perform this learning operation are extensively reviewed by Caruana and Niculescu-Mizil (2006).
 
% generalization bespreken: With neural networks, the main avenue to increase generalization is to decrease over-fitting. Over-fitting happens when a neural network remembers exactly what training input should learn which training output, without having actually made sense of the data. The network achieves a training loss close to 0, but produces garbage output on the testing set. It is like a small child that learns how to read words, and remembers that card number 7 is pronounced ‘cat’, without actually looking at the word written on the card, or being able to read at all. Batch normalization (Ioffe and Szegedy 2015) considers the input of every layer in a neural network, and normalizes it so that, in expectation, the inputs of every layer has a zero mean and a unit variance. Intuitively, this normalization prevents ludicrously large or small values from appearing inside the network, which makes it ‘behave better’ or ‘be smoother’ (so, easier to train, and better at generalization). The actual mathematical way in which batch normalization works is however still unknown, with recent papers providing the first insights (Santurkar et al 2018). Dropout (Srivastava et al 2014) does not modify the values that flow through a neural network, but instead randomly disables neurons every time the network is evaluated during training. The main motivation behind Dropout is to avoid one particular neuron in the network to learn how to compensate (and thus cancel out) another particular neuron in the network. When neurons are constantly randomly disabled and re-enabled, they all have to learn independently from each other. More mathematically, Dropout leads to a neural network that is made of a different set of neurons every time it is evaluated. This leads to a large ensemble of ‘sub-networks’, all trained on different datapoints. Ensembles of function approximators such as this are known to help with generalization (Dietterich 2000). Both batch normalization (Tayeb et al 2019, Tam et al 2020) and Dropout (Gautam et al 2020, Tortora et al 2020a) are often used in biosignal decoding papers, sometimes both at the same time. Other normalization techniques are possible, such as L1- normalization or clipping the gradients (Zhang et al 2019a), but they have been superseded by Batch Normalization and Dropout.

% In a new chapter, reset the GLS to once again use full version in first occurence
\glsresetall

\chapter{Processing brain-signals and taking actions from their interpretation}
\label{ch:processing_signals}



% ---------------------------------------------- 
% INTRODUCTION
% ---------------------------------------------- 
\section{Introduction to this chapter}
\label{sec:processing_signals_introduction}
% NOTE: "Introduction" exists in each chapter and gives short intro to chapter + what can be expected in chapter

\lipsum[1-3]



% ---------------------------------------------- 
% GENERAL EEG BCI PIPELINE
% ---------------------------------------------- 
\section{A general EEG-based BCI pipeline}
\label{sec:processing_signals_general_pipeline}

% link back naar pipeline van cad

% duidelijk bespreken welk ding echt CS en welk ding eigenlijk derden

\lipsum[1-3]

% - - - - - - - - - -
% Data acquisition
% - - - - - - - - - -

\subsection{Data acquisition}
\label{subsec:processing_signals_general_pipeline_data_acquisition}

% duidelijk bespreken welk ding echt CS en welk ding eigenlijk derden
% Bespreken data truc * 100xxx voor volt (of al besproken?)
\lipsum[1-2]

% - - - - - - - - - -
% Preprocessing
% - - - - - - - - - -

\subsection{Preprocessing}
\label{subsec:processing_signals_general_pipeline_preprocessing}

% Hardware & software
% Volgens review arnau niet perse nodig want DL kan van "elk signaal" leren

% as discussed by review arnau: Limit required preprocessing: extensive preprocessing will yield clean signals with a high SNR, but this usually comes at the cost of expensive computational requirements that take resources and time. It will often be necessary to balance preprocessing requirements with latency and power consumption constraints.

% Conceptually, given enough layers and neurons, and the proper architecture, a neural network can learn any mapping from inputs to outputs (Sonoda and Murata 2017) (they are universal function approximators). This means that any method of acquiring a signal, and representing it as floatingpoint values will eventually allow the network to make sense of the inputs, and learn something. However, a more careful design of the inputs allows to improve two important properties of the neural network: learning speed (important when the network is used in an adaptive system that learns as it is being used) and generalization power (the ability of making high-quality predictions for unseen inputs, even if training on a small amount of input-output pairs). Designing the input, also called feature engineering, is highly domain-specific. In the signal processing literature, especially in settings that consider EEG data

% Signal filtering: applied on the signals, filters remove frequencies of the signals, to only keep those of interest. This is a form of noise removal, in which the expert designer knows that some frequencies never convey information and can only be noise. There are many different types of filters, which fall outside of the scope of this publication. For more information on filters and digital signal processing, interested readers are referred to (Orfanidis 1996).

\lipsum[1-5]

% - - - - - - - - - -
% Windowing
% - - - - - - - - - -

\subsection{Windowing}
\label{subsec:processing_signals_general_pipeline_windowing}

% Several windowing methods exist and are reviewed by Podder et al (2014).

% Windowing: when specific events in a signal (such as a spike or pattern) matters more than the overall shape of the complete signal, windowing allows to split a signal into fixed-length, usually overlapping, sub-sequences. Having the network focus on small sub-sequences allows it to be faster (less compute intensive, as less data is being processed), and generalize better, as a small number of easily-recognizable patterns (on which the network focuses) can appear in various positions in longer signals (that the network does not have to bother with). Several windowing methods exist, and are reviewed by Podder et al (2014). Jeong et al (2020) and Nguyen and Chung (2019) use Hamming windowing.

\lipsum[1-3]

% - - - - - - - - - -
% Feature
% - - - - - - - - - -

\subsection{Feature engineering}
\label{subsec:processing_signals_general_pipeline_features}

% The CSP method performs feature extraction based on learned spatial filters.
% https://www.youtube.com/watch?v=zsOULC16USU

% Feature extraction: this final step is highly variable and depends on the exact context (sleep staging, Motor Imagery detection, epilepsy seizure detection, etc) in which the signal should be decoded. In general, DL models have been shown to perform better when the input is the raw (preprocessed) signal that is still represented as timeseries of samples for each signal channel. One of the most commonly used feature extraction methods is the Fourier transform, which allows to decompose a temporal signal (a sequence of signal readings over time) to a sum of sinuses of various frequencies. The Fourier transform transforms data from the time domain to the frequency domain. This transform is loss-less and invertible, which means that it does not destroy information. It allows the neural network to more easily focus on the existence of a particular frequency in a signal, instead of having to make sense of the entire (time-domain) signal.

% For a detailed review of possible feature extraction methods, we refer interested readers to Rashid et al (2020).

\lipsum[1-5]

% - - - - - - - - - -
% Classification
% - - - - - - - - - -

\subsection{Classification model}
\label{subsec:processing_signals_general_pipeline_classification}

\lipsum[1-5]

% - - - - - - - - - -
% Performing an action
% - - - - - - - - - -

\subsection{Performing an action}
\label{subsec:processing_signals_general_pipeline_perform_action}

\lipsum[1-3]

% - - - - - - - - - -
% Evaluation
% - - - - - - - - - -

\subsection{Evaluating the classification system}
\label{subsec:processing_signals_general_pipeline_evaluating}

% accuracy, roc, froc, specificity, sensititivity etc etc

\lipsum[1-6]



% ---------------------------------------------- 
% ALTERNATIVE PIPELINES
% ---------------------------------------------- 
\section{Alternative BCI pipelines}
\label{sec:processing_signals_alternative_pipelines}

% - - - - - - - - - -
% No data acquisition
% - - - - - - - - - -

\subsection{Using existing open-source data for training and testing}
\label{subsec:processing_signals_alternative_pipelines_using_existing_data}

\lipsum[1-3]

% - - - - - - - - - -
% No action performing
% - - - - - - - - - -

\subsection{Calibrating an existing system}
\label{subsec:processing_signals_alternative_calibration}

% uitleggen hoe TL

\lipsum[1-3]

% - - - - - - - - - -
% No preprocessing & feature engineering
% - - - - - - - - - -

\subsection{Neglecting preprocessing and feature engineering}
\label{subsec:processing_signals_alternative_pipelines_no_preprocessing_and_features}

\lipsum[1-2]

% - - - - - - - - - -
% No action performing
% - - - - - - - - - -

\subsection{No action performing step}
\label{subsec:processing_signals_alternative_pipelines_no_actions}

% niet strict conform met wolpaw's def want communication device

\lipsum[1-2]

% - - - - - - - - - -
% Offline vs online
% - - - - - - - - - -

\subsection{Offline vs online BCI systems}
\label{subsec:processing_signals_alternative_pipelines_offline_vs_online}

\lipsum[1-4]



% ---------------------------------------------- 
% ML AND DL TECHNIQUES
% ---------------------------------------------- 

\section{The role of machine learning and deep learning}
\label{sec:processing_signals_ml_and_dl}

% learning and prediction phase


% The dominance of CNN with regard to model choice can be attributed to the relative ease of use and the popularity of this architecture in other research fields that use DL. While RNN architectures have been successful in closely related fields such as speech recognition and natural language processing, they have only seen limited deployment in a biosignal decoding context. Typically, CNNs also have less trainable parameters which makes them less sensitive to small datasets and lower their computational requirements. Other architectures were investigated for biosignal decoding, but state-of-the-art research mostly focuses on CNN architectures (Buongiorno et al 2019, Roy et al 2019).

% nog een pro CNN en waarom wij focussen op CNN: The choice of DL model is highly dependent on the application requirements and how the data is preprocessed. CNNs can often work with raw data that is only cleaned in the preprocessing step, while other models will typically necessitate feature extraction before passing the input to the ANN (Schirrmeister et al 2017). Alternatively, RNNs are also used for biosignal decoding, but these architectures typically need more technical knowledge to deploy and evaluate. The literature review clearly shows that CNN is the most deployed model and that biosignal decoding models are typically rather shallow, which can be attributed to the limited availability of data.

% Neural networks e.v. lezen van review arnau

% dus wel RNN vermelden en uitleggen ma eigenlijk enkel zien naarr CNN based zoals de EEGnet en ShallowConvNet

% Designing a neural network requires experience, as there is no systematic approach. More information on neural networks and their architectures is presented in the appendix, section ‘Neural networks’.

% We refer readers interested in knowing more than what we present to books such as Goodfellow et al (2016) and Aggarwal (2018).

% - - - - - - - - - -
% Difference ML & DL
% - - - - - - - - - -

\subsection{Difference between traditional machine learning and deep learning}
\label{subsec:processing_signals_ml_and_dl_difference}

% nn_can_learn_from_raw
% % Conceptually, given enough layers and neurons, and the proper architecture, a neural network can learn any mapping from inputs to outputs (Sonoda and Murata 2017) (they are universal function approximators). This means that any method of acquiring a signal, and representing it as floatingpoint values will eventually allow the network to make sense of the inputs, and learn something. bci review arnau

\lipsum[1-3]

% - - - - - - - - - -
% Common regular ML classifiers
% - - - - - - - - - -

\subsection{Supervised, Semi-Supervised, Unsupervised, and Self-Supervised Learning}
\label{subsec:processing_signals_ml_and_dl_tyes_of_learning_supervision}

% The most common form of ML is supervised learning, in which we assume that the data is presented as a set of input-output pairs, a dataset, which we call labeled data, as each input is labeled with its corresponding output (Caruana and NiculescuMizil 2006). Alternatively, unsupervised learning techniques do not use outputs for learning, but rather learn the (unknown) structure of the data. Semi-supervised learning methods use both labeled and unlabeled data, usually to learn the structure of the training data to become able to generate more (artificial) training points (Aznan et al 2019), that are used for conventional supervised learning in a second learning phase. Self-supervised learning (Jing and Tian 2019) is a similar approach that is used to learn the relevant structure in EEG data by first learning an unsupervised pretext task, after which the model is further trained on the target task with labeled data (Banville et al 2020, Kostas et al 2021). The remainder of this review will focus on supervised learning methods.

% Semi-supervised learning methods use both labeled and unlabeled data. Their objective is to learn a supervised learning task, even in cases where only a small amount of training data is available. Usually, semi-supervised approaches learn the structure of the training data to become able to generate more (artificial) training points (Aznan et al 2019), that are used for conventional supervised learning in a second learning phase. Self-supervised learning (Jing and Tian 2019) is a similar approach which is currently gaining traction in the larger ML community. This technique was previously used to learn the relevant structure in EEG data by first learning an unsupervised pretext task, after which the model is further trained on the target task with labeled data (Banville et al 2020, Kostaset al 2021). The remainder of this review will focus on supervised learning methods.

% Therefore, low-confidence labelled data is often used in a semi-supervised fashion as explained by \citet{deep_learn_low_label}.

\lipsum[1-3]

% - - - - - - - - - -
% Common regular ML classifiers
% - - - - - - - - - -

\subsection{Common regular machine learning classifiers}
\label{subsec:processing_signals_ml_and_dl_ml_classifiers}

% KNN
% RF
% SVM
% Single-layer Perceptron
% bekijk ml_strats_used_in_papers

\lipsum[1-7]

% - - - - - - - - - -
% Common regular DL classifiers
% - - - - - - - - - -

\subsection{Common deep learning classifiers}
\label{subsec:processing_signals_ml_and_dl_dl_classifiers}

% ANN (MLP fully connected -> wanneer DL en wanneer regular ML, uitleggen soms gezienals regular ML)
% CNN (conv layer, pooling layer)
% RNN (e.g. long term short term memory voor die layer)

% Veel al discussed in bci_opportunities_obstacles_motivating_examples_mi_models (BCIs)

% TODO: cnn bespreken a la: The most common type of DL neural networks at this time are the previously described CNNs (LeCun et al 1989). The subsequent application of convolutional layers results in a high-level representation of the input as stipulated by Goodfellow et al (2016). For example, in an object classification task, the network learns to extract primitive shapes from the raw input (a matrix of pixel values) in the first layer and then learns to extract objects from these primitive features in the next layer.
% en dus niet perse laatste layers voor classification maar vaak laatste layers terug mlps oid ipv puur convolutional layer

% In practice, neural networks can combine many layers of different kinds. It is not unusual to find neural networks that start with a few convolutional layers, to detect patterns independently of where they are in the input (such as edges in an image, or features in a 1D signal), then have one LSTM layer to be able to make sense of sequences of inputs, followed by a few feed-forward MLP-like layers to map what the LSTM layer learned to actual outputs. In the papers that we review in this article, great care is always given to explain and motivate the choice of neural architecture. Designing a neural network requires experience, as there is no systematic approach. We refer readers interested in knowing more than what we present here to books such as Goodfellow et al (2016) and Aggarwal (2018).


% bekijk ml_strats_used_in_papers

\lipsum[1-7]


% ---------------------------------------------- 
% COMMON ISSUES
% ---------------------------------------------- 

\section{Common issues when processing brain-signals and how to avoid them}
\label{sec:processing_signals_common_issues}

\lipsum[1-2]

% - - - - - - - - - -
% biased data
% - - - - - - - - - -

\subsection{Biased data}
\label{subsec:processing_signals_common_issues_bias}

\lipsum[1-3]

% - - - - - - - - - -
% evaluation
% - - - - - - - - - -

\subsection{Incorrect or ambiguous evaluation}
\label{subsec:processing_signals_common_issues_generalisation}
% e.g. only look at a person we also used in training etc

\lipsum[1-3]

% - - - - - - - - - -
% explainability and interpretability
% - - - - - - - - - -

\subsection{No explainability or interpretability}
\label{subsec:processing_signals_common_issues_exaplainable}

\lipsum[1-3]

% TOODO
\Gls{dl} often requires significant processing power and time to train, impacting the affordability of \gls{bci} research.
This is especially true when working with many \gls{eeg} sensors and features, and thus a high dimensional setting. 
\Gls{dl} is often also used in a black-box principle.
This means that the trained system lacks explainability and interpretability.
% TODO explain both terms
Recent governmental reports have suggested that laws will be coming in place to require these properties \citep{eu_ai_blackbox_report, explainable_ai_policy}.

% - - - - - - - - - -
% overfitting
% - - - - - - - - - -

\subsection{Overfitting}
\label{subsec:processing_signals_common_issues_overfitting}

% To avoid over-fitting, Batch Normalization (Ioffe and Szegedy 2015) considers the input of every layer in a neural network, and normalizes it so that, in expectation, the inputs of every layer has a zero mean and a unit variance. Dropout (Srivastava et al 2014) does not modify the values that flow through a neural network, but instead randomly disables neurons every time the network is evaluated during training. Both these methods contribute to improving training speed and generalization of the network, and ensure that the model converges to an optimal loss. Both batch normalization (Tayeb et al 2019, Tam et al 2020) and dropout (Gautam et al 2020, Tortora et al 2020a) are often used in biosignal decoding papers, sometimes both at the same time. Other normalization techniques are possible, such as L1- normalization or clipping the gradients (Zhang et al 2019a), but they have been superseded by Batch Normalization and Dropout.

% uit bci_review_arnau

\lipsum[1-5]

% ---------------------------------------------- 
% CONCLUSIONS OF CHAPTER
% ---------------------------------------------- 
\section{Chapter conclusions}
\label{sec:processing_signals_summary}
% TODO: summary of this chapter

\lipsum[1-3]