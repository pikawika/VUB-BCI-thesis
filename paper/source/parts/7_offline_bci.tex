% TODO:
%   - Kijk na of titels in header overflowen
% ----------  
% Questions:
%   - XXX

% Kijk naar wat Arnau review zegt over "minimal reporting"

\part{Development of motor imagery EEG classifiers}
\label{part:development}

\chapter{EEG-based offline classification system for motor imagery tasks}
\label{ch:offline_bci_system}

% ---------------------------------------------- 
% INTRODUCTION
% ---------------------------------------------- 
\section{Introduction to this chapter}
\label{sec:offline_bci_system_introduction}
% NOTE: "Introduction" exists in each chapter and gives a short intro to the chapter + what can be expected in the chapter

This Chapter discusses the seven different \gls{eeg} based \gls{mi} classification pipelines that were considered in this master thesis.
In particular, two traditional two-step \gls{ml} approaches were considered, both based on the \gls{csp} feature extraction technique.
These two approaches differ by the frequency filter they use in their preprocessing step, where one is fixed throughout all experiments and the other is configured using hyperparameter tuning.
The proven to be better performing extension to \gls{csp}, \gls{fbcsp}, is also discussed but not considered for the experiments in this master thesis.

The other five classification pipelines that were considered in this master thesis are one-step \gls{dl} approaches, as this master thesis focuses primarily on these kinds of approaches.
First, three literature proposed state-of-the-art \gls{cnn}-based approaches are discussed: EEGNet, Deep\-Conv\-Net and ShallowConvNet.
Both their architectural design and some implementation details are discussed.
The visualisation possibilities of these \gls{cnn}-based \gls{eeg} classification algorithms are also addressed.

The final two \gls{dl} classification pipelines that were considered in this master thesis are an extension to EEGNet proposed by this master thesis.
Both of these extensions aim to provide additional memory to EEGNet by incorporating \gls{lstm} functionality. 
One reuses all components of the EEGNet architectural design but adds a tunable \gls{lstm} layer right before the softmax layer.
The other approach only reuses part of the original EEGNet architectural design and makes use of a convolutional \gls{lstm} layer.
Again, some of the implementation details are also addressed.

All of the implemented pipelines together with the performed experiments and saved results are available on the GitHub page of this master thesis \citep{github_project}.
The state-of-the-art literature proposed \gls{cnn}-based \gls{dl} models are a modified version of the Keras reimplementation provided by \citet{arl_eegmodels} and are made available in the utillity file: $\texttt{EEGModels.py}$.
The EEGnet extensions are provided in the utility file: $\texttt{EEGNet\_with\_lstm.py}$.
A custom filter that is hyperparameter tunable with \gls{sklearn} has also been made and is available in the utility file $\texttt{custom\_sklearn\_components.py}$.
More general Keras and TensorFlow tools are also made available in a separate utility file: $\texttt{TF\_tools.py}$.
All these utility files can easily be imported and have the required inline documentation to make reusing them easy.

% ---------------------------------------------- 
% Two-step ML approaches
% ---------------------------------------------- 
\section{CSP-based two-step ML approaches}
\label{sec:offline_bci_system_two_step_ml}

As discussed, the focus of this master thesis lies on one-step \gls{dl} \gls{mi} \gls{eeg} classification pipelines, as these are most widely used in \gls{bc} systems according to \citet{bci_review_arnau}.
However, considering the significant role \gls{csp} played in allowing \gls{ml} approaches to effictively learn from \gls{eeg} data, accelerating the field of \gls{bci} research, this master thesis also considers two traditional two-step \gls{mi} \gls{eeg} \gls{ml} classification approaches based on \gls{csp}.
The architectural design of both of these pipelines is discussed in what follows.

It is noted that since the introduction of \gls{csp} by \citet{first_csp}, many extensions have been proposed that have proven to be far superior to this original version \citep{eeg_model_fbcsp, bci_book_csp_extension}.
As such, the results obtained from these two regular \gls{csp} pipelines discussed in Chapter \ref{ch:evaluation} should not be taken as an indicator of the performance for traditional two-step \gls{mi} \gls{eeg} \gls{ml} classification approaches in general.

% - - - - - - - - - -
% CSP explained
% - - - - - - - - - -

\subsection{The idea behind common spatial pattern(s) (CSP)}
\label{subsec:offline_bci_system_two_step_ml_csp_explained}

Just as \gls{svm} was a classification method originally proposed to solve binary classification tasks (see Section \ref{subsec:processing_signals_ml_and_dl_ml_classifiers}), \gls{csp} was originally proposed to find a spatial filter for feature extraction that leads to optimal variances between two classes of \gls{eeg} data.
The general idea of a spatial filter for feature extraction was already discussed in Section \ref{subsec:processing_signals_general_pipeline_features}.
This is a non-trivial task due to \gls{ers} and \gls{erd} signals, such as \gls{mi}, being time-locked to the event but not phase-locked and them being oscillatory processes.
To achieve an effective optimal variance between two classes of \gls{eeg} data, \gls{csp} has to make some assumptions.
\gls{csp} assumes the frequency band of interest is known, as discussed in Section \ref{subsec:biomedical_signals_working_with_eeg_brain_waves}, this was around 7\gls{hz} to 30\gls{hz} in case of \gls{mi} tasks.
Another assumption that is made is that the time window is known, which is more difficult to ensure as windowing the data around an event in the prediction timeline is not always possible, as further discussed in Section \ref{subsec:processing_signals_general_pipeline_windowing}.
Luckily, \gls{csp} can still work reasonably well even if both frequency band and time window estimations are off.
Besides these assumptions, \gls{csp} also assumes that the band-passed signal is jointly Gaussian within the time window and that there is a difference in the oscillatory signals between both classes.
Both of these assumption hold for most \gls{mi} \gls{eeg} data.

As discussed in Section \ref{subsec:processing_signals_general_pipeline_features}, the goal of a spatial filter is determining an optimal weight matrix $W$ for Equation \ref{eq:processing_signals_spatial_filter}.
In that section, it was also discussed how some data-driven solutions such as \gls{pca} and \gls{ica} do not use the class information, which can have degrading effects.
The goal of \gls{csp} is to find a weight matrix $W$ so that the variance of one signal is minimal, whilst the variance of the other is maximal.
This is shown in Figure \ref{fig:offline_bci_system_csp} where before filtering (Figure \ref{fig:offline_bci_system_csp_pre}) the signals are highly overlapping and though to discriminate, but after using \gls{csp} the signals are far easier to discriminate.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/offline/pre_csp.pdf}
        \captionsetup{width=\linewidth}
        \captionsetup{justification=centering}
        \caption{An arbitrary two-channel signal for two classes before \gls{csp} feature extraction. Free figure by MalcolmSlaney, CC BY-SA 4.0, via Wikimedia Commons.}
        \label{fig:offline_bci_system_csp_pre}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/offline/post_csp.pdf}
        \captionsetup{width=\linewidth}
        \captionsetup{justification=centering}
        \caption{An arbitrary two-channel signal for two classes after \gls{csp} feature extraction. Free figure by MalcolmSlaney, CC BY-SA 4.0, via Wikimedia Commons.}
        \label{fig:offline_bci_system_csp_post}
    \end{subfigure}
    \captionsetup{width=\linewidth}
    \captionsetup{justification=centering}
    \caption{Visualisation of the spatial transformation performed by \gls{csp}.}
    \label{fig:offline_bci_system_csp}
\end{figure}

There are multiple approaches to calculating the optimal weight matrix $W$ based on the optimal variance criteria.
Most commonly are using an optimisation problem, describing it as a generalized Eigenvalue problem and a more geometric approach.
The remainder of this section will discuss the generalized Eigenvalue problem approach.
\Citet{csp_optim_problem} discuss an optimisation approach in more detail, \citet{csp_geometry} does the same for a geometric approach.

The optimal weight matrix $W$ for Equation \ref{eq:processing_signals_spatial_filter} based on the \gls{csp} criteria can be mathematically described as shown in Equation \ref{eq:offline_bci_system_csp_w}.
In this equation, $X_i$ denotes the windows of class $i$ represented as a 2D matrix of $n$ channels by $t_i$ \gls{eeg} measurements per channel.
Due to the jointly Gaussian assumption, the covariance matrix of the signals per class can be easily calculated using Equation \ref{eq:offline_bci_system_csp_cov} where $i$ denotes the class of interest.
From this, the generalized Eigenvalue problem can be created to solve the problem.
Namely, the goal is to find the Eigenvectors $\mathbf{P}$ which contain the Eigenvector of any channel $j$ ($\mathbf{p_j}$) such that Equation \ref{eq:offline_bci_system_csp_solution} holds.
In this Equation, $\mathbf{I}_n$ denotes the identiy matrix for $i$ channels.
This problem can be further reduced to the Eigenvalue decomposition shown in Equation \ref{eq:offline_bci_system_csp_solution_eigen}.
From this, $\mathbf{w}$ can finally be found by taking $\mathbf{w} = \mathbf{p}^T_1$.
\Citet{csp_eigenvals} provides a more detailed explanation of the generalized Eigenvalue problem approach for solving \gls{csp} and some additional steps that help make it more applicable to \gls{bci} settings.

\begin{equation}
    \label{eq:offline_bci_system_csp_w}
    \mathbf{w} = \underset{\mathbf{w}}{\arg\max} \frac{|| \mathbf{w} X_1 ||^2}{|| \mathbf{w} X_2 ||^2}
\end{equation}

\begin{equation}
    \label{eq:offline_bci_system_csp_cov}
    \mathbf{R}_i = \frac{ \mathbf{X}_i \mathbf{X}_i^T }{t_i}
\end{equation}

\begin{equation}
    \label{eq:offline_bci_system_csp_solution}
    \begin{aligned}
        &\mathbf{P}^T \mathbf{R}_1 \mathbf{P} = \mathbf{D}
        \\
        &\text{and}
        \\
        &\mathbf{P}^T \mathbf{R}_2 \mathbf{P} = \mathbf{I}_n
    \end{aligned}
\end{equation}

\begin{equation}
    \label{eq:offline_bci_system_csp_solution_eigen}
    \mathbf{R}^{-1}_2 \mathbf{R}_1 = \mathbf{P} \mathbf{D} \mathbf{P}^{-1}
\end{equation}


% - - - - - - - - - -
% tradtional CSP
% - - - - - - - - - -

\subsection{Traditional CSP with LDA, SVM and RF}
\label{subsec:offline_bci_system_two_step_ml_basic_csp}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/offline/csp_fixed_filter_pipeline.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{Visual overview of the pipeline used for \gls{mi} \gls{eeg} classification using the standard \gls{csp} method and multiple traditional two-step \gls{ml} classifiers.}
    \label{fig:offline_bci_system_csp_fixed_filter_pipeline}
\end{figure}

Having explained the idea behind \gls{csp}, the pipelines using \gls{csp} in the experiments of this master thesis can be discussed.
The first pipeline uses the standard \gls{csp} feature extraction method for classifying \gls{mi} \gls{eeg} data in an offline setting and is visualised in Figure \ref{fig:offline_bci_system_csp_fixed_filter_pipeline}.
As discussed further in Section \ref{sec:evaluation_data_source} the used data considers a fixed 3-second window surrounding a known event, including 1 second before the event onset and two seconds after the event onset.
As discussed in Section \ref{subsec:processing_signals_general_pipeline_windowing}, different windowing stragies exist but are not epxlored in this master thesis, in part due to \gls{csp} assuming a known fixed time window as discussed in Section \ref{subsec:offline_bci_system_two_step_ml_csp_explained}.

The windowed data is first preprocessed with multiple preprocessing techniques that were already discussed in Section \ref{subsec:processing_signals_general_pipeline_preprocessing}.
First, the window is baseline correct on one second of data before the event onset.
The resulting data is further preprocessed to only include frequencies between 2\gls{hz} and 32\gls{hz}.
This is done by the use of a \gls{fir} filter design using the Blackman window method.
Whilst Section \ref{subsec:biomedical_signals_working_with_eeg_brain_waves} discussed \gls{mi} is mostly present in the frequency range of 7\gls{hz} to 30\gls{hz}, the boundaries were loosened to account for a fixed approach that is not hyper-tuned on a subject basis and the inclusion of a \textit{neutral} class, which is not a specific \gls{mi} task and most likely has lower frequency brain waves when taking into account the brainwave subdivision given in Table \ref{tab:biomedical_signals_brainwaves}.
Finally, the resulting data is clipped to a 2D matrix (channels x \gls{eeg} measurements) including the samples from either 0.1 seconds after the event onset to 0.6 seconds after the event onset or from 0.25 seconds before the event onset to 1.25 seconds after the event onset, depending on the experiment setup.
It should be noted there is no explicit band-stop filter in place to cancel out the \gls{ac} artefacts discussed in Section \ref{subsec:biomedical_signals_working_with_eeg_artefacts} as the used dataset has already filtered out these frequencies \citep{eeg_data}.

For feature engineering, the multiclass \gls{csp} method described by \citet{used_mc_csp} is used.
The use of a variant is needed as the \gls{csp} method discussed in Section \ref{subsec:offline_bci_system_two_step_ml_csp_explained} only support binary classification where the experiments from this master thesis use three-class \gls{mi} \gls{eeg} data.
The number of \gls{csp} components is hyperparameter tuned based on the possible selection of 2, 3, 4, 6 or 10 components.
This number may seem limited but providing more components will make overfitting more likely and in practice, the differences between six or more \gls{csp} components were found to be minimal after some pilot studies done in the experimental notebooks available on the GitHub repository of this master thesis \citep{github_project}.

For classification, three different classifiers were considered, \gls{lda}, \gls{svm} and \gls{rf}, all three of which were discussed in detail in Section \ref{subsec:processing_signals_ml_and_dl_ml_classifiers}.
As discussed, part of the reason the \gls{lda} classifier is so attractive is due to it not requiring specific hyperparameter tuning.
As such, the \gls{lda} classifier was not hyperparameter tuned in all but a few experiments that validated there was indeed negligible difference when changing the solver and tolerance values from their default parameters.
For the \gls{svm} classifier, the earlier discussed C value for controlling the smoothness of the boundary when using the kernel trick was hyperparameter tuned by trying multiple values between 0.01 and 100.
The kernel was also hyperparameter tuned by trying the \gls{rbf}, sigmoid and linear kernels.
For the \gls{rbf} and sigmoid kernel, the gamma parameter denoting the kernel coefficient was also hyper-tuned on values between 0.001 and 10, including some calculated values provided by \gls{sklearn}.
Finally, the \gls{rf} classifier was hyperparameter tuned for the number of decision trees (values between 10 and 500) in its ensemble as well as the maximum depth (3, 10 or no limit), minimal sample split (2, 5 or 10) and features (different percentual proportion including no limit) to be used by each of those decision trees.

The used preprocessing techniques and \gls{csp} methods are supplied by the Python MNE library \citep{mne}.
The traditional two-step \gls{ml} classifiers are provided by the \gls{sklearn} library from which the hyperparameter tuning functionalities were also used to hyperparameter tune the required components of this pipeline \citep{sklearn}.

% - - - - - - - - - -
% issue basic CSP
% - - - - - - - - - -

\subsection{The issue with a traditional CSP approach}
\label{subsec:offline_bci_system_two_step_ml_basic_csp_issue}
It has been discussed that in the classification of \gls{mi} \gls{eeg} data using \gls{csp} feature extraction paired with almost any classifier, even simple one such as \gls{lda}, can produce pleasant results for some experimental settings.
However, \gls{csp} in itself is a limited feature extraction method compared to the proposed extensions that have far outperformed it \citep{eeg_model_fbcsp, bci_book_csp_extension, four_class_mi_CSP_good, eeg_mi_model_lda_csp}.
To understand the intuitive reasoning behind most \gls{csp} extension, a different pipeline is configured where the fixed frequency filtering in the preprocessing step of the pipeline shown in Figure \ref{fig:offline_bci_system_csp_fixed_filter_pipeline} is now hyperparameter tunable.
Practically, this is done by providing an equal filtertering strategy as the one described in Section \ref{subsec:offline_bci_system_two_step_ml_basic_csp} through a \gls{sklearn} transformer.
For this custom \gls{sklearn} transformer provided in the $\texttt{custom\_sklearn\_components.py}$ utility file of this GitHub project, the lower bound and upper bound can be configured \citep{github_project}.
Being a \gls{sklearn} transformer, it can be used in the same hyperparameter tuning strategy as described in Section \ref{subsec:offline_bci_system_two_step_ml_basic_csp} to find optimal lower bound values and upper bound values.

From the results of these experiments, which are further discussed in Chapter \ref{ch:evaluation}, it becomes clear that the performance of \gls{csp} is heavily dependent on the used frequency filtering.
Accuracy for the worst found parameters and best parameters in 4-fold cross-validation ranged from 30\% to 75\% in an intrasession testing setup.
This huge fluctuation in performance follows from the assumption of a know frequency band that \gls{csp} makes as discussed in Section \ref{subsec:offline_bci_system_two_step_ml_csp_explained}.
The best found frequencies were also subject dependent, as Section \ref{subsec:biomedical_signals_working_with_eeg_brain_waves} already suggested.
Whilst incorporating this hyperparameter tuning as a training procedure in the \gls{csp} approach would already increase the performance of \gls{csp} by automating the finding of the best frequency range for a subject, even better solutions have been proposed.
These follow mainly from an equal intuitive idea of learning the frequency band(s) of interest.
The next session will discuss the commonly used \gls{fbcsp} extension of \gls{csp}.


% - - - - - - - - - -
% improving traditional CSP
% - - - - - - - - - -

\subsection{Improving traditional CSP with FBCSP}
\label{subsec:offline_bci_system_two_step_ml_basic_csp_improving}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/offline/fbcsp_pipeline.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{Visual overview of a proposed \gls{fbcsp} pipeline used for \gls{mi} \gls{eeg} using multiple traditional two-step \gls{ml} classifiers.}
    \label{fig:offline_bci_system_fbcsp_pipeline}
\end{figure}

Since the frequency band has such a significant influence on the performance of \gls{csp}, as discussed in Section \ref{subsec:offline_bci_system_two_step_ml_basic_csp_issue}, many extensions to \gls{csp} have been introduced to automate the learning of the best frequency band(s).
One popular extension to \gls{csp} that builds upon the idea of learning frequency-bands from labelled data is \gls{fbcsp} as proposed by \citet{eeg_model_fbcsp}.
Figure \ref{fig:offline_bci_system_fbcsp_pipeline} shows how a potential \gls{fbcsp} pipeline may look.
There are two main differences with the previous pipeline that was shown in Figure \ref{fig:offline_bci_system_csp_fixed_filter_pipeline}: the removal of the manual frequency filter and the replacement of \gls{csp} feature extraction by \gls{fbcsp} feature extraction.

Intuitively, \gls{fbcsp} performs multiple frequency band filters with different boundaries and stores the results on separate independent threads.
For each of those threads, traditional \gls{csp} is then performed, resulting in many different \gls{csp} features.
Since this data would likely be too complex to be handled by a traditional two-step \gls{ml} classifier, a smaller subset of features is selected from the \gls{csp} filtered signal.
The list of these final features, with multiple features per original thread, is then provided to the classifier as before.
This \gls{fbcsp} concept is also illustrated in Figure \ref{fig:offline_bci_system_fbcsp_pipeline}.
It should become clear that many different approaches for creating potentially overlapping frequency bands are possible and that the final feature extraction from the \gls{csp} transformed signals can also differ greatly.
These are all aspects that can be optimized based on the implementation of the proposed algorithm.
It should also be noted that \gls{fbcsp} does not explicitly learn frequency bands but rather provides multiple features from multiple frequency bands for which it relies on the classifier to ignore those that have no or little information.

Whilst being a relatively simple extension to \gls{csp}, \gls{fbcsp} combined with a classifier such as \gls{svm} or \gls{rf} has been proven successful at classifying \gls{mi} \gls{eeg} with far greater accuracy then the traditional \gls{csp} pipeline used in this master thesis \citep{eeg_model_fbcsp, four_class_mi_CSP_good, fbcsp_classi_eeg_mi}.
The performance of \gls{fbcsp} even rivals state-of-the-art one-step \gls{dl} classifiers such as DeepConvNet and ShallowConvNet \citep{eeg_model_hbm}.
However, the implementation and evaluation of such a pipeline fall outside the scope of this master thesis.
It should also be noted that whilst \gls{fbcsp} is often used in literature it is not included in any of the popular Python \gls{eeg} processing libraries.
Providing this feature extraction method as an \gls{sklearn} compatible component can be a helpful future work to facilitate pipeline development of future research.
It is noted that some open-source Python implementations of \gls{fbcsp} are available but they have limited support and documentation \citep{fbcsp_git1, fbcsp_git2}.



% ---------------------------------------------- 
% One-step DL approaches
% ---------------------------------------------- 
\section{Convolutional-based one-step DL approaches}
\label{sec:offline_bci_system_one_step_dl}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/dl_pipeline.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{Visual overview of a pipeline using a \gls{dl} classifier for \gls{mi} \gls{eeg} classification.}
    \label{fig:offline_bci_system_dl_pipeline}
\end{figure}

The majority of the models considered in this master thesis use a \gls{dl} classifier.
Their pipelines are simplified from those used in traditional two-step \gls{ml} classification since they require minimal preprocessing and no feature extraction as was discussed in Section \ref{subsec:processing_signals_ml_and_dl_difference}.
Figure \ref{fig:offline_bci_system_dl_pipeline} shows the general pipeline structure used for the \gls{dl} \gls{mi} \gls{eeg} classifiers that will be discussed in greater detail below.
All of these classifiers are made using Keras and are available via the appropriate utility files on the GitHub repository of this master thesis \citep{github_project, keras}.

It should be noted once again that the used open-source dataset has already performed a band-stop filter to cancel out the \gls{ac} artefacts discussed in Section \ref{subsec:biomedical_signals_working_with_eeg_artefacts}.
However, it could be argued that even this filtering of the signal to remove the \gls{ac} artefacts can be learned by the \gls{dl} model.
Although due to this frequency being overpopulated by the \gls{ac} artefact there is likely very slim to no information present in the filtered out frequency range.
As such, removing the \gls{ac} artefact and other artefacts manually can facilitate the learning process, decreasing the computational time it takes to train the model.
For these reasons, minimal preprocessing, such as \gls{ac} artefacts removal is often still done in \gls{dl} pipelines.


% - - - - - - - - - -
% EEGNet
% - - - - - - - - - -
% TODO: We gebruiken speciale dropoff, niet de "traditionele"
% TODO: Refs voorzien naar DL explains

\subsection{EEGNet}
\label{subsec:offline_bci_system_one_step_dl_eegnet}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/eegnet_intuitive.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{A conceptual overview of EEGNet after the overview by \citet{eeg_model_eegnet}.}
    \label{fig:offline_bci_system_dl_eegnet_intuitive}
\end{figure}

EEGNet is a \gls{cnn} based \gls{eeg} classification model proposed by \citet{eeg_model_eegnet}.
There are some key properties of EEGNet that make it different from other \gls{dl} models proposed in the \gls{bci} field.
First, rather than using traditional convolutional layers, it uses depthwise and separable convolutional layers to reflect known performant feature extraction methods from the \gls{bci} field.
These special types of convolutional layers were already discussed in Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers}.
As a result of using these special types of convolutional filters and by being a compact \gls{cnn}, EEGNet can learn from relatively few samples \citep{eeg_model_eegnet}.

Second, instead of focusing on one specific \gls{bci} paradigm such as \gls{ers} and \gls{erd} or even more specifically \gls{mi}, the model is created in such a way that it is applicable to many \gls{bci} paradigms.
In the paper proposing the model, four distinct datasets from different paradigms were used to compare the performance with other models.
The first dataset consists of visual P300 \gls{erp} signals.
The second dataset revolves around another type of \gls{erp} signal which occurs after an unusual event occurs in the subject's environment or task: an \gls{ern}.
The third datasets contains both \gls{erp} signals and oscillatory components as a consequence of \gls{ers} or \gls{erd}.
This type of potential is known as a \gls{mrcp}.
The fourth and final dataset revolves \gls{mi}, the most challenging paradigm according to \citet{eeg_model_eegnet}.
\citet{eeg_model_eegnet} have shown that EEGNet has comparable performance across all these tested paradigms compared to the state-of-the-art of that specific paradigm when few data samples are available.

Third, \citet{eeg_model_eegnet} proposed a method of visualising the learned features of the model.
This helps in reducing the black-box problem discussed in Section \ref{subsec:processing_signals_common_issues_exaplainable}.
Together with DeepConvNet and ShallowConvNet that is discussed later in this chapter, EEGNet is considered a state-of-the-art method in the \gls{bci} field.

The most important components of EEGNet are the different convolutional layers as shown in the conceptual overview of \ref{fig:offline_bci_system_dl_eegnet_intuitive}.
The first convolutional layer is a regular two-dimensional convolutional layer and is used to learn frequency-like filters.
The second convolutional layer is a depthwise convolution layer and is used to learn spatial filters specific to the frequency-converted signal given as output by the previous layer.
The third and final convolutional layer is a separable convolution layer.
As discussed in Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers}, this is a combination of a depthwise convolution followed by a pointwise convolution.
\Citet{eeg_model_eegnet} state this depthwise convolution learns a temporal summary for each feature map individually and the pointwise convolution layer learns how to optimally mix the feature maps together.

When comparing this design with the \gls{fbcsp} method visualised in Figure \ref{fig:offline_bci_system_fbcsp_pipeline} it is clear that they share a lot of similarities.
The first convolutional layer which tries to learn frequency-like filters relates to the first step of \gls{fbcsp} performing different band-pass filters.
The second convolutional layer which tries to learn a spatial filter for the frequency filtered signal is comparable to the use of \gls{csp} in the second phase of \gls{fbcsp}.
The third and final convolutional layer which aims to extract temporal features from the spatial filter and connect all layers back together is comparable to the feature extraction from the \gls{csp} threads in \gls{fbcsp} and the traditional two-step \gls{ml} classifier used in \gls{fbcsp}.

An overview of all layers effectively used is given in Figure \ref{fig:offline_bci_system_dl_eegnet_full}.
This combines the use of dropout and batch normalization to combat overfitting tendencies and the internal covariate shift as discussed in Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers} among other technicalities.
Compared to the implementation of EEGNet provided by \citet{arl_eegmodels} many more parameters of the model were made tunable.
In particular, the following parameters are tunable and their defaults for the experiments are given in brackets: channel amount (21), amount of timepoints (100), amount of classes (3), the dropout rate (0.5), the kernel length (50), F1 (8), F2 (16), D (2), the normalisation rate (0.25) and the dropout type (2D spatial dropout).
Most of these parameters are based on the recommnedation provided by \citet{eeg_model_eegnet} and \citet{arl_eegmodels}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/eegnet_full.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{All layers of the EEGNet model.}
    \label{fig:offline_bci_system_dl_eegnet_full}
\end{figure}

% - - - - - - - - - -
% DeepConvNet
% - - - - - - - - - -

\subsection{DeepConvNet}
\label{subsec:offline_bci_system_one_step_dl_deepconvnet}

DeepConvNet is another \gls{cnn} based \gls{eeg} classification model.
It is proposed by \citet{eeg_model_hbm} and was made for the more specific use of decoding \gls{mi} and effective motory tasks.
Although DeepConvNet took a relatively general \gls{dl} approach, it is shown by by \citet{eeg_model_eegnet} that it generalizes worse to \gls{bci} paradigms beside \gls{mi}.
In particular, \citet{eeg_model_eegnet} found DeepConvNet to perform poorly on the \gls{mrcp} paradigm dataset where EEGNet got almost double the \gls{auc}.
\Citet{eeg_model_eegnet} found the performance of EEGNet and DeepConvNet to be comparable for the \gls{mi} dataset.
To further validate these results, DeepConvNet was also used in the experiments of this master thesis.

An additional motivation for using the DeepConvNet model proposed by \citet{eeg_model_eegnet} is the fact that it also has a visualisation technique of the learned features just like EEGNet.
Another motivating aspect of including DeepConvNet in the experiments of this master thesis follows from the authors also proposing a more shallow alternative, ShallowConvNet.
ShallowConvNet is also used in the experiments of this master thesis and discussed in Section \ref{subsec:offline_bci_system_one_step_dl_shallowconvnet}.
Having both an explicitly deep and shallow network allows for comparing the two besides the theoretical discussion from Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers}.
Finally, since DeepConvNet is a relatively general \gls{dl} model that is inspired by successful \gls{cnn} from the computer vision field such as ImageNet by \citet{imagenet}, it doesn't strictly represent feature extraction in terms of knowing good strategies.
This is sharp contrast with EEGNet that took clear inspiration of \gls{fbcsp} as discussed in Section \ref{subsec:offline_bci_system_one_step_dl_eegnet}.

An overview of all layers effectively used is given in Figure \ref{fig:offline_bci_system_dl_deepconvnet_full}.
One noteworthy aspect of this architecture is that there is no activation function between the second and third layers, which are both convolutional layers.
The first convolutional layer goes over the timepoints while the second goes over the channels.
Since these are two separate dimensions and there is no activation function in between, they could have been combined into a singular convolutional layer. 
Whilst the base implementation of DeepConvNet was provided by \citet{arl_eegmodels}, many more parameters of the model were made tunable once again.
In particular, the following parameters are tunable and their defaults for the experiments are given in brackets: number of classes (3), number of channels (21), amount of timepoints (100), dropout rate (0.6), the kernel length of the first convolutional layer (4), striding of 2D convolution layers (1, 2), pool size (1, 2).
Most of these parameters are lowered from their recommended settings by \citet{eeg_model_hbm} due to the low amount of training data present for the experiments of this master thesis.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/deepconvnet_full.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{All layers of the DeepConvNet model.}
    \label{fig:offline_bci_system_dl_deepconvnet_full}
\end{figure}


% - - - - - - - - - -
% ShallowConvNet
% - - - - - - - - - -

\subsection{ShallowConvNet}
\label{subsec:offline_bci_system_one_step_dl_shallowconvnet}

As discussed in Section \ref{subsec:offline_bci_system_one_step_dl_deepconvnet}, ShallowConvNet is a more shallow \gls{cnn} based \gls{eeg} classificiation model proposed in the same paper of the DeepConvNet proposal \citep{eeg_model_hbm}.
As opposed to DeepConvNet which took inspiration from complex \gls{cnn} from the computer vision field, ShallowConvNet takes inspiration from \gls{fbcsp} in their design.
\Citet{eeg_model_hbm} argues the first convolutional layer corresponds to the band pass filtering of \gls{fbcsp} and the second to the \gls{csp} filter in \gls{fbcsp}.
\Citet{eeg_model_hbm} argues the rest of the model relates to the \gls{fbcsp} phase of finding features in the \gls{csp} streams, combining them and training a traditional \gls{ml} classifier.

An overview of all layers effectively used is given in Figure \ref{fig:offline_bci_system_dl_deepconvnet_full}.
Again, there is no activation function between the second and third layers, which are both convolutional layers.
As discussed for DeepConvNet, this means they could have been combined into a singular convolutional layer. 
Whilst the base implementation of ShallowConvNet was provided by \citet{arl_eegmodels}, some more parameters of the model were made tunable once again.
In particular, the following parameters are tunable and their defaults for the experiments are given in brackets: number of classes (3), number of channels (21), amount of timepoints (100), dropout rate (0.5), the kernel length of the first convolutional layer (10), striding of 2D convolution layers (1, 6), pool size (1, 30).
Most of these parameters correspond to the recommended settings by \citet{eeg_model_hbm}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/shallowconvnet_full.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{All layers of the ShallowConvNet model.}
    \label{fig:shallowconvnet_full}
\end{figure}

% - - - - - - - - - -
% Interpreting 
% - - - - - - - - - -

\subsection{Interpreting these black box models}
\label{subsec:offline_bci_system_one_step_dl_interpreting}

It was discussed for both the EEGNet model as well as the DeepConvNet and ShallowConvNet model that the visualisation methods are described in the papers proposing the models \citep{eeg_model_eegnet,eeg_model_hbm}.
The intuitive reasoning behind these visualisations is that, as discussed, the first two convolutional layers of these models relate to the first two steps of the \gls{fbcsp} feature extraction method.
This means that the transformation offered by the first convolutional layer corresponds to frequency-like filters.
As such, the trained model can be shortened to include only this first convolutional layer and when passing through the source \gls{eeg} data, each output channel (filter) of the convolutional layer represents a frequency-like filtered \gls{eeg} signal when plotted.
Both through frequency analysis of these output channels and by analysis of the weights of the learned kernel, insight into what exact modifications have been made can be determined.
Intuitively, this is how both papers visualise or determine the learned frequency band-pass filters.

Likewise, the second convolutional layer of these models corresponds to the temporal filtering done in \gls{fbcsp}.
Looking at the learned kernel weights, it can be seen how much each channel of the input \gls{eeg} contributes to the output of the second convolutional layers.
This allows for creating \glspl{topomap} similar to those that can be created from a \gls{csp} feature extraction map.
An example of such \glspl{topomap} created from multiple \gls{csp} components is given in Figure \ref{fig:offline_bci_csp_visual}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/offline/csp_vis.pdf}
    \captionsetup{width=0.85\linewidth}
    \captionsetup{justification=centering}
    \caption{\glspl{topomap} created from multiple \gls{csp} components using subsampled channels from the international 10-20 system.}
    \label{fig:offline_bci_csp_visual}
\end{figure}

For more details on how these visualisation work for the discussed \gls{cnn} models, the reader is referred to the original paper proposing them \citep{eeg_model_eegnet,eeg_model_hbm}.
Neither \citet[][Authors of EEGNet ]{eeg_model_eegnet} or \citet[][Authors of DeepConvNet and ShallowConvNet ]{eeg_model_hbm} provide their source code to recreate these visualisations.
Adding to this, the proposed visualisation method in the EEGNet paper makes use of the DeepExplain Python library by \citep{deepexplain} for visualising Keras models.
However, this library only has official support for TensorFlow V1, whilst the Keras and the TensorFlow version used for the implementations in this master thesis are V2.
Because of this, providing further details on this visualisation through means of reimplementation of these methods falls outside the scope of this master thesis.
More details on visualisation techniques for \gls{cnn} in general is given by \citet{cnn_visualisations}.

% ---------------------------------------------- 
% Adding recurrent NN
% ---------------------------------------------- 
\section{Adding memory to one-step DL approaches}
\label{sec:offline_bci_system_adding_memory}

Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers} discussed that convolutional layers used on time series data, or on the time axis of a 2D signal such as \gls{eeg}, can learn kernels which detect local patterns in the time series.
In that same section, it is also discussed that \gls{lstm} layers have explicit memory built into their design.
This makes processing time series where temporal features are of most importance an ideal application for \gls{lstm}.
For these reasons, studies across many fields have been done to encorperate both convolutional layers for feature extraction and an \gls{lstm} layers for working with the time-series features into a singular \gls{cnn}-\gls{lstm} model \citep{lstm_cnn_mi_eeg, cnn_bilstm_eeg_robot_arm}.

Given that the EEGNet model works well on multiple \gls{bci} paradigms, it is assumed that the features extracted by EEGNet carry a lot of information.
Following this reasoning, this master thesis proposes two \gls{cnn}-\gls{lstm} models where the \gls{cnn} based feature extraction is heavily inspired by the EEGNet model.
Having the EEGNet architecture first, the EEGNet-specific visualisation could still be used.
Likewise, the visualisation of \gls{lstm} layers to determine which areas in a temporal domain are of interest has also already been widely studied.
Most commonly in the processing of \gls{ecg}, this technique has seen great success, with \citet{lstm_visual_ecg} using \gls{lstm} to visualise which regions of an \gls{ecg} was used most for determining the classification of possible arrhythmia.
As such, it is believed that a \gls{cnn}-\gls{lstm} model can give great insight into both the spatial features and temporal features learned when using appropriate visualisation techniques.
The study of which is an interesting future work for this master thesis.

% - - - - - - - - - -
% LSTM eegnet
% - - - - - - - - - -

\subsection{Adding an additional LSTM layer to EEGNet}
\label{subsec:offline_bci_system_adding_memory_lstm_eegnet}

% TODO: add figure of model summary so that the conversion etc are clear en trainable params vermelden

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/eegnetlstm_full.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{All layers of the CNN-LSTM model using all EEGNet layers for feature extraction and the addition of LSTM functionality for time series processing.}
    \label{fig:eegnetlstm_full}
\end{figure}

The first proposed \gls{cnn}-\gls{lstm} model consists of the combination of all EEGNet layers discussed in Section \ref{subsec:offline_bci_system_one_step_dl_eegnet} with a traditional bidirectional \gls{lstm} layer.
This layer type was discussed in Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers}.
An overview of all layers used is given in Figure \ref{fig:eegnetlstm_full}.
Note some subtle changes in the EEGNet feature extraction part of the model, namely a reduction in the downsampling performed by the pooling layers.
This was done to retain a reasonable amount of temporal resolution.
Besides this, some dimensionality transformations and permutations are in place to ensure a correct data representation for the \gls{lstm} layer.
It is important to note that the \gls{lstm} layer also makes use of internal dropout functionality provided by Keras \citep{keras}.
The dropout layer shown after the \gls{lstm} layer is an additional regular dropout layer using half of the internal dropout rate provided for the \gls{lstm} layer.
Such an aggressive dropout strategy is needed due to the many additional trainable parameters introduced by the \gls{lstm} increasing the risk of overfitting.

The input of the \gls{lstm} layer using the default parameters given below corresponds to a 2D matrix of 25 timepoints, a reduction of four compared to the input signal due to the two pooling layers, having 16 features each.
The following parameters are tunable for the EEGNet part of the model and their defaults for the experiments are given in brackets: channel amount (21), amount of timepoints (100), amount of classes (3), the dropout rate (0.5), the kernel length (50), F1 (8), F2 (16), D (2), the normalisation rate (0.30) and the dropout type (2D spatial dropout).
For the added layers, the parameters and their defaults for the experiments given in brackets are: \gls{lstm} units (64), \gls{lstm} specific dropout (0.6) and \gls{lstm} specific kernel legularizer (L1= L2 = 0.0001).

% - - - - - - - - - -
% Convolutional LSTM eegnet
% - - - - - - - - - -

\subsection{EEGNet with convolutional LSTM layer}
\label{subsec:offline_bci_system_adding_memory_convlstm_eegnet}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../images/pipeline/eegnetlstmconv_full.pdf}
    \captionsetup{width=0.8\linewidth}
    \captionsetup{justification=centering}
    \caption{All layers of the CNN-LSTM model using a reduced version of EEGNet for feature extraction and the addition of convolutional LSTM functionality for time series processing.}
    \label{fig:eegnetlstmconv_full}
\end{figure}

The second \gls{cnn}-\gls{lstm} model that this master thesis proposes consists of the combination of only the first two convolutional layers from EEGNet discussed in Section \ref{subsec:offline_bci_system_one_step_dl_eegnet} with a bidirectional one dimensional convolutional \gls{lstm} layer.
This layer type was discussed in Section \ref{subsec:processing_signals_ml_and_dl_dl_classifiers}.
An overview of all layers used is given in Figure \ref{fig:eegnetlstmconv_full}.
In essence, the last convolutional layer that was present in EEGNet is replaced by a \gls{lstm} layer that uses a comparable convolution.

For the same reasons as before, a reduction in the downsampling performed by the pooling layer is performed, with the default setting used for this experiment not using this pooling layer at all.
Again, the \gls{lstm} layer makes use of internal dropout functionality provided by Keras \citep{keras} and the dropout layer shown after the \gls{lstm} layer is an additional regular dropout layer using half of the internal dropout rate provided for the \gls{lstm} layer.
It is noted that the Keras implementation of the one-dimensional convolutional LSTM layer has no CUDA support and as such is incredibly slow to train.

The input of the \gls{lstm} layer using the default parameters given below corresponds to 100 timepoints having 16 features each.
These 16 features are over 1 channel due to the depthwise convolutional layer present in the model.
The following parameters are tunable for the EEGNet part of the model and their defaults for the experiments are given in brackets: channel amount (21), amount of timepoints (100), amount of classes (3), the dropout rate (0.5), the kernel length (50), F1 (8), D (2), the normalisation rate (0.25), average pooling before LSTM (None) and the dropout type (dropout).
For the added layers, the parameters and their defaults for the experiments given in brackets are: \gls{lstm} filters (64), \gls{lstm} kernel size (4), \gls{lstm} specific dropout (0.6), strides used for convolution (1) and \gls{lstm} specific kernel legularizer (L1= L2 = 0.0001).

% ---------------------------------------------- 
% CONCLUSIONS OF CHAPTER
% ---------------------------------------------- 
\section{Chapter conclusions}
\label{sec:offline_bci_summary}

This chapter discussed the seven different \gls{mi} \gls{eeg} classification pipelines used for the experiments in this master thesis.
Two traditional two-step \gls{ml} approaches were discussed, both relying on the \gls{csp} spatial filter for feature extraction.
One used a hyperparameter tuned frequency band-pass filter for preprocessing the signal and the other a fixed one.
This led to the discussion of \gls{fbcsp}, a pipeline that is not implemented but the working of which is addressed.
The main focus of this master thesis revolves around one-step \gls{dl} methods and five different such pipelines were discussed.
Three implemented state-of-the-art literature proposed \gls{cnn} based models were discussed: EEGNet, DeepConvNet and ShallowConvNet.
Besides their architectural design, their relation to the \gls{fbcsp} method was also discussed as well as the possibilities of visualising them.
Finally, two \gls{cnn}-\gls{lstm} models were proposed that extend upon EEGNet.
One makes use of an additional bidirectional \gls{lstm} layer on top of the EEGNet-derived features whilst the other aims to directly incorporate LSTM functionality into the EEGNet architecture by using a bidirectional convolutional \gls{lstm} layer.
For these models, it was also discussed how potential visualisation of the learned behaviour could be done.